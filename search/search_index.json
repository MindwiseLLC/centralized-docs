{"config": {"lang": ["en"], "separator": "[\\s\\-]+", "pipeline": ["stopWordFilter"]}, "docs": [{"location": "onboarding/dvc-guide/", "title": "\ud83d\udce6 DVC + Git Tracking Tutorial (Internal Guide)", "text": "<p>This guide teaches you how to use Git and DVC from scratch to track code, data, models, and experiments in a  clean, reproducible way.</p> <p>We assume no prior experience with either tool.</p>"}, {"location": "onboarding/dvc-guide/#what-gets-tracked-where", "title": "\ud83e\uddf0 What Gets Tracked Where?", "text": "Type Use Tool Why? Source code Git Version control for all scripts/configs Docs (Markdown) Git Easy collaboration + versioning Small config files Git Human-readable, diffable Large data files DVC Git isn't optimized for large files Model binaries DVC Models can be big, binary, auto-updating Outputs (plots/logs) DVC Optional: reproducible pipeline artifacts <p>\ud83d\udd12 Never add raw datasets or model files directly to Git. Always track them with DVC.</p>"}, {"location": "onboarding/dvc-guide/#1-project-setup-from-scratch", "title": "\ud83d\udd27 1. Project Setup From Scratch", "text": ""}, {"location": "onboarding/dvc-guide/#a-initialize-git-repo", "title": "a) Initialize Git repo", "text": "<pre><code>mkdir my-ml-project &amp;&amp; cd my-ml-project\ngit init\n</code></pre> <p>Create a basic folder structure:</p> <pre><code>mkdir -p data/raw models notebooks scripts docs\n</code></pre>"}, {"location": "onboarding/dvc-guide/#b-create-gitignore", "title": "b) Create <code>.gitignore</code>", "text": "<pre><code>echo \"__pycache__/\\n*.pyc\\n.env\\n.vscode/\\ndata/*\\nmodels/*\" &gt; .gitignore\ngit add .gitignore\n</code></pre>"}, {"location": "onboarding/dvc-guide/#c-commit-base-structure", "title": "c) Commit base structure", "text": "<pre><code>git add .\ngit commit -m \"Initialize repo structure\"\n</code></pre>"}, {"location": "onboarding/dvc-guide/#2-set-up-dvc", "title": "\ud83d\udce6 2. Set Up DVC", "text": ""}, {"location": "onboarding/dvc-guide/#a-install-and-initialize-dvc", "title": "a) Install and initialize DVC", "text": "<pre><code>pip install dvc\ndvc init\n</code></pre> <p>This creates a <code>.dvc</code> directory and updates <code>.gitignore</code>.</p>"}, {"location": "onboarding/dvc-guide/#b-commit-dvc-setup-to-git", "title": "b) Commit DVC setup to Git", "text": "<pre><code>git add .dvc .gitignore dvc.yaml dvc.lock\ngit commit -m \"Initialize DVC\"\n</code></pre>"}, {"location": "onboarding/dvc-guide/#c-configure-remote-storage-internal", "title": "c) Configure remote storage (internal)", "text": "<pre><code>dvc remote add -d storage s3://internal-dvc-storage/project-name\n</code></pre> <p>\ud83e\udde0 Ask your lead for the correct internal S3 path and credentials.</p>"}, {"location": "onboarding/dvc-guide/#3-tracking-data-with-dvc", "title": "\ud83d\uddc2\ufe0f 3. Tracking Data with DVC", "text": ""}, {"location": "onboarding/dvc-guide/#a-add-raw-data", "title": "a) Add raw data", "text": "<pre><code>mv ~/Downloads/train.csv data/raw/\ndvc add data/raw/train.csv\n</code></pre>"}, {"location": "onboarding/dvc-guide/#b-commit-data-reference", "title": "b) Commit data reference", "text": "<pre><code>git add data/raw/train.csv.dvc .gitignore\ngit commit -m \"Track train.csv with DVC\"\n</code></pre>"}, {"location": "onboarding/dvc-guide/#c-push-to-remote", "title": "c) Push to remote", "text": "<pre><code>dvc push\n</code></pre>"}, {"location": "onboarding/dvc-guide/#4-collaborating-with-git-dvc", "title": "\ud83e\udd1d 4. Collaborating with Git + DVC", "text": ""}, {"location": "onboarding/dvc-guide/#pull-team-members-changes", "title": "Pull team member\u2019s changes:", "text": "<pre><code>git pull origin main\ndvc pull  # fetch corresponding datasets or models\n</code></pre>"}, {"location": "onboarding/dvc-guide/#5-tracking-models-and-artifacts", "title": "\ud83e\udde0 5. Tracking Models and Artifacts", "text": ""}, {"location": "onboarding/dvc-guide/#a-add-model-files-after-training", "title": "a) Add model files after training", "text": "<pre><code>python train.py  # outputs to models/model.pkl\n\ndvc add models/model.pkl\ngit add models/model.pkl.dvc\ngit commit -m \"Track model with DVC\"\ndvc push\n</code></pre>"}, {"location": "onboarding/dvc-guide/#b-track-evaluation-results-logs-plots", "title": "b) Track evaluation results, logs, plots", "text": "<pre><code>dvc add outputs/metrics.json\n\ngit add outputs/metrics.json.dvc\ngit commit -m \"Log metrics for experiment v2\"\ndvc push\n</code></pre>"}, {"location": "onboarding/dvc-guide/#6-full-experiment-workflow", "title": "\ud83d\udd01 6. Full Experiment Workflow", "text": "<pre><code>git checkout -b experiment/lstm-v1\n\n# Update script/config\npython train.py\n\n# Track model + data\nmv model.pkl models/\ndvc add models/model.pkl\ndvc push\n\ngit add .\ngit commit -m \"Run LSTM baseline with config A\"\ngit push origin experiment/lstm-v1\n</code></pre> <p>To reproduce someone\u2019s run:</p> <pre><code>git checkout experiment/lstm-v1\ndvc pull\npython evaluate.py --model models/model.pkl\n</code></pre>"}, {"location": "onboarding/dvc-guide/#7-git-workflow-guidelines", "title": "\ud83d\udcd8 7. Git Workflow Guidelines", "text": ""}, {"location": "onboarding/dvc-guide/#follow-branch-naming", "title": "\u2705 Follow branch naming:", "text": "<ul> <li><code>main</code>, <code>develop</code></li> <li><code>feature/&lt;name&gt;</code></li> <li><code>experiment/&lt;name&gt;</code></li> <li><code>fix/&lt;bug&gt;</code></li> </ul>"}, {"location": "onboarding/dvc-guide/#use-clean-readable-commit-messages", "title": "\u2705 Use clean, readable commit messages:", "text": "<pre><code>git commit -m \"Add DVC tracking to preprocessing outputs\"\n</code></pre>"}, {"location": "onboarding/dvc-guide/#tag-major-runs", "title": "\u2705 Tag major runs:", "text": "<pre><code>git tag -a exp-2025-06-01-lstm -m \"LSTM v1 trained on reduced set\"\ngit push origin --tags\n</code></pre>"}, {"location": "onboarding/dvc-guide/#common-git-dvc-commands", "title": "\ud83d\udee0 Common Git &amp; DVC Commands", "text": "Task Command Initialize Git &amp; DVC <code>git init</code>, <code>dvc init</code> Track new file <code>dvc add file.csv</code> Commit file metadata <code>git add file.csv.dvc &amp;&amp; git commit</code> Push large files to remote <code>dvc push</code> Pull files from teammates <code>git pull</code> + <code>dvc pull</code> Visualize DVC DAG <code>dvc dag</code>"}, {"location": "onboarding/dvc-guide/#final-checklist", "title": "\ud83d\udd10 Final Checklist", "text": "<p>\u2705 Data and model files are tracked with DVC \u2705 Git repo contains only <code>.dvc</code> references \u2705 All changes pushed to GitHub \u2705 <code>dvc push</code> run after every experiment \u2705 Metadata (<code>.dvc</code>, <code>params.yaml</code>, <code>dvc.lock</code>) are versioned in Git</p>"}, {"location": "onboarding/how-to-write-documentation/", "title": "\ud83d\udcdd How to Write Documentation (Team Guide)", "text": "<p>This is your go-to guide for writing great documentation within our ML/AI teams. We organize everything into 3 main  sections:</p> <ol> <li> <p>Business Documentation  \u2013 for context, goals, and decisions</p> </li> <li> <p>Technical Documentation  \u2013 for architecture, systems, and code</p> </li> <li> <p>User Manuals  \u2013 for guides and usage help</p> </li> </ol> <p>Before you write anything, ask yourself:</p> <p>\u201cWhat is the  purpose  of this information, and  who  is it for?\u201d</p>"}, {"location": "onboarding/how-to-write-documentation/#1-business-documentation", "title": "1. \ud83d\udcca Business Documentation", "text": "<p>Audience: Product owners, managers, external stakeholders, and non-technical team members Goal: Explain the  what  and  why  of our work</p>"}, {"location": "onboarding/how-to-write-documentation/#include", "title": "\u2705 Include:", "text": "<ul> <li> <p>Project goals and KPIs</p> </li> <li> <p>Use cases</p> </li> <li> <p>High-level architecture overview</p> </li> <li> <p>Design decisions and trade-offs (why we chose X over Y)</p> </li> </ul>"}, {"location": "onboarding/how-to-write-documentation/#writing-tips", "title": "\ud83d\udccc Writing Tips:", "text": "<ul> <li> <p>Remember that you are writing for managers, and they do not have technical background</p> </li> <li> <p>Write in plain language (no code or internal jargon)</p> </li> <li> <p>Use diagrams to illustrate concepts</p> </li> <li> <p>Keep paragraphs short and focused</p> </li> <li> <p>Link to relevant technical pages if needed</p> </li> </ul>"}, {"location": "onboarding/how-to-write-documentation/#2-technical-documentation", "title": "2. \ud83e\uddf1 Technical Documentation", "text": "<p>Audience: Engineers, DevOps, data scientists Goal: Explain  how  the system works and  how to maintain it</p> <p>Audience: Data Scientists, Developers, Maintainers, Reviewers Goal: Explain the internal code, architecture, and technical decisions. Show how the system  works and  how to maintain it</p>"}, {"location": "onboarding/how-to-write-documentation/#include_1", "title": "\u2705 Include:", "text": "<ul> <li> <p>Detailed Code Descriptions (and docstrings)</p> </li> <li> <p>Code architecture diagrams  and internal design</p> </li> <li> <p>Model training/inference pipelines</p> </li> <li> <p>Data flow diagrams and structure</p> </li> <li> <p>Dependency explanations (e.g., HuggingFace, FAISS, etc.)</p> </li> <li> <p>Version control setup (Git, DVC, etc.)</p> </li> <li> <p>Model registry and data lineage</p> </li> <li> <p>APIs and services if applicable </p> </li> <li> <p>CI/CD setup if applicable </p> </li> </ul>"}, {"location": "onboarding/how-to-write-documentation/#writing-tips_1", "title": "\ud83d\udccc Writing Tips:", "text": "<ul> <li> <p>Be precise and complete</p> </li> <li> <p>Always include file paths, commands, and examples</p> </li> <li> <p>Document default values, configs, and edge cases</p> </li> <li> <p>Keep it up-to-date with the code!</p> </li> </ul>"}, {"location": "onboarding/how-to-write-documentation/#3-user-manuals", "title": "3. \ud83e\uddd1\u200d\ud83d\udcbb User Manuals", "text": "<p>Audience: Internal users, DevOps, Backend/ Frontend devloeprs, QAs, Reviewers Goal: Help users run and use the system without needing to understand or read the code  </p> <p>This section should not include technical internals. It's about giving users the tools and steps to get things  done.</p>"}, {"location": "onboarding/how-to-write-documentation/#include_2", "title": "\u2705 Include:", "text": "<ul> <li> <p>How to install and set up the environment  </p> </li> <li> <p>How to run pipelines, scripts, or workflows  </p> </li> <li> <p>Expected inputs/outputs (e.g., CSV formats, samll table reperenting sample input/output)  </p> </li> <li> <p>Command-line examples  </p> </li> <li> <p>Where to find results and logs  </p> </li> <li> <p>Troubleshooting and FAQ</p> </li> </ul>"}, {"location": "onboarding/how-to-write-documentation/#tips", "title": "\ud83d\udccc Tips:", "text": "<ul> <li> <p>Be step-by-step and beginner-friendly</p> </li> <li> <p>Use real commands that users can copy-paste</p> </li> <li> <p>Include example inputs and outputs</p> </li> <li> <p>Add screenshots if helpful</p> </li> </ul>"}, {"location": "onboarding/how-to-write-documentation/#4-experiments", "title": "4. \ud83e\uddea Experiments", "text": "<p>Audience: Team members running ML experiments, Team Leads Purpose: Track ML experiments with results, metrics, and decisions</p> <p>Each experiment should be documented in a clear, repeatable format.</p>"}, {"location": "onboarding/how-to-write-documentation/#include_3", "title": "\u2705 Include:", "text": "<ul> <li>The purpose of experiment  </li> <li>Dataset &amp; code version  </li> <li>Version of submmodules (for example which cleansing and translation strategy was used)</li> <li>Hyperparameters  </li> <li>Evaluation metrics  </li> <li>Observations and conclusions  </li> <li>Next steps</li> <li>reference to MLflow webpage  </li> </ul>"}, {"location": "onboarding/how-to-write-documentation/#tips_1", "title": "\ud83d\udccc Tips:", "text": "<ul> <li>Use a consistent format across all experiment logs  </li> <li>Auto-export from MLflow if possible  </li> <li>Include tables for easy comparison</li> </ul> <p>\ud83e\udde0 REMEMBER: - Business Docs = What &amp; Why - User Manuals = How to Use - Technical Docs = How It Works - Experiments = What We Tried</p> <p>Find detailed markdown tutorial here.</p>"}, {"location": "onboarding/markdown-guide/", "title": "\ud83e\uddfe Markdown for MkDocs: The Complete Developer Guide", "text": "<p>This tutorial teaches you everything you need to know to write Markdown for our MkDocs-based documentation  platform, including:</p> <ul> <li>Basic markdown syntax</li> <li>Advanced formatting (tables, callouts, tabs)</li> <li>Using code blocks for YAML, JSON, bash, etc.</li> <li>Creating diagrams and admonitions</li> <li>Tips specific to Material for MkDocs (icons, collapsible blocks, tooltips)</li> </ul> <p>Easy in Brower Markdown Editor</p>"}, {"location": "onboarding/markdown-guide/#1-basic-markdown", "title": "1. \ud83d\udd24 Basic Markdown", "text": ""}, {"location": "onboarding/markdown-guide/#headings", "title": "Headings", "text": "<pre><code># H1 \u2013 Page Title\n## H2 \u2013 Section Title\n### H3 \u2013 Subsection\n</code></pre> <p>How it looks</p>"}, {"location": "onboarding/markdown-guide/#h1-page-title", "title": "H1 \u2013 Page Title", "text": ""}, {"location": "onboarding/markdown-guide/#h2-section-title", "title": "H2 \u2013 Section Title", "text": ""}, {"location": "onboarding/markdown-guide/#h3-subsection", "title": "H3 \u2013 Subsection", "text": ""}, {"location": "onboarding/markdown-guide/#text-styles", "title": "Text styles", "text": "<pre><code>*italic*, **bold**, `inline code`, ~~strikethrough~~\n</code></pre> <p>How it looks</p> <p>italic, bold, <code>inline code</code>, strikethrough</p>"}, {"location": "onboarding/markdown-guide/#lists", "title": "Lists", "text": "<pre><code>- Bullet list item\n  - Nested item\n\n1. Numbered item\n2. Another\n</code></pre> <p>How it looks</p> <ul> <li>Bullet list item</li> <li> <p>Nested item</p> </li> <li> <p>Numbered item</p> </li> <li>Another</li> </ul>"}, {"location": "onboarding/markdown-guide/#links-and-images", "title": "Links and images", "text": "<pre><code>[Link text](https://github.com/MindwiseLLC)\n![Alt text](img/image.png)\n</code></pre> <p>How it looks</p> <p>Link text </p>"}, {"location": "onboarding/markdown-guide/#2-code-blocks", "title": "2. \ud83d\udd27 Code Blocks", "text": "<p>Use triple backticks with the language name:</p> <pre><code>```python\nprint(\"Hello, world!\")\n```\n</code></pre> <p>Supported languages: <code>bash</code>, <code>python</code>, <code>yaml</code>, <code>json</code>, <code>sql</code>, etc.</p> <p>Add titles and line numbers: <pre><code>```python title=\"main.py\" linenums=\"1\"\ndef add(x, y):\n    return x + y\n```\n</code></pre></p> <p>How it looks</p> <pre><code>print(\"Hello, world!\")\n</code></pre> main.py<pre><code>def add(x, y):\n    return x + y\n</code></pre>"}, {"location": "onboarding/markdown-guide/#3-tables", "title": "3. \ud83d\udcca Tables", "text": "<pre><code>| Name     | Type     | Description          |\n|----------|----------|----------------------|\n| `name`   | `string` | The name of product  |\n| `count`  | `int`    | Quantity in stock    |\n</code></pre> <p>How it looks</p> Name Type Description <code>name</code> <code>string</code> The name of product <code>count</code> <code>int</code> Quantity in stock"}, {"location": "onboarding/markdown-guide/#4-admonitions-callouts", "title": "4. \ud83d\udce6 Admonitions (Callouts)", "text": "<p>Admonitions, also known as call-outs, are an excellent choice for including side content without significantly  interrupting the document flow. </p> <p>Use <code>!!!</code> to highlight notes, warnings, tips, etc.</p> <pre><code>!!! note\n    This is a simple note.\n\n!!! warning \"Be careful\"\n    This will highlight a warning with a custom title.\n</code></pre>"}, {"location": "onboarding/markdown-guide/#supported-types", "title": "Supported Types:", "text": "<ul> <li><code>note</code></li> <li><code>info</code></li> <li><code>tip</code></li> <li><code>success</code></li> <li><code>warning</code></li> <li><code>danger</code></li> <li><code>example</code></li> </ul> <p>You can configure icons in <code>mkdocs.yml</code>:</p> <pre><code>theme:\n  icon:\n    admonition:\n      note: octicons/tag-16\n      tip: octicons/squirrel-16\n</code></pre> <p>How it looks</p> <p>Note</p> <p>This is a simple note.</p> <p>Be careful</p> <p>This will highlight a warning with a custom title.</p>"}, {"location": "onboarding/markdown-guide/#5-tabs-and-collapsibles", "title": "5. \ud83e\uddea Tabs and Collapsibles", "text": ""}, {"location": "onboarding/markdown-guide/#tabs", "title": "Tabs:", "text": "<pre><code>=== \"Python\"\n    ```python\n    print(\"Hello\")\n    ```\n\n=== \"Bash\"\n    ```bash\n    echo \"Hello\"\n    ```\n</code></pre> <p>How it looks</p> PythonBash <pre><code>print(\"Hello\")\n</code></pre> <pre><code>echo \"Hello\"\n</code></pre>"}, {"location": "onboarding/markdown-guide/#collapsible-blocks", "title": "Collapsible blocks:", "text": "<pre><code>??? example \"Click to expand\"\n    This content is hidden until clicked.\n</code></pre> <p>How it looks</p> Click to expand <p>This content is hidden until clicked.</p>"}, {"location": "onboarding/markdown-guide/#6-diagrams-with-mermaid", "title": "6. \ud83e\uddec Diagrams with Mermaid", "text": "<p>Materials Tutorial on Mermaid Mermaid Official webpge and Documentation</p> <p><pre><code>```mermaid\ngraph TD\n  A[User] --&gt; B[Backend]\n  B --&gt; C[Database]\n```\n</code></pre> Enable with: <pre><code>```yaml\nmarkdown_extensions:\n  - pymdownx.superfences\n  - pymdownx.tabbed\n  - pymdownx.details\n  - pymdownx.snippets\n  - pymdownx.highlight\n  - attr_list\n  - admonition\n  - pymdownx.arithmatex\n  - pymdownx.emoji\n  - pymdownx.tasklist\n```\n</code></pre></p> <p>How it looks</p> <pre><code>graph TD\n  A[User] --&gt; B[Backend]\n  B --&gt; C[Database]</code></pre> <p>Coding Workflow</p> <pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre>"}, {"location": "onboarding/markdown-guide/#7-extra-tips", "title": "7. \ud83e\uddf0 Extra Tips", "text": ""}, {"location": "onboarding/markdown-guide/#add-badges", "title": "Add badges", "text": "<pre><code>![Build](https://img.shields.io/github/actions/workflow/status/user/repo/ci.yml?branch=main)\n</code></pre> <p>How it looks</p> <p></p>"}, {"location": "onboarding/markdown-guide/#add-horizontal-lines", "title": "Add horizontal lines", "text": "<pre><code>---\n</code></pre>"}, {"location": "onboarding/markdown-guide/#add-keyboard-keys", "title": "Add keyboard keys", "text": "<pre><code>++ctrl+alt+del++\n</code></pre> <p>How it looks</p> <p>Ctrl+Alt+Del</p>"}, {"location": "onboarding/ml-environment/", "title": "\ud83d\udda5\ufe0f ML Environment Setup Tutorial", "text": "<p>To contribute to any ML project, you must set up your environment with access to our internal services.</p>"}, {"location": "onboarding/ml-environment/#please-communicate-with-your-administrator", "title": "\ud83d\udce2 Please communicate with your administrator.", "text": ""}, {"location": "onboarding/ml-environment/#to-use-all-the-tools-you-will-need", "title": "To use all the tools you will need", "text": "<ul> <li> Access to our organisational Github</li> <li> VPN credentials for acces to the server</li> <li> MLFlow credentials for access to the unified MLFlow website</li> <li> Server credentials for DVC usage</li> </ul>"}, {"location": "onboarding/ml-environment/#connect-to-github-create-your-branch-start-working", "title": "\ud83d\udd17 Connect to GitHub, Create your branch, Start working", "text": "<pre><code>git clone ghttps://github.com/MindwiseLLC/Your-Project-Name.git\ncd project\ngit checkout -b feature/your-branch\n</code></pre>"}, {"location": "onboarding/ml-environment/#use-mlflow-for-unified-experiment-tracking", "title": "\ud83d\udcca Use MLflow for Unified Experiment Tracking", "text": "<p>Receive VPN credentials from you administrator. </p> <p>Receive MLFlow credentials from you administrator. </p> <p>MLflow is already running on our server: <code>http://mlflow.daniam.am</code></p> <p>Set tracking URI:</p> <pre><code>import mlflow\nmlflow.set_tracking_uri(\"http://mlflow.daniam.am\")\nmlflow.set_experiment(\"product-matching\")\n</code></pre> <p>Find detailed Mlflow tutorial and workflow here.</p>"}, {"location": "onboarding/ml-environment/#use-dvc-with-internal-remote", "title": "\ud83d\udce6 Use DVC with Internal Remote", "text": "<p>DVC is pre-configured on the server. </p> <p>You will use following address to as your remote.</p> <pre><code>ssh://aiml@192.168.150.222:/mnt/sdd/git_data/dvc_storage\n</code></pre> <p>Find detailed DVC tutorial and workflow here.</p>"}, {"location": "onboarding/ml-environment/#edit-and-build-documentation-locally", "title": "\ud83e\uddfe Edit and Build Documentation Locally", "text": "<pre><code>pip install mkdocs\nmkdocs new my-project \ncd my-project\n</code></pre> <p>Edit <code>docs/</code> markdown files, then:</p> <pre><code>mkdocs serve  # preview at http://127.0.0.1:8000\n</code></pre> <p>Find our how to add your local documentation to our Unified Documentation system here</p>"}, {"location": "onboarding/ml-environment/#your-first-experiment-checklist", "title": "\ud83e\uddea Your First Experiment Checklist", "text": "<ul> <li> Create a new Git branch: <code>experiment/lstm-v1</code> or new github repository</li> <li> Run your experiment, track wiht internal MLflow </li> <li> Add data via DVC</li> <li> Commit everything to GitHub with references to MLflow run ID</li> <li> Create/update related documentation (manuals or technical)</li> <li> Add your local docs to Unified Documentation Hub</li> </ul> <p>\ud83d\udccc Remember: No result exists until it is reproducible via:</p> <ul> <li>A Git commit</li> <li>MLflow run ID</li> <li>DVC-tracked dataset </li> <li>Documentation</li> </ul>"}, {"location": "onboarding/ml-team-workflow/", "title": "\ud83d\ude80 ML/AI Team General Workflow Guide", "text": "<p>This guide describes how we work as an ML/AI team \u2014 how we plan, document, track, and version everything we do.  Every team member must follow these principles to ensure transparency, reproducibility, and collaborative  efficiency.</p>"}, {"location": "onboarding/ml-team-workflow/#our-workflow-philosophy", "title": "\ud83d\udccc Our Workflow Philosophy", "text": "<p>We operate on three foundational principles:</p> <ol> <li>Document every decision and change</li> <li>Track every experiment</li> <li>Version everything (code, data, models)</li> </ol> <p>This ensures that any result we present can be reproduced and understood weeks or months later.</p>"}, {"location": "onboarding/ml-team-workflow/#general-ml-project-workflow", "title": "\ud83d\udee0\ufe0f General ML Project Workflow", "text": "<pre><code>graph TD\n  A[Define Business Problem] --&gt; B[Write Business Documentation]\n  B --&gt; C[Design Data/Model Strategy]\n  C --&gt; D[Set Up Tracking: MLflow + Git + DVC]\n  D --&gt; E[Implement Code and Train Models]\n  D --&gt; F[Track Experiment in MLflow]\n  D --&gt; N[Docuement Experiments, Docuemnt Code]\n  E --&gt; G[Push Code, Metrics, and Docs to GitHub]\n  F --&gt; G[Push Code, Metrics, and Docs to GitHub]\n  N --&gt; G[Push Code, Metrics, and Docs to GitHub]\n  G --&gt; H[Write User Manual if Needed, Update Business Docs]\n  H --&gt; I[Review, Refactor, Repeat]</code></pre>"}, {"location": "onboarding/ml-team-workflow/#what-you-must-document", "title": "\ud83e\uddfe What You Must Document", "text": "Step Where to Document Problem Definition Business Documentation Data Sources + Preprocessing Technical Documentation Model Architecture Technical Documentation How to Run/Train User Manuals Experiment Logs and Results Experiments"}, {"location": "onboarding/ml-team-workflow/#experiments-must-be", "title": "\ud83e\uddea Experiments Must Be:", "text": "<ul> <li>Tracked with MLflow</li> <li>Logged in Markdown and added to centrilized documentation</li> <li>Tagged in GitHub (e.g., <code>exp-2025-06-04-xgb-vs-lstm</code>)</li> <li>Pushed to remote as soon as an experiment finishes </li> <li>You should have intermidary pushes as well, if the experiment takes too long, has too many code changes, and  consists of multiple steps.</li> </ul>"}, {"location": "onboarding/ml-team-workflow/#version-control-rules", "title": "\ud83e\uddec Version Control Rules", "text": "<ul> <li>Use Git branches: <code>feature/</code>, <code>experiment/</code>, <code>fix/</code></li> <li>Create Git tags for major experiments</li> <li>Use DVC for versioning datasets and model binaries</li> </ul>"}, {"location": "onboarding/ml-team-workflow/#documentation-is-not-optional", "title": "\ud83d\udcd8 Documentation is Not Optional!", "text": "<p>Every new script, major change, or result must be accompanied by a doc or edit to an existing doc.</p> <ul> <li>Add code explanations to <code>technical/</code></li> <li>Add guides to <code>manuals/</code></li> <li>Link all relevant Markdown pages in centrilized docuemntation hub</li> </ul>"}, {"location": "onboarding/mlflow-guide/", "title": "\ud83d\udcca MLflow Usage Tutorial (Internal Guide)", "text": "<p>This guide provides a complete walkthrough of how to use MLflow for experiment tracking in our internal ML infrastructure. It covers:</p> <ul> <li>How to connect to our internal MLflow server</li> <li>How to track experiments during and after training</li> <li>How to track classic ML, Hugging Face transformers, and inference-only use cases</li> <li>How to log custom preprocessing code and comments as artifacts</li> <li>Understanding the MLflow UI</li> <li>Markdown and Jupyter Notebook examples provided</li> </ul>"}, {"location": "onboarding/mlflow-guide/#1-connecting-to-internal-mlflow-server", "title": "\ud83d\udd10 1. Connecting to Internal MLflow Server", "text": "<p>All tracking is done on our central MLflow instance (ask your lead for access credentials). You should be connected to VPN.</p> <pre><code># \u2500\u2500 MLflow connection \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nimport os, mlflow\nMLFLOW_TRACKING_URI = \"http://mlflow.daniam.am\"\nmlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n\nos.environ[\"MLFLOW_TRACKING_USERNAME\"] = \"your_username\"  \nos.environ[\"MLFLOW_TRACKING_PASSWORD\"] = \"your_password\"\n\nmlflow.set_experiment(\"Your-Experiment-Name\")\n</code></pre>"}, {"location": "onboarding/mlflow-guide/#2-tracking-after-training", "title": "\ud83d\udfe8 2. Tracking After Training", "text": "<p>If you forgot to use MLflow during training, log all relevant outputs after the experiment has completed:</p> <p><pre><code># train normally\nmodel = train_model(...)\nmetrics = evaluate(model)\n\n# later: open or create run\n\nrun = mlflow.start_run(run_name=\"descriptive-run-name\")   \nmlflow.log_metric(\"f1\", metrics[\"f1\"])\nmlflow.sklearn.log_model(model, \"model\") \n\nmlflow.end_run()\n</code></pre> OR <pre><code># Assume results already saved locally\nwith mlflow.start_run(run_name=\"posthoc-run\"):\n    mlflow.log_param(\"data_split\", \"2024-week-18\")\n    mlflow.log_metric(\"f1_score\", 0.821)\n    mlflow.log_artifact(\"configs/final_config.yaml\")\n    mlflow.log_artifact(\"plots/confusion_matrix.png\")\n    mlflow.set_tag(\"note\", \"Logged after experiment completion\")\n</code></pre></p> <p>If you forgot to add something to your run find the run-id in Mlflow website and restart the run </p> <pre><code>mlflow.start_run(run_id=\"abc123\")   # resumes that run\n# add everything you forgot\nmlflow.end_run()\n</code></pre>"}, {"location": "onboarding/mlflow-guide/#3-tracking-during-run", "title": "\ud83d\udfe93. Tracking During Run", "text": "<pre><code># Start an MLflow run\n\nwith mlflow.start_run():\n\n    # Log parameters\n\n    learning_rate = 0.01\n\n    epochs = 10\n\n    random_state = 42\n\n\n\n    mlflow.log_param(\"learning_rate\", learning_rate)\n\n    mlflow.log_param(\"epochs\", epochs)\n\n    mlflow.log_param(\"random_state\", random_state)\n\n    # Load data\n\n    iris = load_iris()\n\n    X, y = iris.data, iris.target\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n\n\n\n    # Train a classic ML model (Logistic Regression)\n\n    model = LogisticRegression(solver='liblinear', random_state=random_state, max_iter=epochs)\n\n\n\n    # Simulate training with intermediate logging\n\n    for epoch in range(epochs):\n\n        model.fit(X_train, y_train)\n\n        y_pred = model.predict(X_test)\n\n        accuracy = accuracy_score(y_test, y_pred)\n\n        precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n\n        recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n\n        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n\n\n\n        # Log metrics for each epoch\n\n        mlflow.log_metric(\"accuracy\", accuracy, step=epoch)\n\n        mlflow.log_metric(\"precision\", precision, step=epoch)\n\n        mlflow.log_metric(\"recall\", recall, step=epoch)\n\n        mlflow.log_metric(\"f1_score\", f1, step=epoch)\n\n\n\n        print(f\"Epoch {epoch+1}/{epochs}: Accuracy={accuracy:.4f}\")\n\n\n\n    # Log the final model\n\n    mlflow.sklearn.log_model(model, \"logistic_regression_model\")\n\n\n\n    # Log a tag for this run\n\n    mlflow.set_tag(\"model_type\", \"LogisticRegression\")\n\n    mlflow.set_tag(\"data_source\", \"Iris Dataset\")\n\n\n\n    print(f\"MLflow Run ID: {mlflow.active_run().info.run_id}\")\n</code></pre>"}, {"location": "onboarding/mlflow-guide/#4-logging-classic-ml-models", "title": "\u2699\ufe0f 4. Logging Classic ML Models", "text": "<pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.datasets import load_iris\nfrom sklearn.metrics import accuracy_score\n\nX, y = load_iris(return_X_y=True)\nmodel = LogisticRegression().fit(X, y)\npreds = model.predict(X)\n\nwith mlflow.start_run():\n    mlflow.log_metric(\"accuracy\", accuracy_score(y, preds))\n    mlflow.sklearn.log_model(model, \"model\")\n</code></pre>"}, {"location": "onboarding/mlflow-guide/#5-logging-hugging-face-transformer-models", "title": "\ud83e\udd17 5. Logging Hugging Face Transformer Models", "text": ""}, {"location": "onboarding/mlflow-guide/#when-training-your-own", "title": "When Training Your Own", "text": "<pre><code>from transformers import Trainer\n\ntrainer = Trainer(...)\ntrainer.train()\n\nmlflow.pytorch.log_model(trainer.model, \"hf_model\")\nmlflow.log_param(\"transformer_name\", \"bert-base-multilingual-cased\")\n</code></pre>"}, {"location": "onboarding/mlflow-guide/#when-using-pretrained-only", "title": "When Using Pretrained Only", "text": "<p>Run inference code <pre><code>from transformers import pipeline\n\n# Define model and task\nmodel_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n\nexample_input = \"MLflow is an amazing tool for experiment tracking!\"\nexample_output = nlp_pipeline(example_input)\n</code></pre></p> <p>Log resulsts</p> <pre><code># Start an MLflow run\n\nwith mlflow.start_run(run_name=\"Sentiment Analysis with Pre-trained Model after logging\"):\n\n    # Log model name as a parameter\n\n    mlflow.log_param(\"hf_model_name\", model_name)\n\n    mlflow.log_param(\"hf_task\", \"sentiment-analysis\")\n\n    mlflow.log_param(\"example_input\", example_input)\n\n    mlflow.log_param(\"example_output\", example_output[0][\"label\"])\n\n    mlflow.log_param(\"example_output_score\", example_output[0][\"score\"])\n\n    # Log a tag\n\n    mlflow.set_tag(\"usage_type\", \"pre-trained_inference\")\n\n    mlflow.set_tag(\"library\", \"HuggingFace Transformers\")\n</code></pre>"}, {"location": "onboarding/mlflow-guide/#6-logging-preprocessing-code-notes", "title": "\ud83e\uddf1 6. Logging Preprocessing Code + Notes", "text": ""}, {"location": "onboarding/mlflow-guide/#a-save-preprocessing-snippet-from-notebook", "title": "a) Save preprocessing snippet from notebook", "text": "<pre><code>with open(\"cleaning_v3.py\", \"w\") as f:\n    f.write(cleaning_function_code)\nmlflow.log_artifact(\"cleaning_v3.py\")\n</code></pre>"}, {"location": "onboarding/mlflow-guide/#b-log-preprocessing-method-comment", "title": "b) Log preprocessing method comment", "text": "<pre><code>mlflow.set_tag(\"translation_type\", \"word-by-word\")\nmlflow.set_tag(\"note\", \"Used aggressive stopword filtering\")\n</code></pre>"}, {"location": "onboarding/mlflow-guide/#6-understanding-mlflow-ui", "title": "\ud83e\udded 6. Understanding MLflow UI", "text": "<p>Visit http://mlflow.daniam.am</p> Section What You See Experiments List of experiments tracked Runs All executions (filter by name, tag, etc.) Metrics Plot training progress Parameters Show hyperparameters used Artifacts Code, configs, logs, visualizations Tags Notes and context (e.g., model type)"}, {"location": "onboarding/mlflow-guide/#best-practices", "title": "\ud83e\uddea Best Practices", "text": "<ul> <li>Always use a <code>with mlflow.start_run():</code> block</li> <li>Use descriptive run names and tags</li> <li>Log code versions and configs as artifacts</li> <li>Don\u2019t forget to <code>push</code> your code and docs to GitHub afterward</li> </ul>"}, {"location": "test/docs/", "title": "Welcome to the test page of Danaiam docuentation", "text": ""}, {"location": "test/docs/business/", "title": "First Business Docuimentation of Ml/AI team", "text": ""}, {"location": "test/docs/technical/", "title": "First technical documetnation of Daniam LLC", "text": ""}, {"location": "test/docs/user_manual/", "title": "First user manual of Daniam", "text": ""}, {"location": "nlp-pipeline/readme/", "title": "Daniam-NLP Project", "text": ""}, {"location": "nlp-pipeline/readme/#project-overview", "title": "Project Overview", "text": "<p>This project is designed to process, cleanse, and extract relevant information from data files. The pipeline consists of various NLP (Natural Language Processing) tasks, including Armenian cleansing, translation, English cleansing, category extraction, brand name and unit extraction. The data files are primarily in <code>.xlsx</code> and <code>.csv</code> format, and the project is capable of handling batch processing for efficient data management.</p>"}, {"location": "nlp-pipeline/readme/#features", "title": "Features", "text": "<p>The project implements the following key steps in its pipeline: - ArmenianCleansing: Cleansing Armenian data. - Translation: Translating Armenian data to English. - EnglishCleansing: Cleansing English data. - CategoryExtraction: Extracting category-related information from the data. - BrandExtraction: Extracting brand names. - UnitMeasureExtraction: Extracting unit measures from the data.</p> <p>The pipeline is highly configurable, and different processes can be enabled or disabled using a configuration file (<code>config.json</code>).</p>"}, {"location": "nlp-pipeline/readme/#data-sources", "title": "Data Sources", "text": "<p>The following data files are required for the pipeline: - <code>Abbreviation.xlsx</code> - <code>CityArmenian.xlsx</code> - <code>CityEnglish.xlsx</code> - <code>SasArmenian.xlsx</code> - <code>SasEnglish.xlsx</code> - <code>grouped_data.csv</code></p> <p>These files should be placed in the appropriate directories as referenced in the <code>config.json</code>.</p>"}, {"location": "nlp-pipeline/readme/#configuration", "title": "Configuration", "text": "<p>The <code>config.json</code> file contains the paths to the input data and configurable parameters for the pipeline. Key sections include:</p> <ul> <li>Data: Specifies paths to input data files.</li> <li>Params: Controls settings like <code>n_jobs</code> for parallel processing and <code>batch_size</code> for handling batch operations.</li> <li>Pipeline: Enables or disables specific steps in the processing pipeline.</li> </ul>"}, {"location": "nlp-pipeline/readme/#installation", "title": "Installation", "text": "<p>Follow these steps to set up the project on your local machine:</p> <ol> <li>Clone the repository:</li> </ol> <p>```bash    git clone https://github.com/your-repo/daniam-nlp.git 2. Ensure Python 3.11 is installed:</p> <p>This project requires Python 3.11. You can download and install it from the official Python website: https://www.python.org/downloads/.</p> <ol> <li>Create a virtual environment</li> </ol> <p>After installing Python 3.11, create a virtual environment for the project:</p>"}, {"location": "nlp-pipeline/readme/#for-windows", "title": "For Windows:", "text": "<pre><code>python -m venv nlp-env\nnlp-env/Scripts/activate\n</code></pre>"}, {"location": "nlp-pipeline/readme/#for-macoslinux", "title": "For macOS/Linux:", "text": "<p><pre><code>python3.11 -m venv nlp-env\nsource nlp-env/bin/activate\n</code></pre> 4. Installation and Activation of CUDA 11.8 with PyTorch 2.4.1</p> <p>This guide provides step-by-step instructions to install and activate CUDA 11.8 along with PyTorch 2.4.1 on both Windows and macOS/Linux systems.</p>"}, {"location": "nlp-pipeline/readme/#for-windows_1", "title": "For Windows", "text": ""}, {"location": "nlp-pipeline/readme/#prerequisites", "title": "Prerequisites", "text": "<ol> <li>Ensure you have an NVIDIA GPU that supports CUDA 11.8. Check GPU compatibility on the CUDA GPUs page.</li> <li>Install the latest NVIDIA drivers from NVIDIA's driver page.</li> <li>Install Python 3.8 or later. Use the Python website or Anaconda (Anaconda website).</li> </ol>"}, {"location": "nlp-pipeline/readme/#installation-steps", "title": "Installation Steps", "text": "<ol> <li>Install CUDA 11.8 Toolkit:</li> <li>Download the CUDA 11.8 Toolkit from NVIDIA's CUDA Toolkit archive.</li> <li> <p>Follow the installation wizard, ensuring the PATH variables are set correctly.</p> </li> <li> <p>Install cuDNN Library:</p> </li> <li>Download the cuDNN library for CUDA 11.8 from the cuDNN page.</li> <li> <p>Extract the archive and copy the contents into the CUDA installation directory (e.g., <code>C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v11.8</code>).</p> </li> <li> <p>Install PyTorch 2.4.1:    Open a terminal (or Anaconda Prompt) and run:    <pre><code>pip install torch==2.4.1+cu118 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n</code></pre></p> </li> <li> <p>Verify Installation:    Launch Python and verify CUDA compatibility:    <pre><code>import torch\nprint(torch.__version__)\nprint(torch.cuda.is_available())\n</code></pre>    Ensure the version is <code>2.4.1</code> and <code>True</code> is printed for CUDA availability.</p> </li> <li> <p>Install dependencies</p> </li> </ol> <p>After activating the virtual environment, install the required dependencies:</p> <p><pre><code>pip install -r requirements.txt\n</code></pre> 6. Run the main script</p> <p>First, navigate to the 'code' directory using the command:</p> <pre><code>cd code\n</code></pre> <p>To run the main script, execute the following command:</p> <p><pre><code>python main.py\n</code></pre> This will execute the pipeline according to the settings defined in config.json.</p> <p>Note: Using this script you can process all HDM, Inovice and Declarations data received from SRC.  Here is the specific requiremnts that you nee to change in the Config.json file to ensure that scripts works properly.  - HDM data: For Hdm data in the Pipeline part of the <code>Config.json</code> file set the <code>DataSource</code> parameter to <code>'HDM'</code>. - Invoice data: For Hdm data in the Pipeline part of the <code>Config.json</code> file set the <code>DataSource</code> parameter to <code>'Inv'</code>. - Declaration data: For Hdm data in the Pipeline part of the <code>Config.json</code> file set the <code>DataSource</code> parameter to <code>'Dec'</code>,  set the <code>UnitMeasureExtraction</code> parameter to <code>false</code>.</p>"}, {"location": "nlp-pipeline/readme/#classes", "title": "Classes", "text": ""}, {"location": "nlp-pipeline/readme/#1-brandextractor", "title": "1. <code>BrandExtractor</code>", "text": ""}, {"location": "nlp-pipeline/readme/#responsibilities", "title": "Responsibilities", "text": "<ul> <li>Pattern Matching: Extracts values enclosed in specific patterns.</li> <li>Fuzzy Matching: Matches product names with brands using <code>fuzzywuzzy</code>.</li> <li>Transliteration: Converts Armenian text to English using <code>unidecode</code>.</li> <li>Row Processing: Extracts relevant values for each row in a dataset.</li> <li>Parallel Processing: Processes large datasets in chunks using multiprocessing.</li> </ul>"}, {"location": "nlp-pipeline/readme/#methods", "title": "Methods", "text": "<ul> <li><code>extract_enclosed_values(text)</code>: Extracts values enclosed in predefined patterns.</li> <li><code>fuzzy_match_brands(product_name, all_brands_df, category, final_score, threshold=76)</code>: Matches product names with reference brands.</li> <li><code>extract_english_in_armenian(text)</code>: Extracts embedded English words in Armenian text.</li> <li><code>process_row(row, all_brands_df)</code>: Processes a single row to extract values.</li> <li><code>apply_parallel_extraction(df, all_brands_df, num_workers=8, chunk_size=1000)</code>: Applies extraction logic in parallel.</li> <li><code>get_extracted_brands(data, all_brand_list)</code>: Extracts brands and adds them to the dataset.</li> </ul>"}, {"location": "nlp-pipeline/readme/#2-brandprocessor", "title": "2. <code>BrandProcessor</code>", "text": ""}, {"location": "nlp-pipeline/readme/#responsibilities_1", "title": "Responsibilities", "text": "<ul> <li>Brand Finalization: Processes extracted values to identify the final brand.</li> <li>Preprocessing: Cleans and standardizes brand names.</li> <li>Matching Logic: Matches brand names against reference data with configurable thresholds.</li> </ul>"}, {"location": "nlp-pipeline/readme/#methods_1", "title": "Methods", "text": "<ul> <li><code>transliterate_armenian_to_english(text)</code>: Converts Armenian text to English.</li> <li><code>process_extracted_values(row)</code>: Processes extracted values to determine the final brand.</li> <li><code>preprocess(brand)</code>: Cleans and preprocesses brand names.</li> <li><code>match(brand, sub_category=None, final_score=None)</code>: Matches a single brand name.</li> <li><code>match_all(data)</code>: Matches all brand names in the dataset.</li> <li><code>get_final_brand(data)</code>: Adds finalized brands to the dataset.</li> <li><code>get_matched_brands(data)</code>: Adds matched brands and their scores to the dataset.</li> </ul>"}, {"location": "nlp-pipeline/readme/#workflow-diagram", "title": "Workflow Diagram", "text": "<p>Below is a sample placeholder image showing the workflow of the <code>BrandExtractor</code> class. Replace this with your actual diagram.</p> <p></p>"}, {"location": "nlp-pipeline/docs/business_brand/", "title": "Pipeline Description for Brand Extraction", "text": ""}, {"location": "nlp-pipeline/docs/business_brand/#pipeline-overview", "title": "Pipeline Overview", "text": "<p>This document provides a detailed description of the fuzzy matching pipeline, including each component's function,  data flow, and processing steps. This pipeline is structured to process a large dataset efficiently by applying text  extraction, transliteration, fuzzy matching, and parallelization techniques on product names.</p>"}, {"location": "nlp-pipeline/docs/business_brand/#loading-and-preprocessing-data", "title": "Loading and Preprocessing Data", "text": "<p>1. Load Data: The pipeline begins by loading the dataset containing product names and category information from  CSV files (e.g., 'data.csv'). An Excel file ('pairs.xlsx') is loaded as a reference data to hold scraped brand names  and categories for fuzzy matching. 2. Data Cleaning: Missing values are filled with empty strings, and duplicate rows are removed from the  DataFrame.</p>"}, {"location": "nlp-pipeline/docs/business_brand/#defining-patterns-for-enclosed-value-extraction", "title": "Defining Patterns for Enclosed Value Extraction", "text": "<p>A list of regular expression patterns is defined to identify and extract values enclosed within various symbols,  such as \u00ab\u00bb, ``, '', \\&lt;\\&lt;&gt;&gt;, etc. These patterns are passed as an argument to the processing function to identify  specific keywords or terms within product names.</p>"}, {"location": "nlp-pipeline/docs/business_brand/#brandextractor-class", "title": "BrandExtractor Class", "text": "<p>The `BrandExtractor` class contains the following methods:</p> <p>1. extract_enclosed_values: Uses the defined patterns to find and extract enclosed terms from a given  product name. It returns the first matched term. 2. fuzzy_match_brands: This function applies fuzzy matching to compare product names against brand names in  the DataFrame. It incorporates Armenian to English transliteration, text cleaning, and uses a fuzzy matching library  (RapidFuzz) based on conditions. The method returns brands with similarity scores above the threshold. 3. extract_english_in_armenian: Finds and extracts English characters embedded in Armenian text. This can  be helpful for identifying English terms in product names.</p>"}, {"location": "nlp-pipeline/docs/business_brand/#row-processing-function-process_row", "title": "Row Processing Function (process_row)", "text": "<p>The `process_row` function is defined outside of the class and applies the following operations on each row of  the DataFrame:</p> <p>1. Extract enclosed values in product names using `extract_enclosed_values`. 2. Perform fuzzy matching of product names with brand names in the category using         `fuzzy_match_brands`. 3. Extract English words embedded in Armenian text using `extract_english_in_armenian`.</p> <p>Results are returned as a tuple containing three lists: enclosed values, exact matches, and  English words.</p>"}, {"location": "nlp-pipeline/docs/business_brand/#parallel-processing-with-apply_parallel_extraction", "title": "Parallel Processing with apply_parallel_extraction", "text": "<p>The `apply_parallel_extraction` function manages parallel processing of the dataset:</p> <p>1. Chunking Data: The DataFrame is divided into chunks of rows, with each chunk processed separately. 2. Parallel Execution: Using `ProcessPoolExecutor`, the `process_row` function is applied in parallel to  each row in a chunk. This maximizes efficiency for large datasets by utilizing multiple CPU cores. 3. Collecting Results: Results from all chunks are collected and assembled into a single DataFrame. A new  column, 'combined', is created that stores each row\u2019s results in a tuple format containing enclosed values, exact  matches, and English words.</p>"}, {"location": "nlp-pipeline/docs/business_brand/#saving-results", "title": "Saving Results", "text": "<p>The final results are appended to the original dataset, and the combined results column is saved to a CSV file  ('fuzzy_output_fixed_full.csv'). Optionally, results can also be saved to an Excel file for further analysis.</p>"}, {"location": "nlp-pipeline/docs/business_brand/#pipeline-diagram", "title": "Pipeline Diagram", "text": "<p>The following flowchart-style outline represents the data flow and operations:</p> <p>1. Load Data  2. Define Patterns  3. BrandExtractor Class - a. extract_enclosed_values - b. fuzzy_match_brands - c. extract_english_in_armenian 4. Row Processing Function (process_row) 5. Parallel Processing with apply_parallel_extraction 6. Save Results</p>"}, {"location": "nlp-pipeline/docs/business_brand/#pipeline-description-for-brand-matching", "title": "Pipeline Description for Brand Matching", "text": ""}, {"location": "nlp-pipeline/docs/business_brand/#pipeline-overview_1", "title": "Pipeline Overview", "text": "<p>The BrandMatcher class is designed to efficiently match brand names from a dataset to a reference list of brands. It  also includes utilities for preprocessing brand names, handling extracted values, and applying advanced matching  logic. This documentation provides a detailed description of the class and its methods.</p>"}, {"location": "nlp-pipeline/docs/business_brand/#initialization-__init__", "title": "Initialization (__init__)", "text": "<p>The __init__ method initializes the BrandMatcher class with the following parameters:</p> <ul> <li>ref_brands(list): A list of reference brand names for matching.  </li> <li>ref_sub_categories (list): A list of subcategories corresponding to the reference brands.  </li> <li>unnecessary_words (list, optional): Words to be removed during preprocessing (default: None).  </li> <li>ratio (function, optional): A similarity scoring function from rapidfuzz (default: fuzz.token_set_ratio).  </li> <li>score_threshold (int, optional): A threshold score for determining additional matching criteria (default:  75).</li> </ul>"}, {"location": "nlp-pipeline/docs/business_brand/#preprocessing-preprocess", "title": "Preprocessing (preprocess)", "text": "<p>The preprocess method normalizes a brand name by:</p> <ul> <li>Converting it to lowercase.  </li> <li>Removing unnecessary words (defined during initialization).  </li> <li>Stripping extra spaces.</li> </ul> <p>Args:  - brand (str): The brand name to preprocess. Returns:  - str: The cleaned and normalized brand name.</p>"}, {"location": "nlp-pipeline/docs/business_brand/#processing-extracted-values-process_extracted_values", "title": "Processing Extracted Values (process_extracted_values)", "text": "<p>This method processes extracted values from a given row to determine the final brand name. It evaluates potential  brand candidates, applies specific rules to filter valid brands, and normalizes or corrects the results based on  predefined mappings.</p>"}, {"location": "nlp-pipeline/docs/business_brand/#key-steps", "title": "Key Steps:", "text": "<ol> <li>Input Handling:  </li> <li>Converts extracted_values from string to a list of lists.  </li> <li>Initializes a list of units of measurement to exclude irrelevant entries.  </li> <li>Brand Validation:  </li> <li>Checks if a value is a potential brand based on length and the absence of units.  </li> <li>Applies transliteration for Armenian or Cyrillic characters.  </li> <li>Brand Selection:  </li> <li>Prioritizes valid enclosed brands from extracted_values[0].  </li> <li>Falls back to matched brands (extracted_values[1]) no brand is identified from the first value.  </li> <li>If matched brands are also unavailable, extracted english names will be processed and selected  (extracted_values[2]).   </li> <li>Replacement Map:  </li> <li>Corrects common brand misidentifications using a predefined dictionary.  </li> <li>Output:  </li> <li>Returns the final brand name, normalized and corrected as needed.</li> </ol> <p>Matching (match)</p> <p>The match method performs a similarity-based match for a single brand against the reference list. It uses rapidfuzz  to calculate similarity scores and applies additional logic if the score exceeds a threshold.</p> <p>Args:  - brand (str): The brand name to match.  - sub_category (str, optional): The subcategory to refine matching (default: None).  - final_score (float, optional): Precomputed score for additional logic (default: None).</p> <p>Returns:  - tuple: A tuple containing the matched brand and its score.</p>"}, {"location": "nlp-pipeline/docs/business_brand/#batch-matching-match_all", "title": "Batch Matching (match_all)", "text": "<p>The match_all method processes a list of brands and applies the match method to each. Progress tracking is enabled  via tqdm for real-time feedback during execution.</p> <p>Args:  - brands (list): A list of brand names to match.  - sub_categories (list, optional): Corresponding subcategories for each brand (default: None).  - final_scores (list, optional): Precomputed scores for each brand (default: None).</p> <p>Returns:  - list: A list of tuples containing matched brands and their scores.</p>"}, {"location": "nlp-pipeline/docs/business_brand/#conclusion", "title": "Conclusion", "text": "<p>The BrandMatcher class provides a robust and extensible framework for matching brand names in large datasets. It  incorporates preprocessing utilities, advanced similarity scoring mechanisms, and the flexibility to handle  additional logic for complex matching scenarios. The inclusion of configurable parameters such as unnecessary words,  scoring functions, and thresholds makes the class highly adaptable to diverse use cases.</p> <p>Through efficient batch processing capabilities, the class is well-suited for handling high-volume datasets,  ensuring accurate and scalable brand matching solutions. Potential future enhancements may include additional  preprocessing techniques, support for multilingual datasets, and the integration of advanced similarity algorithms  to further improve performance and adaptability to various domains.</p> <p></p>"}, {"location": "nlp-pipeline/docs/business_categorization/", "title": "Product Categorization", "text": "<p>NLP based product categorization is an essential step for structuring tax receipt information and utilizing huge  potential hidden in that data. In order to achieve that several steps were implemented\u0589 Important: the variety of products in final consumption sales documents (HDM) is huge. Only in one supermarket  chain the unique products (product lines) sold over three month period were more than 50,000. Overall, to ensure  maximum benefit it was decided to start product categorization with the largest FMCG store chains (supermarkets).  According to our estimates these supermarkets cover 30 % of overall final consumption in the country, and more than  80% of essential products consumption.  Steps for product categorization already implemented: Scraping Getting alternative data with scraping based on selenium for having reference dataset from two main  supermarket  chains- sas and city. However, as their structure is a bit different we created a new column, Final Category, so  that the matching can be consistent.  In addition, despite product level information we scraped the brand names in Armenian, Russian, and English  languages for separately identifying brand names from the receipts alongside with the production country.  Data cleansing We implement basic cleansing on Armenian text from removing noise, such as removing specific words, product codes,  punctuation and special characters, values enclosed in parenthesis and mapping unit measures for consistency (both  for English and Armenian), such as g, gr, or grams to gram, etc. Translation Translating Armenian text into English for having larger opportunities to work with state-of-the art NLP algorithms.  We implement this step by checking two translation algorithms in case one of them is not working, the code will use  the second one.  Overall, this is quite a time consuming process even with the parallel processing, on average translating 180,000  unique product names takes approximately 3 hours.  Data cleansing Cleansing translated text for removing additional noise by removing digits, re-mapping unit measures, removing  additional spaces and replacing some stand alone letters, such as g-gram, l-litr. Main subject extraction  In order to get structured information from tax receipts it was decided to implement  transformer based sentence  similarity embeddings based on cosine similarity. Several different combinations of matching approaches were tested  and the final pipeline consists of three main steps.</p> <ol> <li>Matching cleaned-translated product name to cleaned products from supermarkets  </li> <li>Matching cleaned-translated product name to final categories  </li> <li>Comparing which one of the two approaches have higher cosine similarity score and for the final group choosing  that one. </li> </ol> <p>This approach not only enables a more general method for main topic identification but also enables filtering based  on the matching score; higher the score more certain is the model that products are similar.   Brand name and unit measure extraction Although product is one of the main subjects in the text unit measure and brand and product country is very crucial,  as a result this was separately handled. Unit Measure Extraction</p> <ul> <li>We have obtained a list of all the possible unit measure a product can have  </li> <li>Using python regular expressions we are retrieving the unit measure and its value from the product description  (note that a product might have more that 1 unit measures, example \u201csausage  10pcs 20gr\u201d we are extracting up to 4  unit measures from the product description (more doesn't make sense and is not present in the data)  </li> <li>Besides the unit measure we also extract the value itself for the example mentioned above the output will be  [(10, pcs), (20, gr)]</li> </ul> <p>Brand name and product country extraction Brand name matching is done by the following steps</p> <ul> <li>Extracting brand names enclosed within special characters (examples: [\u201cbrand_name\u201d, \u2018brand_name\u2019,  \\&lt;brand_name&gt;, etc. ])  </li> <li>Exact match for armenian brands list (the brand list is obtained by scraping supermarket data (city, sas))  </li> <li>Exact match for english brands list  </li> <li>Match armenian brands to English brands to obtain final brand name in english  </li> <li>Merge the final brand with the Origin Country data obtained from supermarket data scraping</li> </ul>"}, {"location": "nlp-pipeline/docs/business_categorization/#future-actions", "title": "Future Actions", "text": "<ul> <li>More targeted cleansing for removing adjectives, for example apple juice or apple cookies sometimes are matched  with apple instead of juice or cookie.   </li> <li>Manually double check matching dictionaries and remove misleading candidates and checking final categories  </li> <li>Separate approach for each category, such as rule based for fruits and vegetables or  some price filter for higher  categories, </li> </ul>"}, {"location": "nlp-pipeline/docs/business_unit_measure/", "title": "UNIT MEASURE EXTRACTION", "text": "<p>One of the initial steps of processing textual data from HDM checks is extraction of unit measures and their  respective value from the textual data. For this purpose unit measure extraction and unit measure standardisation  methods were implemented. </p>"}, {"location": "nlp-pipeline/docs/business_unit_measure/#unit-measure-extraction_1", "title": "Unit Measure Extraction", "text": "<p>Unit measure extraction consists of multiple parts. First the textual data is cleaned and transformed to make the  extraction process easy. Then the extraction happens with two steps.</p> <ol> <li>Firstly we extract unit measures using Qunatulum3 library, a tool for  extraction of unit measures. While Quantulum is well suited for extracting unit measure and numerical value pairs,  it is not effective for instances where numerical value is not specified.  </li> <li>Therefore for the next step we extract unit measures and their corresponding values using regular expressions.  Here we concentrate on extracting Unit Measures even if the numerical values are not attached.   </li> <li>As a complementary step we try to extract the size of the product if it is mentioned (this is a place for future  improvement). Example: paper towel 20 x 30 mm Extraction\u2192 20 x 30 mm   </li> <li>After completing the extraction steps we sort the extracted units by importance, assuming that weight and volume  indicating unit measures are the most important. This way at the end of this process we get multiple possible  options for unit measure which are sorted by the importance.</li> </ol>"}, {"location": "nlp-pipeline/docs/business_unit_measure/#unit-measure-standardisation", "title": "Unit Measure Standardisation", "text": "<p>After Extracting the possible unit measures we need to find out which of the extracted measures are the most  reliable to be labelled as the true unit measure of the product, and the values need to be standardised. Here the  algorithm passes through multiple steps to obtain the most reliable unit measure of the product.</p> <ol> <li>First we define the rules by which each unit should be standardised, bringing every product to the same unit of  measure in their category (mass - gram, volume ml etc.)  </li> <li>After defining the rules the process of finalisation starts. At first we check the first extracted unit measure  which is already sorted as the most relevant in the previous part of the program. Check if that unit measure passes  the rule of standardisation, and if it does we assign that unit measure to the product.   </li> <li>If the measure is rejected by rule, the algorithm falls back to the original unit measure column from the  original data. If the original unit measure is in the accepted format (indicating mass or volume) it is accepted as  the final unit measure.   </li> <li>If the desired unit measure is not found from the previous steps, the algorithm starts to search a unit measure  that passes the rules, in the other extracted measures. Algorithm stops if it finds a unit measure and numerical  value that satisfy the predefined rules.  </li> <li>After passing through checks if the unit measure is still not found the algorithm does one more fallback and  checks whether the first Unit Measure and the numerical value can be accepted in case of some transformations. If  the condition is satisfied, the algorithm completes the transformation and assigns Unit measure and the values.   </li> <li>If the unit measure is not found in all of the previous steps the program does not return any unit measure for  that specific product.   </li> <li>As an additional step, the program adds an additional information column which returns the information about  product size extracted in the first part if it exists, and adds the percentage information extracted from data. </li> </ol>"}, {"location": "nlp-pipeline/notebooks/aggregation_code/", "title": "Notebook test", "text": "In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[\u00a0]: Copied! <pre>data = pd.read_csv('rt_57394.csv')\n</pre> data = pd.read_csv('rt_57394.csv') In\u00a0[\u00a0]: Copied! <pre># data processing and rename dataframe columns\ndata['ADG_CODE'] = data['GOODSTNVEDCODE'].astype(str).apply(lambda x: '0' + x if len(x) == 10 else str(x))\ndata['ADG_CODE'] = data['ADG_CODE'].str.extract('(\\d{4})')\n\ndata['TOTAL_WITH_TAXES'] = data['INVOICED_COST']\ndata['UNIT_PRICE'] = data['INVOICED_COST']/data['GOODS_QUANTITY']\ndata['UNIT'] = data['MEASUR_UNIT_QUALIFIER_NAME']\ndata = data.rename({'COD':'TIN', 'GOODS_DSC':'GOOD_NAME'}, axis=1)\ndata['APPLICATION_DATE_DT'] = pd.to_datetime(data['APPLICATION_DATE'])\n# filter for the needed timeframe\ndata = data[\n    (pd.to_datetime('2023-10-01')&gt;=pd.to_datetime(data['APPLICATION_DATE_DT']))&amp;\n    (pd.to_datetime(data['APPLICATION_DATE_DT'])&lt;=pd.to_datetime('2024-06-01'))\n]\n</pre> # data processing and rename dataframe columns data['ADG_CODE'] = data['GOODSTNVEDCODE'].astype(str).apply(lambda x: '0' + x if len(x) == 10 else str(x)) data['ADG_CODE'] = data['ADG_CODE'].str.extract('(\\d{4})')  data['TOTAL_WITH_TAXES'] = data['INVOICED_COST'] data['UNIT_PRICE'] = data['INVOICED_COST']/data['GOODS_QUANTITY'] data['UNIT'] = data['MEASUR_UNIT_QUALIFIER_NAME'] data = data.rename({'COD':'TIN', 'GOODS_DSC':'GOOD_NAME'}, axis=1) data['APPLICATION_DATE_DT'] = pd.to_datetime(data['APPLICATION_DATE']) # filter for the needed timeframe data = data[     (pd.to_datetime('2023-10-01')&gt;=pd.to_datetime(data['APPLICATION_DATE_DT']))&amp;     (pd.to_datetime(data['APPLICATION_DATE_DT'])&lt;=pd.to_datetime('2024-06-01')) ] In\u00a0[\u00a0]: Copied! <pre>df_gr = data.groupby(['TIN', 'GOOD_NAME']).agg({\n    'TOTAL_WITH_TAXES': ['min', 'max', 'mean', 'sum'],\n    'UNIT_PRICE': ['min', 'max', 'mean', 'sum'],\n    'UNIT': [lambda x: x.mode()[0] if len(x.mode()) else None, lambda x:set(x) if len(x) else None],\n    'ADG_CODE': [lambda x:set(x) if len(x) else None, lambda x: x.mode()[0] if len(x.mode()) else None]\n})\n</pre> df_gr = data.groupby(['TIN', 'GOOD_NAME']).agg({     'TOTAL_WITH_TAXES': ['min', 'max', 'mean', 'sum'],     'UNIT_PRICE': ['min', 'max', 'mean', 'sum'],     'UNIT': [lambda x: x.mode()[0] if len(x.mode()) else None, lambda x:set(x) if len(x) else None],     'ADG_CODE': [lambda x:set(x) if len(x) else None, lambda x: x.mode()[0] if len(x.mode()) else None] }) In\u00a0[\u00a0]: Copied! <pre>df_gr.columns = ['TOTAL_WITH_TAXES_min', 'TOTAL_WITH_TAXES_max','TOTAL_WITH_TAXES_mean','TOTAL_WITH_TAXES_sum',\n                'UNIT_PRICE_min', 'UNIT_PRICE_max','UNIT_PRICE_mean','UNIT_PRICE_sum',\n                 'UNIT_mode', 'UNIT_set',\n                 'ADG_CODE_set','ADG_CODE_mode'\n                ]\n\ndf_gr = df_gr.reset_index()\n</pre> df_gr.columns = ['TOTAL_WITH_TAXES_min', 'TOTAL_WITH_TAXES_max','TOTAL_WITH_TAXES_mean','TOTAL_WITH_TAXES_sum',                 'UNIT_PRICE_min', 'UNIT_PRICE_max','UNIT_PRICE_mean','UNIT_PRICE_sum',                  'UNIT_mode', 'UNIT_set',                  'ADG_CODE_set','ADG_CODE_mode'                 ]  df_gr = df_gr.reset_index() In\u00a0[\u00a0]: Copied! <pre>df_gr.to_csv('D:/data/decl_grouped_data.csv', index=False)\n</pre> df_gr.to_csv('D:/data/decl_grouped_data.csv', index=False) In\u00a0[\u00a0]: Copied! <pre>import pandas as pd\n</pre> import pandas as pd In\u00a0[\u00a0]: Copied! <pre>data = pd.read_csv('hdm_data.csv')\n</pre> data = pd.read_csv('hdm_data.csv') In\u00a0[\u00a0]: Copied! <pre>df_gr = data.groupby(['TIN', 'GOOD_NAME']).agg({\n    'TOTAL_WITH_TAXES': ['min', 'max', 'mean', 'sum'],\n    'UNIT_PRICE': ['min', 'max', 'mean', 'sum'],\n    'UNIT': [lambda x: x.mode()[0] if len(x.mode()) else None,  lambda x:set(x) if len(x) else None],\n    'ADG_CODE': [lambda x:set(x) if len(x) else None, lambda x: x.mode()[0] if len(x.mode()) else None]\n})\n</pre> df_gr = data.groupby(['TIN', 'GOOD_NAME']).agg({     'TOTAL_WITH_TAXES': ['min', 'max', 'mean', 'sum'],     'UNIT_PRICE': ['min', 'max', 'mean', 'sum'],     'UNIT': [lambda x: x.mode()[0] if len(x.mode()) else None,  lambda x:set(x) if len(x) else None],     'ADG_CODE': [lambda x:set(x) if len(x) else None, lambda x: x.mode()[0] if len(x.mode()) else None] }) In\u00a0[\u00a0]: Copied! <pre>df_gr.columns = ['TOTAL_WITH_TAXES_min', 'TOTAL_WITH_TAXES_max','TOTAL_WITH_TAXES_mean','TOTAL_WITH_TAXES_sum',\n                'UNIT_PRICE_min', 'UNIT_PRICE_max','UNIT_PRICE_mean','UNIT_PRICE_sum',\n                 'UNIT_mode', 'UNIT_set',\n                 'ADG_CODE_set','ADG_CODE_mode'\n                ]\ndf_gr = df_gr.reset_index()\ndf_gr.to_csv('D:/data/hdm_data_grouped.csv', index=False)\n</pre> df_gr.columns = ['TOTAL_WITH_TAXES_min', 'TOTAL_WITH_TAXES_max','TOTAL_WITH_TAXES_mean','TOTAL_WITH_TAXES_sum',                 'UNIT_PRICE_min', 'UNIT_PRICE_max','UNIT_PRICE_mean','UNIT_PRICE_sum',                  'UNIT_mode', 'UNIT_set',                  'ADG_CODE_set','ADG_CODE_mode'                 ] df_gr = df_gr.reset_index() df_gr.to_csv('D:/data/hdm_data_grouped.csv', index=False) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}, {"location": "nlp-pipeline/notebooks/aggregation_code/#customs-data-aggregation", "title": "Customs Data Aggregation\u00b6", "text": ""}, {"location": "nlp-pipeline/notebooks/aggregation_code/#hdm-data-aggregation", "title": "Hdm Data Aggregation\u00b6", "text": ""}, {"location": "data-cleansing-pipeline/readme/", "title": "Data Cleanisng", "text": ""}, {"location": "data-cleansing-pipeline/readme/#description", "title": "Description", "text": "<p>This repository contains data cleansing scripts for various parts of the program.  - Tabular data cleansing - Text validation - Data cleansing for Main and Electronics NLP-pipeline  - Data cleansing for Address-Analytics </p> <p>The cleansing processes includes Text validation, Armenian cleansing, English cleansing, Translation, Address cleansing, and tabular data cleansing. The data files are primarily in .xlsx and .csv format, and the project is capable of handling batch processing for efficient data management. </p>"}, {"location": "data-cleansing-pipeline/readme/#features", "title": "Features", "text": "<p>The repository implements the following key steps: - ValidatingText: Validates text, to make sure that good names are interpretable. - NLPCleasning: Cleans,preprocesses and preapares data for NLP-pipeline - TabularDataCleasning: General cleasning for Tabular data - AddressPreprocessing: Processing Address information for address analytics.</p> <p>Using a configuration file (<code>config.json</code>) you need to specify the input and ouptut data paths, as well as parameters if needed. You need to specify parameters in NLP-pipeline, and text validation.</p>"}, {"location": "data-cleansing-pipeline/readme/#configuration", "title": "Configuration", "text": "<p>The <code>config.json</code> file contains the paths to the input data and configurable parameters for the preprocesing. You need to specify the input and ouptut data paths, as well as parameters if needed.</p> <p>Data: Specifies paths to input data and ouput files. Params: Controls settings for  - n_jobs for parallel processing  -  batch_size for handling batch operations. -  input_column_name: cahnge columns name if the original column name is not a \"GOOD_NAME\". (This is the case for invoice and declaration data).</p>"}, {"location": "data-cleansing-pipeline/readme/#inputs-and-ouputs-of-program", "title": "Inputs and Ouputs of program", "text": ""}, {"location": "data-cleansing-pipeline/readme/#address", "title": "Address", "text": "<p>Input data name - <code>division information full.xlsx</code>  Output data name - <code>address_output_data.csv</code> Output contains following columns * OIN  * full_address</p>"}, {"location": "data-cleansing-pipeline/readme/#text-validation", "title": "Text Validation", "text": "<p>Input data name - <code>input.csv</code> -- could be any file containig good name Output data name - <code>output.csv</code> Output contains following columns * original good name column * Is_Valid_Product_Name</p>"}, {"location": "data-cleansing-pipeline/readme/#nlp_cleansing", "title": "NLP_Cleansing", "text": "<p>Input data name - <code>input.csv</code> -- could be any file containig good name Output data name - <code>output.csv</code> Output contains following columns * original good name column  GOOD_NAME_CL  GOOD_NAME_CL_TR   GOOD_NAME_CL_TR2  GOOD_NAME_CL_EL   GOOD_NAME_CL_EL_TR    GOOD_NAME_CL_EL_TR2</p> <p>All this columns are needed for NLP pipeline</p>"}, {"location": "data-cleansing-pipeline/readme/#tabular-data-cleasning", "title": "Tabular Data Cleasning", "text": "<p>Input data name - <code>input.csv</code> -- could be any tabular data Output data name - <code>output.csv</code> Output original data columns in cleaned form</p>"}, {"location": "data-cleansing-pipeline/readme/#installation", "title": "Installation", "text": "<p>Follow these steps to set up the project on your local machine:</p> <ol> <li>Clone the repository:</li> </ol> <p>```bash    git clone https://github.com/MindwiseLLC/data-cleansing.git 2. Ensure Python 3.11 is installed:</p> <p>This project requires Python 3.11. You can download and install it from the official Python website: https://www.python.org/downloads/.</p> <ol> <li>Create a virtual environment</li> </ol> <p>After installing Python 3.11, create a virtual environment for the project:</p>"}, {"location": "data-cleansing-pipeline/readme/#for-windows", "title": "For Windows:", "text": "<pre><code>python -m venv data-prep\ndata-prep/Scripts/activate\n</code></pre>"}, {"location": "data-cleansing-pipeline/readme/#for-macoslinux", "title": "For macOS/Linux:", "text": "<p><pre><code>python3.11 -m venv data-prep\nsource data-prep/bin/activate\n</code></pre> 4. Install dependencies</p> <p>After activating the virtual environment, install the required dependencies:</p> <p><pre><code>pip install -r requirements.txt\n</code></pre> 5. Run the main script</p> <p>First, navigate to the 'code' directory using the command:</p> <pre><code>cd code\n</code></pre> <p>Second, choose the cleasning script you need.</p> <p>To run the chosen script, execute the following command:</p> <p><pre><code>python your_chosen_script.py\n</code></pre> This will execute the pipeline according to the settings defined in config.json.</p> <p>Possible scripts are the following</p> <p><pre><code>python NLP_Cleansing.py\n</code></pre> <pre><code>python address_cleansing.py\n</code></pre> <pre><code>python tabular_data_cleansing.py\n</code></pre> <pre><code>python text_validation.py\n</code></pre></p>"}, {"location": "product-tracing/readme/", "title": "Product Tracing", "text": ""}, {"location": "product-tracing/readme/#description", "title": "Description", "text": "<p>This repository contains the codes for performing product tracing. The program takes HDM, Inovice and Declaration data, takes a timeframe for each data as an input and retruns the product paths by whcih the product reached the final seller starting from importer. The prgoram is also responsible for reproting the statistics of the the paths found. The products in this case are identified as a category brand pair. In detail documentation of thiss scripts can be found here.</p>"}, {"location": "product-tracing/readme/#data-sources", "title": "Data Sources", "text": "<p>This program uses the the raw hdm, invoice and declaratiosn data recevived from SRC along with preprocessed data receiced from NLP Pipeline. The aggregation method that we are using for tracing data can be found in <code>data_aggregation.ipynb</code> jupyter notebook. </p> <p>Before inputing the data into our program you have to aggregate the raw and processed data with the logic in the jupyter notebook. </p> <p>The samples for raw and processed data can be found in folder  <code>Data/data_aggregation_examples</code>.  The final prepared sample datas that can be inputed into program can be found in folder <code>Data/final_tracing_data_samples</code></p>"}, {"location": "product-tracing/readme/#configuration", "title": "Configuration", "text": "<p>The <code>config.json</code> file contains the paths to the input data and configurable parameters for the pipeline. Key sections include:</p> <ul> <li>Data: Specifies paths to input data files.</li> <li>Params: Specifies input parameters for tracing category, brand(optional), HDM tin and optional path penght configuraation parameters.</li> <li>Tracing: Enables or disables specific Tracing method.</li> <li>DataRequrirments: Input data requiremnts for all three datassets.s</li> </ul>"}, {"location": "product-tracing/readme/#worklflow", "title": "Worklflow", "text": "<ol> <li>Data Processing: Recieving raw and preprocessed data, aggregating and merging data. </li> <li>Preparing Declarations data: Extracting and normalizing imported product quantities. </li> <li>Price Processing: Extracting and Normalizing all prices from all three datasets. </li> <li>Tracing the brand category pair product.</li> </ol>"}, {"location": "product-tracing/readme/#usage-guide", "title": "Usage Guide", "text": "<p>Follow these steps to set up the project on your local machine:</p> <ol> <li>Clone the repository:</li> </ol> <p>```bash    git clone https://github.com/MindwiseLLC/Product-tracing.git</p> <ol> <li>Ensure Python 3.11 is installed:</li> </ol> <p>This project requires Python 3.11. You can download and install it from the official Python website: https://www.python.org/downloads/.</p> <ol> <li>Create a virtual environment</li> </ol> <p>After installing Python 3.11, create a virtual environment for the project:</p>"}, {"location": "product-tracing/readme/#for-windows", "title": "For Windows:", "text": "<pre><code>python -m venv tracing-env\ntracing-env/Scripts/activate\n</code></pre>"}, {"location": "product-tracing/readme/#for-macoslinux", "title": "For macOS/Linux:", "text": "<p><pre><code>python3.11 -m venv tracing-env\nsource tracing-env/bin/activate\n</code></pre> 4. Install dependencies</p> <p>After activating the virtual environment, install the required dependencies:</p> <p><pre><code>pip install -r requirements.txt\n</code></pre> 5. Run the main script</p> <p>First, navigate to the 'code' directory using the command:</p> <pre><code>cd code\n</code></pre> <p>To run the main script, execute the following command:</p> <p><pre><code>python main.py\n</code></pre> This will execute the pipeline according to the settings defined in config.json.</p>"}, {"location": "product-tracing/docs/general_tracing_business/", "title": "GENERAL TRACING", "text": "<p>General tracing is designed to perform full market calculations for specific products based on product and brand  (OPTIONAL). </p> <p>Input Parameters  Mandatory: Category  Optional: Brand</p> <p>Workflow     First The Received data is processed and filtered by the specified category (and brand).  All the Importers, Suppliers and Seller Tins are identified from the received data. </p> <p>Next  the declaration data is processed and prepared. According to the input timeframe we extract the quantity of  the products imported and normalise them with the same logic as in Unit Measure Extraction for HDM and Invoice. This  process should be done within the tracing process since it is dependent on the timeframe of a specific tracing  instance.   </p> <pre><code>Next we process the prices from all data frames, we normalise them according to the extracted unit of\n</code></pre> <p>measure. This process is also dependent on the timeframe of a specific tracing instance and should be completed  within the tracing process. </p> <p>Based on the calculations made above we report general statistics of the product. </p> <p>Final Report contains the following information. </p> <ul> <li>Data Distribution: What percentage of the data is categorized with soccer higher or equal to 0.8  </li> <li>List of Importers  </li> <li>List of Suppliers  </li> <li>List of Sellers  </li> <li>Transaction counts and unique ids for all three datasets  </li> <li>Average Unit price for all three sources (import, supply, final sale)  </li> <li>Weight for all three sources  </li> <li>whole imported  </li> <li>Whole supplied  </li> <li>whole sold  </li> <li>Price Margins for  </li> <li>From DEC to HDM mean price marginal growth   </li> <li>From DEC to INVOICE mean price marginal growth   </li> <li>From INVOICE to HDM mean price marginal growth </li> </ul> <p>Example of General Tracing</p> <ul> <li>Category - Rice  </li> <li>Data Distribution   </li> <li>Dec_percentage -  100.0,  </li> <li>INV_percentage -  60.08222144731771,  </li> <li> <p>HDM_percentage - 100.0  </p> </li> <li> <p>TransactionCount  </p> </li> <li>HDM -  66,  </li> <li>Dec -  8,  </li> <li> <p>Inv -  772</p> </li> <li> <p>TransactionIDs  </p> </li> <li>\"HDM\": [14, 16, \u2026.]  </li> <li>\"Dec\": [ 172727710,  176617168,  \u2026.]  </li> <li> <p>\"Inv\": [A2907448090, A2993795450,  \u2026.]</p> </li> <li> <p>Importers - [ \"06921803\",\"01847813\", \"01282006\", \u2026 ]  </p> </li> <li>Sellers - [ \"00492515\", \"00103587\", \"00099093\", \u2026 ]  </li> <li>Suppliers -  [ \"00471352\",  \"01536316\",\"00884093\", \u2026 ]  </li> <li>UnitPrice  </li> <li>HDM  <ul> <li>Kg - 1106  </li> <li>Pcs - 860  </li> </ul> </li> <li>DEC  <ul> <li>KG - 398  </li> </ul> </li> <li>INV  <ul> <li>KG - 750  </li> <li>Pcs - 483  </li> </ul> </li> <li>Weight  </li> <li>Whole_sold - 161393164 Gram  </li> <li>Whole_imported -  5517634560 Gram  </li> <li>Whole_supplied - 3486683811 Gram  </li> <li>Price Margins  </li> <li>Declaration to HDM   <ul> <li>For KG  - 177  </li> </ul> </li> <li>Declaration_to_Invoice  <ul> <li>For Kg -  88  </li> </ul> </li> <li>Invoice_to_HDM  <ul> <li>For Kg -  45</li> </ul> </li> </ul>"}, {"location": "product-tracing/docs/tracing_business/", "title": "PRODUCT TRACING", "text": "<p>One of the key aspects of the program is the construction of the path of the product from the importer to the final  seller. To achieve this product tracing logic was implemented. This part of the program is responsible for finding  the most probable paths by which the product reached the final seller using various metrics.</p>"}, {"location": "product-tracing/docs/tracing_business/#tracing-workflow", "title": "Tracing Workflow", "text": "<p>1. Input      As an input we take the raw and processed datasets of HDM, invoice and Declarations.  Next we take user input </p> <ul> <li>Timeframe: start and end date for each data which the user wants to explore.  </li> <li>HDM Tin: Tin of the seller that the user wants to explore.  </li> <li>Category: Category which the user wants to explore  </li> <li>Brand: Brand which the user wants to explore</li> </ul> <p>2. Data Preparation      In this part we receive data from two different sources. First source is the raw data and the second source  is the processed data received from the NLP pipeline. We aggregate the raw data with the metrics that are needed in  the tracing and merge it with the processed data to get the additional information required for tracing. As a result  we receive three working dataframes, containing information from </p> <ol> <li>Hdm Checks   </li> <li>Invoiced   </li> <li>Declarations </li> </ol> <p>3. Declaration Processing     In this part we process and prepare the declaration data. According to the input timeframe we extract the  quantity of the products imported and normalise them with the same logic as in Unit Measure Extraction for HDM and  Invoice. This process should be done within the tracing process since it is dependent on the timeframe of a specific  tracing instance.    4. Price Processing     Next we process the prices from all data frames, we normalise them according to the extracted unit of  measure. This process is also dependent on the timeframe of a specific tracing instance and should be completed  within the tracing process.  5. Tracing     After preparing data with and adding all the required information we proceed to the tracing and path  construction which consists of several key steps. </p> <ol> <li>For start we isolate the data. HDM and Declaration data is isolated based on the input Brand and Category while  the Invoice data is isolated only based on the Category. This decision was made based on the Invoice data  specification. A lot of sold products in the Invoice do not include brand name and therefore the brand extraction is  not efficient on this data.   </li> <li>Next we separate these datasets into three parts based on the categorization score. This is done to separate the  results of the predictions into three different confidence levels. The separation is done with following product  categorization scores.    High Confidence: 0.8 - 1</li> </ol> <p>Medium Confidence: 0.5 - 0.8</p> <p>Low Confidence: 0 - 0.5</p> <p>All three parts of the datasets are passing through the path construction process separately. At this step we  report the distribution of the data based on the score. </p> <ol> <li>Next part is the preparation of data for path construction. First we receive all the unique importer tins for the  category brand pair. Next we isolate the invoice data (invoice data separated by only threshold, NO CATEGORY) where  we specify that the supplier tin should be in the unique importer tins for the category brand pair OR the buyer tin  should correspond to the input buyer tin. We keep this data and add category isolation to the invoice data.     As a result we get two invoice data. First includes all the transactions between category brand pairs of the  importer and the final seller. Here we do not have high confidence that the product is the exact same product that  we are looking for.</li> </ol> <p>Second data is also isolated by the category (in the invoice). Here we have higher confidence that we found the  right product.</p> <ol> <li>From the two invoice data frames received from the previous step we construct directed graphs using the NetworkX  library. One graph represents all transactions (the full graph), and another represents transactions within the  specified category (the category graph). In these graphs, nodes represent companies, and edges represent  transactions. This Network model allows us to find all the possible paths between the importer and Final Seller.  </li> <li>From the two graphs received in the previous step we extract top shortest paths. From the full graph we will see  the top transactions between the importer and seller, while from the category graph we will see the top transaction  between the importer and seller within our required category. </li> </ol> <p>In this step we report both sets of the paths.     Next we take the paths from the category graph that are the subset of the full paths from the full graph. In this  way we ensure they take the most probable correct paths. This will be our working path from now on. </p> <ol> <li>After reporting the working paths we start exploring each path in detail. </li> </ol> <p>For each path we calculate and report the following information </p> <ol> <li>Unit Price of product in Declaration, Invoice and HDM.   </li> <li>Whole weight of the imported product, whole weight of the supplied product and whole weight of the sold  product   </li> <li>The marginal growth of the price from declaration to invoice from invoice to HDM and from declaration to HDM.  </li> <li>If there are intermediaries we report the number of intermediaries and report all the above information for  the intermediaries as well.</li> </ol> <p>All of these steps are done for all probable paths.</p> <ol> <li>Steps from 3 to 6 are repeated for all three parts of the data separated by categorization score. </li> </ol> <p>Example of final tracing report.</p> <p>{'timeframe': {'HDM': {'start_date': '01-MAY-2024 00:00:00',    'end_date': '31-MAY-2024 23:59:59'},   'Dec': {'start_date': '2023-10-01', 'end_date': '2024-06-01'},   'Inv': {'start_date': ''01-APRIL-2024 00:00:00',    'end_date': '31-MAY-2024 23:59:59'}}, 'HDM_tin': '1282006',  'category': 'rice',  'brand': 'gallo',  'data_division_by_threshold': {'threshold_0.8_2': {'data_distribution': { 'Dec_percentage': 100.0,    'INV_percentage': 60.08222144731771,    'HDM_percentage': 100.0,},       'importers': ['29448'],    '29448': {'top_paths_from_all': ([['1282006', '29448'],       ['1282006', '2556388', '29448'],       ['1282006', '1021368', '29448'],       ['1282006', '2524256', '29448'],       ['1282006', '2538542', '29448']],),     'top_paths_from_category': [['1282006', '29448'],      ['1282006', '2538542', '29448']],     'intersection': [['1282006', '29448'], ['1282006', '2538542', '29448']],     'Product_statistcs': { \"exploring path ['1282006', '29448']\": {'UnitPrice': {'Declaration': {'kg': 1164.9932492905778},        'HDM': {'kg': 4348.6238997089495},        'Invoice': {'kg': 3650.8074615384617}},       'Weight': {'whole_imported': (8545000.0, 'gram'),        'whole_sold': (125000.0, 'gram'),        'whole_supplied': (146500.0, 'gram')},       'PriceMargins': {'Declaration_to_HDM': {'Margin for kg': 273.2746007203941},        'Declaration_to_Invoice': {'Margin for kg': 213.37584692114052},        'Invoice_to_HDM': {'Margin for kg': 19.114030129554564}},       'transaction_type': 'direct',       'number_of_intermediaries': 0,       'brand_in_invoice': 'found'},      \"exploring path ['1282006', '2538542', '29448']\": {'UnitPrice':  {'Declaration': {'kg': 1164.9932492905778},        'HDM': {'kg': 4348.6238997089495},        'invoice_transaction_1': {'kg': 3250.0},        'invoice_transaction_2': {'kg': 1041.6667}},       'Weight': {'whole_imported': (8545000.0, 'gram'),        'whole_sold': (125000.0, 'gram'),        'whole_supplied_transaction_1': (12000.0, 'gram'),        'whole_supplied_transaction_2': (351000.0, 'gram')},       'PriceMargins': {'Declaration_to_HDM': {'Margin for kg': 273.2746007203941},        'Declaration_to_Interemediary_1': {'Margin for kg': 178.97157361032663},        'Interemediary_2_to_HDM': {'Margin for kg': 317.467881013087}}       'transaction_type': 'intermediary',       'number_of_intermediaries': 1,       'brand_in_invoice': 'NOT found'}}}},   'threshold_0.5_0.8': {'data_distribution': {},    'Dec_percentage': 0.0,    'INV_percentage': 38.20544350028447,    'HDM_percentage': 0.0,    'importers': []},   'threshold_0_0.5': {'data_distribution': {},    'Dec_percentage': 0.0,    'INV_percentage': 1.7123350523978196,    'HDM_percentage': 0.0,    'importers': []}}}</p>"}, {"location": "causal-impact/", "title": "Causal_impact", "text": ""}, {"location": "causal-impact/#project-overview", "title": "Project Overview", "text": "<p>The Causal Impact model uses a Bayesian structural time-series framework to estimate the hypothetical outcome of a time series in the absence of an intervention, allowing it to measure the causal effect of the intervention by comparing observed data to this hypothetical scenario.</p>"}, {"location": "causal-impact/#data-sources", "title": "Data Sources", "text": "<p>The model runs on preprocessed data which is aggregated from the following data files:</p> <ul> <li>sector2021-2024.xlsx</li> <li>hdm nor daily 2023.csv</li> <li>audit_new_database_merged_all_2024_oct_update.xlsx</li> </ul> <p>These files should be placed in the appropriate directories as referenced in the config.json.</p>"}, {"location": "causal-impact/#configuration", "title": "Configuration", "text": "<p>The config.json file contains the paths to the input data and configurable parameters for the pipeline. Key sections include:</p> <ul> <li>Data: Specifies paths to input data files and output paths.</li> <li>Dates: Dates for treatment and observation periods.</li> <li>Filtering: Filtering criteria for TINs and LICENSE_NUMBERs.</li> </ul>"}, {"location": "causal-impact/#installation", "title": "Installation", "text": "<p>Follow these steps to set up the project on your local machine:</p>"}, {"location": "causal-impact/#1-clone-the-repository", "title": "1. Clone the repository:", "text": "<pre><code>git clone https://github.com/your-repo/Causal_impact.git\n</code></pre>"}, {"location": "causal-impact/#2-ensure-python-3810-is-installed", "title": "2. Ensure Python 3.8.10 is installed:", "text": "<p>This project requires Python 3.8.10 You can download and install it from the official Python website: https://www.python.org/downloads/.</p>"}, {"location": "causal-impact/#3-create-a-virtual-environment", "title": "3. Create a virtual environment", "text": "<p>After installing Python 3.8, create a virtual environment for the project:</p>"}, {"location": "causal-impact/#for-windows", "title": "For Windows:", "text": "<pre><code>python -m venv causal-env\ncausal-env/Scripts/activate\n</code></pre>"}, {"location": "causal-impact/#for-macoslinux", "title": "For macOS/Linux:", "text": "<pre><code>python3.8 -m venv causal-env\nsource causal-env/bin/activate\n</code></pre>"}, {"location": "causal-impact/#4-install-dependencies", "title": "4. Install dependencies", "text": "<p>After activating the virtual environment, install the required dependencies: <pre><code>pip install -r requirements.txt\n</code></pre></p>"}, {"location": "causal-impact/#5-run-the-main-script", "title": "5. Run the main script", "text": "<p>To run the main script, execute the following command: <pre><code>python main.py\n</code></pre> This will execute the model codes according to the settings defined in config.json.</p>"}, {"location": "causal-impact/#output", "title": "Output", "text": "<p>The model scripts will generate the following outputs:</p>"}, {"location": "causal-impact/#1-charts-for-each-individual-tin", "title": "1. Charts for each individual TIN", "text": "<ul> <li>Observed vs Predicted Behavior Chart: This chart will compare the observed sales to the predicted counterfactual scenario.</li> <li>Difference Chart: This chart will depict the differences between observed and predicted sales behaviors.</li> </ul>"}, {"location": "causal-impact/#2-excel-report", "title": "2. Excel Report", "text": "<p>The Excel file will contain detailed data on the daily and cumulative monetary impact of audits for each TIN, presented in AMD, along with p-values to indicate the statistical significance of the impact.</p>"}, {"location": "causal-impact/docs/business/", "title": "Documentation for Causal Impact Model", "text": ""}, {"location": "causal-impact/docs/business/#december-10-2024", "title": "December 10, 2024", "text": "<p>The Causal Impact model is used for time series analysis to measure the causal effect of an intervention (or treatment) on a target variable. It uses a Bayesian structural time-series framework to estimate the hypothetical outcome of a time series in the absence of an intervention, allowing it to measure the causal effect of the intervention by comparing observed data to the predicted behaviour. This model is especially useful for estimating the impact of events such as audits, notifications, or other changes in a system that may lead to changes in a behavior or outcome. In this context, the intervention or treatment date can represent any significant event or intervention, such as:</p> <ul> <li>Audits,  </li> <li>Notifications,  </li> <li>Policy changes,  </li> <li>Any other treatment affecting the system.</li> </ul>"}, {"location": "causal-impact/docs/business/#overview", "title": "Overview", "text": ""}, {"location": "causal-impact/docs/business/#2-model-key-elements", "title": "2. Model Key Elements", "text": "<ol> <li>Pre-period and Post-period: The model works by comparing the behavior of entities before and after the treatment. The pre-period is a baseline (pre-treatment), while the post-period is the period after the treatment (or intervention).  </li> <li>Treatment Date: This is the date of intervention. The function allows flexibility for the treatment date to represent various types of events, from audits to notifications or any other intervention.  </li> <li>Data Preparation: The data is pre-processed to handle missing values, duplicates, and date conversion. Additionally, fraud detection is integrated into the dataset for analysis.  </li> <li>Significance Testing: The Causal Impact model produces a p-value that helps in determining whether the observed change in the behavior of the entities is statistically significant.</li> </ol>"}, {"location": "causal-impact/docs/business/#3-interpretation-of-results", "title": "3. Interpretation of Results", "text": "<ul> <li>P-value Interpretation: </li> <li>If the p-value is below 0.05, the treatment is considered to have a statistically significant effect on the outcome variable (e.g., sales or fraud behavior).  </li> <li>A higher p-value suggests that the observed changes could be due to random variation and that the treatment did not have a significant impact.</li> </ul>"}, {"location": "causal-impact/docs/technical/", "title": "To be submitted", "text": ""}, {"location": "causal-impact/docs/user_guide/", "title": "To be submitted", "text": ""}, {"location": "commodity-tracing/", "title": "Commodity Tracing", "text": ""}, {"location": "commodity-tracing/#project-overview", "title": "Project Overview", "text": "<p>The Commodity Tracing Model is designed to trace the flow of specific commodities through various stages of the supply chain. It processes data from declarations, invoices, and CRN records to identify and track the movement of goods. The model uses natural language processing (NLP) techniques to clean and translate product descriptions, extract keywords, and calculate text similarity to ensure accurate matching of products across different datasets.</p>"}, {"location": "commodity-tracing/#data-sources", "title": "Data Sources", "text": "<p>The model runs on preprocessed data which is aggregated from the following data files:</p> <ul> <li><code>hdm_data.csv</code></li> <li><code>invoice_base.csv</code></li> <li><code>account_details.csv</code></li> <li><code>declarations.csv</code></li> </ul> <p>These files should be placed in the appropriate directories as referenced in the config.py.</p>"}, {"location": "commodity-tracing/#features", "title": "Features", "text": "<ul> <li>Data Processing: Handles large datasets from declarations, invoices, and HDM records.</li> <li>NLP Cleansing: Cleans and translates product descriptions using custom NLP techniques.</li> <li>Keyword Extraction: Uses YAKE (Yet Another Keyword Extractor) to identify relevant keywords from product descriptions.</li> <li>Text Similarity: Calculates cosine similarity between product descriptions using Sentence Transformers.</li> <li>Supply Chain Analysis: Iteratively analyzes the supply chain to trace the flow of commodities across multiple levels.</li> <li>Custom Configuration: Allows for flexible configuration of product-specific parameters, such as declaration codes, HDM ADG codes, and filtering terms.</li> </ul>"}, {"location": "commodity-tracing/#configuration-parameters", "title": "Configuration Parameters", "text": "<p>The config.json file contains the configurable parameters for the pipeline. Key sections include:</p> <ul> <li>product_name: The name of the product being traced (e.g., \"petrol\").</li> <li>declaration: Configuration for processing declaration data.</li> <li>start_date: Start date for filtering declarations.</li> <li>end_date: End date for filtering declarations.</li> <li>decl_codes: List of declaration codes specific to the product.</li> <li>hdm: Configuration for processing HDM data.</li> <li>start_date: Start date for filtering HDM records.</li> <li>end_date: End date for filtering HDM records.</li> <li>adg_codes: List of HDM ADG codes specific to the product.</li> <li>invoices: Configuration for processing invoice data.</li> <li>start_date: Start date for filtering invoices.</li> <li>end_date: End date for filtering invoices.</li> <li>filtering: Custom filtering terms for product descriptions.</li> <li>user_description_regex_terms: Additional regex terms for filtering descriptions.</li> </ul>"}, {"location": "commodity-tracing/#installation", "title": "Installation", "text": "<p>Follow these steps to set up the project on your local machine:</p>"}, {"location": "commodity-tracing/#1-clone-the-repository", "title": "1. Clone the repository:", "text": "<pre><code>git clone https://github.com/your-repo/commodity_tracing.git\n</code></pre>"}, {"location": "commodity-tracing/#2-ensure-python-311-is-installed", "title": "2. Ensure Python 3.11 is installed:", "text": "<p>This project requires Python 3.11 You can download and install it from the official Python website: https://www.python.org/downloads/.</p>"}, {"location": "commodity-tracing/#3-create-a-virtual-environment", "title": "3. Create a virtual environment", "text": "<p>After installing Python 3.11, create a virtual environment for the project:</p>"}, {"location": "commodity-tracing/#for-windows", "title": "For Windows:", "text": "<pre><code>python -m venv env\nenv/Scripts/activate\n</code></pre>"}, {"location": "commodity-tracing/#for-macoslinux", "title": "For macOS/Linux:", "text": "<pre><code>python3.11 -m venv env\nsource env/bin/activate\n</code></pre>"}, {"location": "commodity-tracing/#4-install-dependencies", "title": "4. Install dependencies", "text": "<p>After activating the virtual environment, install the required dependencies: <pre><code>pip install -r requirements.txt\n</code></pre></p>"}, {"location": "commodity-tracing/#5-prepare-data", "title": "5. Prepare Data", "text": "<p>Ensure that the input data files (such as decl_nov.csv, hdm_nov.csv, invoices_base_petrol.csv, acc_details_2023_nov.csv) are placed in the data/provided directory.</p>"}, {"location": "commodity-tracing/#6-run-the-main-script", "title": "6. Run the main script", "text": "<p>To run the main script, execute the following command: <pre><code>python commodity_tracing.py\n</code></pre> This will execute the model codes according to the settings defined in config.json.</p>"}, {"location": "commodity-tracing/#output", "title": "Output", "text": "<p>The model will generate output files in the data/generated directory, including:</p>"}, {"location": "commodity-tracing/#1-filtered-declarations-invoices-and-hdm-records-and-translated-and-cleaned-product-descriptions", "title": "1. Filtered declarations, invoices, and HDM records and translated and cleaned product descriptions.", "text": "<ul> <li>The data for each supply chain level will be saved in the folder of the corresponding level.</li> </ul>"}, {"location": "commodity-tracing/#2-summary-tables-for-monetary-values-quantities-and-average-prices", "title": "2. Summary tables for monetary values, quantities, and average prices.", "text": "<p>The csv files will contain tables for monetary values, quantities, and average prices as well as level information for the selected commodity.</p>"}, {"location": "commodity-tracing/#code-structure", "title": "Code Structure", "text": "<ul> <li>commodity_tracing.py: The main script that runs the commodity tracing model.</li> <li>config.py: Configuration class that loads and manages settings from config.json.</li> <li>custom_translation/: Directory containing custom NLP cleansing and translation modules.</li> <li>data/: Directory for input and output data files.</li> <li>provided/: Input data files.</li> <li>generated/: Output data files.</li> </ul>"}, {"location": "commodity-tracing/#example", "title": "Example", "text": "<p>To trace the flow of petrol through the supply chain:</p> <ol> <li>Set the product_name to \"petrol\" in the config.json file.</li> <li>Provide the relevant declaration codes, HDM ADG codes, and filtering terms.</li> <li>Run the model to generate the output files.</li> </ol>"}, {"location": "data-preparation/", "title": "Data Preparation", "text": "<p>Preprocessing steps for running supervised and unsupervised models</p>"}, {"location": "data-preparation/#install-dependencies", "title": "Install Dependencies", "text": ""}, {"location": "data-preparation/#using-pip-and-requirementstxt", "title": "Using <code>pip</code> and <code>requirements.txt</code>", "text": "<pre><code># Install system package for venv\nsudo apt update &amp;&amp; sudo apt install python3-venv -y\n\n# Create and activate a virtual environment\npython3 -m venv env\nsource env/bin/activate\n\n# Install Python dependencies\npip install --upgrade pip\npip install -r requirements.txt\n</code></pre>"}, {"location": "data-preparation/#using-poetry", "title": "Using Poetry", "text": "<pre><code># Install Poetry using your distro's package manager:\nsudo apt install python3-poetry -y\n\n# Navigate to project root, create/activate venv, and install dependencies\npython3 -m venv env\nsource env/bin/activate\npoetry install\n</code></pre>"}, {"location": "data-preparation/#download-large-files", "title": "Download Large Files", "text": "<p>Some of the files required for this repository are too large to be stored directly in the repository. To download these files, run the following Python script:</p> <pre><code>python download_large_files.py\n</code></pre>"}, {"location": "data-preparation/#how-it-works", "title": "How It Works", "text": "<ul> <li>The script downloads the required files from Google Drive.</li> <li>By default, the files will be saved to <code>data_samples/generated</code>.</li> <li>Custom Destination: You can specify a custom destination folder by providing it as an argument when running the script. For example:  <pre><code>python download_large_files.py --destination /your/custom/path\n</code></pre> Replace <code>/your/custom/path</code> with the desired folder path to store the downloaded files.</li> </ul>"}, {"location": "data-preparation/#sample-data-structure", "title": "Sample Data Structure", "text": "<p>The <code>data_samples</code> directory is organized to streamline access to both raw and processed datasets required for audit and fraud analysis. Below is an outline of its content:</p> <p>Provided Data: - Data provided by external sources for audits and fraud analysis.</p> <p>Generated Data: - Processed datasets aggregated from raw data for downstream tasks.</p> <pre><code>data_samples\n\u251c\u2500\u2500 generated\n\u2502   \u251c\u2500\u2500 full_data_armenia_pre_correction.csv\n\u2502   \u251c\u2500\u2500 full_data_armenia_pre_correction_complex_only.csv\n\u2514\u2500\u2500 provided\n    \u251c\u2500\u2500 audit\n    \u2502   \u2514\u2500\u2500 audit_new_database_merged_all_2024_oct_update.xlsx\n    \u251c\u2500\u2500 corrections_new\n    \u2502   \u251c\u2500\u2500 Shahutahark\n    \u2502   \u2502   \u251c\u2500\u2500 section1.csv\n    \u2502   \u2502   \u2514\u2500\u2500 tins and hash_tin_Arsine.xls\n    \u2502   \u2514\u2500\u2500 VAT\n    \u2502       \u251c\u2500\u2500 mv_vat_ex(2020).xlsx\n    \u2502       \u2514\u2500\u2500 mv_vat_ex(2021,2022,2023).xlsx\n    \u251c\u2500\u2500 employee\n    \u2502   \u251c\u2500\u2500 employee_count.csv\n    \u2502   \u2514\u2500\u2500 unique_emp_month 2013-2023.xls\n    \u251c\u2500\u2500 invoice\n    \u2502   \u251c\u2500\u2500 invocie buyer 2021-2023.csv\n    \u2502   \u2514\u2500\u2500 invocie supplier 2021-2023.csv\n    \u251c\u2500\u2500 sector\n    \u2502   \u2514\u2500\u2500 sector2021-2024_lighter.xlsx\n    \u2514\u2500\u2500 tpentry\n        \u2514\u2500\u2500 tpentry.xlsx\n</code></pre> <p>For clustering, in addition to the files above, the following files are also generated / provided:</p> <pre><code>\u251c\u2500\u2500 generated\n\u2502   \u251c\u2500\u2500 clustering_data.pkl\n\u2502   \u251c\u2500\u2500 location\n\u2502   \u2502   \u251c\u2500\u2500 TIN_OIN_CRN.csv\n\u2502   \u2502   \u2514\u2500\u2500 region_city.csv\n\u2502   \u2514\u2500\u2500 receipts\n\u2502       \u2514\u2500\u2500 receipt_percentages.csv\n\u2514\u2500\u2500 provided\n    \u251c\u2500\u2500 geographical\n    \u2502   \u2514\u2500\u2500 address_master_all.csv\n    \u2514\u2500\u2500 receipts\n        \u2514\u2500\u2500 ecr_receipt_subtotal_6month.csv\n</code></pre>"}, {"location": "data-preparation/#running-script", "title": "Running script", "text": "<p>The <code>config.py</code> file provides centralized configurations for the data preparation.</p> <p>Run <code>merge_data.py</code> to merges and processes various datasets for audit and fraud analysis. Set <code>complex_only = True</code> for complex audit analysis, and <code>False</code> otherwise. It will generate two different datasets depending on that value.</p> <p>Optional: run <code>check_data.py</code> to check the quality of the created data. You can view the results in <code>public/index.html</code>.</p> <p>Run <code>clustering/data_creation.py</code> to merges and processes various datasets for clustering. You need to first run <code>merge_data.py</code> because it uses the same features there.</p>"}, {"location": "data-preparation/#srcmerge_datapy", "title": "<code>src/merge_data.py</code>", "text": "<p>This script integrates multiple data sources, processes them, and saves merged data for use in analysis and modeling.</p>"}, {"location": "data-preparation/#inputs", "title": "Inputs", "text": "<ol> <li>Audit Data: Complex audit records.</li> <li>Financial Data:</li> <li>VAT correction data</li> <li>Profit correction data (Shahutahark)</li> <li>Employee Data: Processed employee records.</li> <li>Invoice Data: Aggregated invoice records.</li> <li>Sector Data: Processed sector information.</li> <li>config.py: <code>df_path</code> is the path where to save the final dataset.</li> </ol>"}, {"location": "data-preparation/#outputs", "title": "Outputs", "text": "<ul> <li>Merged Dataset: A single processed file saved at the location specified by <code>df_path</code> in the configuration, containing combined data from all input sources.</li> </ul>"}, {"location": "data-preparation/#srcclusteringdata_creationpy", "title": "<code>src/clustering/data_creation.py</code>", "text": "<p>This script integrates additional data sources for clustering/</p>"}, {"location": "data-preparation/#inputs_1", "title": "Inputs", "text": "<ol> <li>Receipts Data: Receipts data.</li> <li>Location Data: TIN and OIN Location data.</li> </ol>"}, {"location": "data-preparation/#outputs_1", "title": "Outputs", "text": "<ul> <li><code>clustering_data.pkl</code> that will later be used by the unspervised/clustering analysis.</li> </ul>"}, {"location": "complex-audit/", "title": "Supervised learning directory", "text": ""}, {"location": "complex-audit/#setup-instructions", "title": "Setup Instructions", "text": "<p>Install Dependencies:    Ensure all required libraries (e.g., <code>pandas</code>, <code>sklearn</code>, <code>numpy</code>) are installed.</p> <p><pre><code>pip install -r requirements.txt\n</code></pre> Install IAI:    Please follow the steps on the documentation here for Python installation:    https://docs.interpretable.ai/stable/IAI-Python/installation/</p> <p>For example, if you are using Option 1, do this once:    <pre><code>import interpretableai\ninterpretableai.install_julia()\ninterpretableai.install_system_image()\n</code></pre>    and use the following at the beginning of the script that calls IAI:    <pre><code>import os\nos.environ['IAI_DISABLE_COMPILED_MODULES'] = 'true'\nfrom interpretableai import iai\n</code></pre></p>"}, {"location": "complex-audit/#repo-structure", "title": "Repo Structure", "text": "<p>The repo has two main folders:  - hdm_fraud - complex_audits_frauds - unregistered_employee_fraud</p> <p>Go to each folder for the corresponding README information</p>"}, {"location": "complex-audit/complex_audits_frauds/", "title": "Complex Audits and Frauds Analysis", "text": "<p>This repository provides tools and functions to process data related to complex audits and fraud detection. It includes utilities to merge various datasets and a training pipeline to build and evaluate machine learning models for fraud detection and audit analysis.</p>"}, {"location": "complex-audit/complex_audits_frauds/#models", "title": "Models", "text": "<p>In the <code>models</code> directory we store the trained models that can be used for the fraud and audit prediction task. Both for the audit and fraud tasks, we select XGBoost as the best model. All the models are trained at most with the 50 most important features.</p> <p>For example, when predicting <code>FRAUD_OR_CORRECT</code> (i.e. whether a TIN is either fraudolent or has submitted a major correction), the best model is the following: - <code>models/FRAUD_OR_CORRECT/training_years_[2022]/data_all/lnr_xgb.pkl</code></p> <p>A fully interpretable model can be found by loading the InterpretableAI Optimal Tree. The tree can be visualized either by openining <code>lnr_oct.html</code> or by running the following code: <pre><code>from interpretableai import iai\nfrom feature_map import mapping_short\n\n# Load Optimal Tree\nlnr_oct = iai.read_json('models/FRAUD_OR_CORRECT/training_years_[2022]/data_all/lnr_oct.json')\n\n# Get Features used by the model\nfeatures_used = lnr_oct.get_features_used()\n\n# Select only the features that are actually used by the model\nnew_feature_map = {key: val for (key, val) in mapping_short.items() if key in features_used}\n\n# Visualize in Browser with mapped features\niai.TreePlot(lnr_oct, feature_renames=new_feature_map).show_in_browser()\n</code></pre></p>"}, {"location": "complex-audit/complex_audits_frauds/#feature-mapping", "title": "Feature Mapping", "text": "<p>We mapped all the features available for the complex audit models. The mapping is accessible in <code>feature_map.py</code> file. - mapping_long: for each column has a lengthy explanation of its content - mapping_short: for each column has a short explanation of its content, used for visualization</p>"}, {"location": "complex-audit/complex_audits_frauds/#targets", "title": "Targets", "text": "<p>We support the following targets for training and evaluation: - <code>AUDIT_ANY</code>: True if a company will receive a complex audit - <code>FRAUD_ANY</code>: True if a company will be fraudolent during a complex audit - <code>CORRECT_PROFIT</code>: True if a company will submit a significant change in its Shahutahark tax form - <code>FRAUD_OR_CORRECT</code>: True if a company will be fraudolent during a complex audit or submit a significant change in its Shahutahark tax form</p>"}, {"location": "complex-audit/complex_audits_frauds/#pipeline-overview", "title": "Pipeline Overview", "text": "<p>The pipeline is meant to be run in the following order: 1. <code>merge_data.py</code>: Merges and processes various datasets for audit and fraud analysis. 2. <code>training_pipeline.py</code>: Contains the pipeline for training machine learning models. 3. <code>evaluation_pipeline.py</code>: Contains the pipeline for evaluating machine learning models.</p>"}, {"location": "complex-audit/complex_audits_frauds/#how-the-configuration-works", "title": "How the Configuration Works", "text": "<p>The <code>config.py</code> file provides centralized configurations for the training and evaluation pipelines. Both <code>training_pipeline.py</code> and <code>evaluation_pipeline.py</code> load their respective configurations from <code>config.py</code> to streamline parameters and ensure consistency.</p>"}, {"location": "complex-audit/complex_audits_frauds/#training_pipelinepy", "title": "<code>training_pipeline.py</code>", "text": "<p>This script constructs machine learning models to classify audits and detect potential fraud based on the merged data from <code>merge_data.py</code>.</p>"}, {"location": "complex-audit/complex_audits_frauds/#inputs", "title": "Inputs", "text": "<ol> <li>Merged Data: The output from <code>merge_data.py</code> with cleaned and processed features for training.</li> <li>config.py: <code>training_config</code> is the configuration specifying the dataset path, target variable, training years, and the directory to save the trained models.</li> </ol>"}, {"location": "complex-audit/complex_audits_frauds/#example-config-training_config-from-configpy", "title": "Example Config (<code>training_config</code> from <code>config.py</code>):", "text": "<pre><code>training_config = {\n   'df_path': '/path/to/dataset.csv',\n   'target': 'FRAUD_OR_CORRECT',\n   'training_years': [2022],\n   'save_path': '/path/to/save/models',\n   'interpretable': True,\n}\n</code></pre>"}, {"location": "complex-audit/complex_audits_frauds/#outputs", "title": "Outputs", "text": "<ul> <li>Model Object: The trained model object is saved using <code>pickle</code> for use in the evaluation step.</li> </ul>"}, {"location": "complex-audit/complex_audits_frauds/#evaluation_pipelinepy", "title": "<code>evaluation_pipeline.py</code>", "text": "<p>This script evaluates the performance of trained machine learning models on unseen data, generates interpretability insights, and produces evaluation metrics. It outputs ranked predictions, identifying the companies (TINs) that should be prioritized for audits based on the likelihood of fraud or anomalies.</p>"}, {"location": "complex-audit/complex_audits_frauds/#inputs_1", "title": "Inputs", "text": "<ol> <li>Trained Models: Models generated and saved by <code>training_pipeline.py</code> are loaded for evaluation.</li> <li>Merged Data: The processed dataset, typically output from <code>merge_data.py</code>, filtered for the evaluation years.</li> <li>config.py: <code>evaluation_config</code> is the configuration that defines the evaluation dataset, model path, and evaluation years.</li> </ol>"}, {"location": "complex-audit/complex_audits_frauds/#example-config-evaluation_config-from-configpy", "title": "Example Config (<code>evaluation_config</code> from <code>config.py</code>):", "text": "<pre><code>evaluation_config = {\n   'df_path': '/path/to/dataset.csv',\n   'inference': False,\n   'models_path': '/path/to/models',\n   'df_path': '/path/to/data.csv',\n   'target': 'FRAUD_OR_CORRECT',\n   'evaluate_years': [2023],\n   'training_years': [2022],\n   'figures_path': '/path/to/figures',\n   'ranking': 'predicted_change_class',\n   'compute_tin_level': True,\n   'interpretable': True,\n   'best_model': 'xgb',\n}\n</code></pre>"}, {"location": "complex-audit/complex_audits_frauds/#outputs_1", "title": "Outputs", "text": "<ol> <li>Always Generated Outputs:</li> <li>Ranked Predictions: Probabilities or rankings for each instance in the evaluation dataset based on the trained model's predictions. Users can specify the ranking criteria in the <code>evaluation_config</code>. Supported options include:<ul> <li><code>fraud_proba</code>: Sort by the probability of fraud (default for fraud models).</li> <li><code>predicted_change</code>: Sort by the predicted magnitude of corrections (useful for corrective tasks).</li> <li><code>predicted_change_class</code>: Sort by the predicted magnitude of corrections using a classification model (Low, Medium, High), empirically shown to be more effective.</li> </ul> </li> <li>dict_tin_tax_year: A dictionary mapping TINs to their corresponding tax years, with the following keys:<ul> <li><code>predicted_prob</code>: The predicted probability of the event of interest.</li> <li><code>decile</code>: The decile in which the prediction falls.</li> <li><code>ranking</code>: The rank of the instance in the dataset based on the prediction.</li> <li><code>sample_shap_values</code>: SHAP values for the specific sample, explaining the model\u2019s prediction.</li> <li><code>expected_value</code>: The expected value of the model output.</li> <li><code>sample</code>: The actual data sample used for evaluation.</li> <li><code>sample_tree_explanation</code>: The TIN level explanation using InterpretableAI's Decision Tree.</li> </ul> </li> <li> <p>full_features: The full set of features used during evaluation, including preprocessed and derived variables, for further analysis and debugging.</p> </li> <li> <p>When <code>inference</code> is <code>False</code>:</p> </li> <li>Evaluation Metrics: Metrics such as accuracy, AUC, precision-recall, and confusion matrices are computed and saved.</li> <li>Interpretability Insights: SHAP plots and other visualizations are generated to explain model predictions.</li> <li>Evaluation Summaries: Reports summarizing model performance, including comparisons across multiple models, are generated for reporting and analysis.</li> </ol> <p>The <code>inference</code> flag in the configuration dictates whether full evaluation metrics and insights are produced (<code>False</code>) or only ranked predictions are generated (<code>True</code>).</p>"}, {"location": "complex-audit/complex_audits_frauds/#exampleipynb", "title": "<code>example.ipynb</code>", "text": "<p>This Jupyter notebook provides a working example of how to tweak the most important functions in this repository, covering: 1. Load large files from Google Drive 2. Data preparation and merging using <code>merge_data.py</code> (Only if data in 1. is not needed anymore) 3. Model training with <code>training_pipeline.py</code> 4. Model evaluation with <code>evaluation_pipeline.py</code> 5. Visualization and interpretation of model results, including global and TIN-level explanation of the models.</p> <p>This notebook is a good starting point for understanding how to use the code in this repository for audit and fraud analysis.</p>"}, {"location": "clustering/clustering/", "title": "Unsupervised learning directory", "text": ""}, {"location": "clustering/clustering/#installation", "title": "Installation", "text": "<p>Simply run <code>poetry install</code> to install the necessary files from the <code>pyproject.toml</code> file.</p>"}, {"location": "clustering/clustering/#data", "title": "Data", "text": "<p>We only require <code>clustering_data.pkl</code> in the data folder. If you do not have this data, run scripts from the <code>data-prepration</code> repo.</p>"}, {"location": "clustering/clustering/#run", "title": "Run", "text": "<p>Adjust the desired settings in <code>configs.json</code> and then run the <code>main.py</code> file.</p>"}, {"location": "time-series-analysis/time_series_analysis/", "title": "Time series analysis", "text": "<p>This repository provides tools and functions to perform time series analysis. </p>"}, {"location": "time-series-analysis/time_series_analysis/#data", "title": "Data", "text": "<p>We need <code>subset_data/grouped.csv</code> in folder <code>data/generated</code> as input. The file is generated from running data-preparation package.</p>"}, {"location": "time-series-analysis/time_series_analysis/#running", "title": "Running", "text": "<p>Running <code>src/main.py</code> will perform the time series outlier detection analysis for the configuration in <code>config.json</code> file.</p>"}, {"location": "time-series-analysis/time_series_analysis/#demo", "title": "Demo", "text": "<p>A working demo with examples of the functionalities of the repository is in <code>src/working_demo.ipynb</code>. </p>"}, {"location": "supermarket-scraper/", "title": "\ud83d\uded2 Supermarket Scraper", "text": "<p>This project is a web scraping tool designed to extract product data from various supermarket websites. It supports both basic and advanced scraping modes, allowing users to collect essential and enriched product information.</p> <p>Supported supermarkets are organized in the <code>supermarkets/</code> folder, with each supermarket having its own dedicated scraping scripts.</p> <p>Follow the setup instructions below to get started.</p>"}, {"location": "supermarket-scraper/#setup-instructions", "title": "\u2699\ufe0f Setup Instructions", "text": ""}, {"location": "supermarket-scraper/#1-install-chromedriver", "title": "1. \u2705 Install ChromeDriver", "text": "<p>Make sure Google Chrome is installed.</p> <p>\ud83d\udd39 macOS <pre><code>brew install chromedriver\n</code></pre></p> <p>\ud83d\udd39 Windows 1. Download the version that corresponds to your Chrome version from: https://googlechromelabs.github.io/chrome-for-testing/ 2. Extract and add the path to chromedriver.exe to your System Environment Variables &gt; Path.</p> <p>\ud83d\udd39 Linux (Debian/Ubuntu) <pre><code>sudo apt update\n</code></pre> <pre><code>sudo apt install -y unzip xvfb libxi6 libgconf-2-4 libnss3 libxss1 libappindicator1 libindicator7\n</code></pre> <pre><code>wget https://chromedriver.storage.googleapis.com/124.0.6367.91/chromedriver_linux64.zip\n</code></pre> <pre><code>unzip chromedriver_linux64.zip\n</code></pre> <pre><code>sudo mv chromedriver /usr/local/bin/\n</code></pre> Test installation: <pre><code>chromedriver --version\n</code></pre></p>"}, {"location": "supermarket-scraper/#2-create-virtual-environment", "title": "2. \ud83d\udc0d Create Virtual Environment", "text": "<p><pre><code>python -m venv venv\n</code></pre> macOS/Linux <pre><code>source venv/bin/activate\n</code></pre> Windows <pre><code>venv\\Scripts\\activate\n</code></pre></p>"}, {"location": "supermarket-scraper/#3-install-dependencies", "title": "3. \ud83d\udce6 Install Dependencies", "text": "<pre><code>pip install -r requirements.txt\n</code></pre>"}, {"location": "supermarket-scraper/#running-the-scraper", "title": "\ud83d\ude80 Running the Scraper", "text": "<p>To run a scraper for a specific supermarket, use the run.py script.</p> <ol> <li>Navigate to the root directory of the project.</li> <li>Open the <code>supermarkets/</code> folder to view the available supermarket directories.</li> <li>Choose one of the folder names (e.g., parma) and replace <code>{supermarket_name}</code> in the command below:</li> </ol> <pre><code>python run.py {supermarket_name} basic\n</code></pre> <p>\ud83d\udd39 Basic Mode</p> <p>Runs only <code>basic_scraper.py</code> for the selected supermarket, collecting essential product data such as name, price, and other details from the category listing page.</p> <pre><code>python run.py {supermarket_name} basic\n</code></pre> <p>\ud83d\udd39 Advanced Mode</p> <p>Runs both basic_scraper.py and advanced_scraper.py. The advanced scraper enriches the data with extra information where available.</p> <p><pre><code>python run.py {supermarket_name} advanced\n</code></pre> \u26a0\ufe0f Note: The availability and type of advanced data depend on the supermarket's website. Some fields (like brand or origin) may be missing or incomplete for certain supermarkets.</p>"}, {"location": "supermarket-scraper/#example-commands", "title": "\ud83d\udcdd Example Commands", "text": "<p>\u270d\ufe0f Run Parma's basic scraper. <pre><code>python run.py parma basic\n</code></pre> \u270d\ufe0f Run Yerevan City's full (basic + advanced) scraping pipeline. <pre><code>python run.py yerevan_city advanced\n</code></pre></p>"}, {"location": "supermarket-scraper/#output", "title": "\ud83d\udce4 Output", "text": "<p>Results are saved to <code>output_data/&lt;supermarket&gt;/</code>.</p> <p>Filenames are timestamped for versioning (e.g. <code>basic_31.05.25_13.45.22.xlsx</code>).</p>"}, {"location": "supermarket-scraper/#notes", "title": "\ud83e\udde0 Notes", "text": "<p><code>basic_scraper.py</code>: Scrapes product data like name, price, and URL.</p> <p><code>advanced_scraper.py</code>: Enriches the data with brand, origin country, and availability.</p> <p>Each supermarket has its dedicated scripts under <code>supermarkets/&lt;name&gt;/</code>.</p>"}, {"location": "supermarket-scraper/#troubleshooting", "title": "\ud83d\udee0\ufe0f Troubleshooting", "text": "<p>ChromeDriver not found: Ensure it's installed and added to your PATH.</p> <p>Version mismatch: Your ChromeDriver version must match your Chrome browser version.</p> <p>Permission denied (Linux): Run chmod +x chromedriver or use sudo.</p>"}, {"location": "causal/causal/docs/business/", "title": "Documentation for Causal Impact Model", "text": ""}, {"location": "causal/causal/docs/business/#december-10-2024", "title": "December 10, 2024", "text": "<p>The Causal Impact model is used for time series analysis to measure the causal effect of an intervention (or treatment) on a target variable. It uses a Bayesian structural time-series framework to estimate the hypothetical outcome of a time series in the absence of an intervention, allowing it to measure the causal effect of the intervention by comparing observed data to the predicted behaviour. This model is especially useful for estimating the impact of events such as audits, notifications, or other changes in a system that may lead to changes in a behavior or outcome. In this context, the intervention or treatment date can represent any significant event or intervention, such as:</p> <ul> <li>Audits,  </li> <li>Notifications,  </li> <li>Policy changes,  </li> <li>Any other treatment affecting the system.</li> </ul>"}, {"location": "causal/causal/docs/business/#overview", "title": "Overview", "text": ""}, {"location": "causal/causal/docs/business/#2-model-key-elements", "title": "2. Model Key Elements", "text": "<ol> <li>Pre-period and Post-period: The model works by comparing the behavior of entities before and after the treatment. The pre-period is a baseline (pre-treatment), while the post-period is the period after the treatment (or intervention).  </li> <li>Treatment Date: This is the date of intervention. The function allows flexibility for the treatment date to represent various types of events, from audits to notifications or any other intervention.  </li> <li>Data Preparation: The data is pre-processed to handle missing values, duplicates, and date conversion. Additionally, fraud detection is integrated into the dataset for analysis.  </li> <li>Significance Testing: The Causal Impact model produces a p-value that helps in determining whether the observed change in the behavior of the entities is statistically significant.</li> </ol>"}, {"location": "causal/causal/docs/business/#3-interpretation-of-results", "title": "3. Interpretation of Results", "text": "<ul> <li>P-value Interpretation: </li> <li>If the p-value is below 0.05, the treatment is considered to have a statistically significant effect on the outcome variable (e.g., sales or fraud behavior).  </li> <li>A higher p-value suggests that the observed changes could be due to random variation and that the treatment did not have a significant impact.</li> </ul>"}, {"location": "causal/causal/docs/technical/", "title": "To be submitted", "text": ""}, {"location": "causal/causal/docs/user_guide/", "title": "To be submitted", "text": ""}, {"location": "docstrings/py_doc_Strings/__init__/", "title": "init", "text": ""}, {"location": "docstrings/py_doc_Strings/matching/", "title": "Matching", "text": "In\u00a0[\u00a0]: Copied! <pre># Data\nimport re\nimport numpy as np\nimport pandas as pd\nimport json\nfrom dotmap import DotMap\n</pre> # Data import re import numpy as np import pandas as pd import json from dotmap import DotMap In\u00a0[\u00a0]: Copied! <pre># Models\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\nimport faiss\n</pre> # Models from sentence_transformers import SentenceTransformer, util import torch import faiss In\u00a0[\u00a0]: Copied! <pre># Lexical Models \nfrom fuzzywuzzy import fuzz, process\n</pre> # Lexical Models  from fuzzywuzzy import fuzz, process In\u00a0[\u00a0]: Copied! <pre># Translation\nfrom googletrans import Translator\nfrom deep_translator import GoogleTranslator\n</pre> # Translation from googletrans import Translator from deep_translator import GoogleTranslator In\u00a0[\u00a0]: Copied! <pre># Parallel procesaing\nfrom joblib import Parallel, delayed\n</pre> # Parallel procesaing from joblib import Parallel, delayed In\u00a0[\u00a0]: Copied! <pre># Measures of code speed\nimport time\n</pre> # Measures of code speed import time In\u00a0[\u00a0]: Copied! <pre># helpers\nfrom tqdm import tqdm\nfrom tqdm_joblib import tqdm_joblib\nimport gc\n</pre> # helpers from tqdm import tqdm from tqdm_joblib import tqdm_joblib import gc In\u00a0[\u00a0]: Copied! <pre>tqdm.pandas()\n</pre> tqdm.pandas() In\u00a0[\u00a0]: Copied! <pre>faiss.omp_set_num_threads(1)\n</pre> faiss.omp_set_num_threads(1) In\u00a0[\u00a0]: Copied! <pre>class NlpPilepine:\n\n    def __init__(self, embeddings_path, data_path, grain_emb_path , veggies_emb_path,\n                 dairy_emb_path , petrol_emb_path , data_source = None,matching_data_params_path = None, filtering_atg_codes_path = None):\n\n        self.perform_isolated_matching = False if data_source == 'Inv' else True\n        self.matching_data_params_path = matching_data_params_path\n        self.filtering_atg_codes_path = filtering_atg_codes_path\n\n\n        self.grain_emb = pd.read_csv(grain_emb_path)\n        self.veggies_emb = pd.read_csv(veggies_emb_path)\n        self.diary_emb = pd.read_csv(dairy_emb_path)\n        self.petrol_emb = pd.read_csv(petrol_emb_path)\n        self.embeddings_data = pd.read_csv(embeddings_path)\n        \n        self.master_data = pd.concat([self.embeddings_data,\n                                              self.grain_emb,\n                                              self.veggies_emb,\n                                              self.diary_emb,\n                                              self.petrol_emb])\n        \n        if self.perform_isolated_matching:\n            pass\n        else:\n            self.embeddings_data = self.master_data\n            \n        self.data = pd.read_csv(data_path, converters={'ADG_CODE': self.converter_func})\n       \n        self.device = self.find_device()\n        self.model = self.get_model()\n\n\n    # prepare reference data for isolated matching\n    def prepare_isolated_matching_data(self):\n            with open(self.matching_data_params_path, 'r') as json_file:\n                isolated_data_params = json.load(json_file)\n            isolated_data_params = DotMap(isolated_data_params)\n            #prepare matching data\n            columns = [\"cleaned_good_name2\", \"sub_category\", 'emb', 'final_cat_emb']\n            self.target_veggies = pd.concat([self.embeddings_data[(self.embeddings_data['category'].isin(isolated_data_params.veggies.reference_data.from_category)) &amp; \n                                        (~self.embeddings_data['sub_category'].isin(isolated_data_params.veggies.reference_data.not_from_subcategory))\n                                        &amp; ~((self.embeddings_data['product_name'].str.contains(isolated_data_params.veggies.reference_data.product_name_contains, case=False)) &amp; \n                                            (self.embeddings_data['sub_category'] != isolated_data_params.veggies.reference_data.product_name_contains))][columns],\n                                        self.veggies_emb[columns],]).reset_index(drop=True)\n            \n            self.target_veggies.loc[self.target_veggies.cleaned_good_name2.str.contains('asparagus',case=False ), 'sub_category'] = 'Asparagus'\n\n            self.target_grain = pd.concat([\n                self.embeddings_data[(self.embeddings_data['category'].isin(isolated_data_params.grain.reference_data.from_category)) &amp; \n                            (~self.embeddings_data['sub_category'].isin(isolated_data_params.grain.reference_data.not_from_sub_category))][columns],\n                self.embeddings_data[(self.embeddings_data['sub_category'].isin(isolated_data_params.grain.reference_data.from_subcategory))][columns],\n                self.grain_emb[columns],\n            ]).reset_index(drop=True)\n\n            self.target_diary = pd.concat([\n                self.embeddings_data[(self.embeddings_data['category'].isin(isolated_data_params.dairy.reference_data.from_category)) &amp; \n                            (~self.embeddings_data['sub_category'].isin(isolated_data_params.dairy.reference_data.not_from_subcategory))][columns],\n                self.diary_emb[columns]]).reset_index(drop=True)\n\n\n            self.target_petrol  = pd.concat([\n                self.embeddings_data[self.embeddings_data['category'].isin(isolated_data_params.petrol.reference_data.from_category)][columns],\n                self.petrol_emb[columns],\n            ]).reset_index(drop=True)\n\n            self.target_canned_food  = self.embeddings_data[self.embeddings_data['category'].isin(isolated_data_params.canned_food.reference_data.from_category) &amp;\n                                                (~self.embeddings_data['sub_category'].isin(isolated_data_params.canned_food.reference_data.not_from_subcategory))][columns]\n\n            self.target_meat_products  = self.embeddings_data[self.embeddings_data['category'].isin(isolated_data_params.meat_products.reference_data.from_category) &amp; \n                                                    (~self.embeddings_data['sub_category'].isin(isolated_data_params.meat_products.reference_data.not_from_subcategory))][columns]\n            \n            #prepare checking data\n            isolated_data_params.veggies.checking_list_veggies.original_list.extend(self.embeddings_data[ \n            (self.embeddings_data['category'].isin(isolated_data_params.veggies.checking_list_veggies.extension_category)) | \n            (self.embeddings_data['sub_category'].isin(isolated_data_params.veggies.checking_list_veggies.extension_subcategory))][\"brand\"].str.replace('_',' ').dropna().unique().tolist())\n\n            isolated_data_params.grain.checking_list_groats.original_list.extend(self.embeddings_data[self.embeddings_data['category'].isin(isolated_data_params.grain.checking_list_groats.extension_category)][\"brand\"].str.replace('_',' ').dropna().unique().tolist())\n            \n            self.checking_list_veggies = isolated_data_params.veggies.checking_list_veggies.original_list\n            self.checking_list_groats = isolated_data_params.grain.checking_list_groats.original_list\n            self.checking_list_diary = isolated_data_params.dairy.checking_list_diary.original_list\n            self.checking_list_petrol = isolated_data_params.petrol.checking_list_petrol.original_list\n            self.checking_list_canned_food = isolated_data_params.canned_food.checking_list_canned_food.original_list\n            self.checking_list_meat_products = isolated_data_params.meat_products.checking_list_meat_products.original_list\n        \n    # isolate data for isolated matching\n    def isolated_data(self, SheetName, HdmData, CheckingList):\n    \n        # loading atg codes data\n        filtering_atg_codes_df = pd.read_excel(self.filtering_atg_codes_path, sheet_name=SheetName, converters={'ATG CODES': self.converter_func})\n        filtering_atg_codes = filtering_atg_codes_df['ATG CODES'].to_list()\n        # getting working data\n        working_df = HdmData[HdmData['ADG_CODE'].isin(filtering_atg_codes)]\n        \n        checking_list = [x.lower() for x in CheckingList]\n        checking_pattern = r'\\b(?:' + '|'.join(checking_list) + r')\\b'\n\n        working_df = working_df[working_df['GOOD_NAME_CL_TR2'].notna()]\n        working_df = working_df[~working_df['GOOD_NAME_CL_TR2'].str.lower().str.contains(checking_pattern, regex=True)]\n        \n        return working_df\n    \n    \n    # ================================= Lexical Matching codes ===================================\n    @staticmethod\n    def adjusted_token_set_ratio(query, choice):\n        # Compute the token set ratio score\n        score = fuzz.token_set_ratio(query, choice)\n        \n        # Calculate word sets\n        query_words = set(query.lower().split())\n        choice_words = set(choice.lower().split())\n        \n        # Calculate the proportion of matched words\n        matched_words = query_words.intersection(choice_words)\n        total_words = query_words.union(choice_words)\n        #word_match_proportion = len(matched_words) / len(total_words) if total_words else 0\n        unmatched_proportion = (len(total_words) - len(matched_words)) / len(total_words) if total_words else 0\n        \n        # Adjust the score by multiplying with the word match proportion\n        adjusted_score = score - unmatched_proportion * 20\n        \n        return adjusted_score\n    \n    # lexical matching  \n    def find_best_match_token_set_ratio(self, good_name, reference_df):\n        choices = reference_df['cleaned_good_name2'].tolist()\n        \n        # Proceed with matching\n        if good_name.strip():\n            # Use the standard scorer\n            best_match = process.extractOne(good_name, choices, scorer= self.adjusted_token_set_ratio)\n            return best_match\n        else:\n            return None\n\n    # helper function for lexical matching, process one row\n    def process_row_for_lexical_match(self, row, reference_df):\n        good_name = row['GOOD_NAME_CL_TR2']\n        \n        # Skip if good_name is just punctuation or whitespace\n        if not good_name or good_name.isspace():\n            return None\n        \n        best_match = self.find_best_match_token_set_ratio(good_name, reference_df)\n        \n        if best_match:\n            best_value, best_score = best_match[0], best_match[1]\n            match = reference_df.loc[reference_df['cleaned_good_name2'] == best_value, 'sub_category']\n            if not match.empty:\n                best_category = match.values[0]\n            \n            return good_name, best_value, best_score, best_category\n        return None\n    \n    # parallelize lexical matching codes\n    def lexical_matching(self, working_df, reference_df, threshold=70, category_name=None):\n        with tqdm_joblib(desc=f\"Lexical Matching: {category_name}\", total=len(working_df)) as progress_bar:\n            results = Parallel(n_jobs=-1)(\n                delayed(self.process_row_for_lexical_match)(row, reference_df)\n                for idx, row in working_df.iterrows()\n            )\n        \n        # Filter out None results\n        results = [res for res in results if res is not None]\n        \n        # Create a DataFrame from the results\n        df_results_token_set_ratio_combined = pd.DataFrame(results, columns=[\n            'GOOD_NAME_CL_TR2', 'sim_cand', 'max_value', 'sim_cand_category'])\n\n        results_df = df_results_token_set_ratio_combined[df_results_token_set_ratio_combined['max_value'] &gt; threshold].reset_index(drop=True)\n        remaining_df = df_results_token_set_ratio_combined[df_results_token_set_ratio_combined['max_value'] &lt;= threshold].reset_index(drop=True)\n        \n        return results_df, remaining_df\n\n    \n    def encode_data(self, column_list):\n        \"\"\"\n        Encodes a list of text data into embeddings using a specified model.\n\n        Args:\n            column_list (list): A list of text items to encode.\n            model (Model): The model used for encoding the text.\n\n        Returns:\n            Tensor: The embeddings tensor generated from the input text.\n        \"\"\"\n        pool = self.model.start_multi_process_pool([self.device, self.device, self.device, self.device])\n        \n        embeddings = self.model.encode_multi_process(column_list, pool, show_progress_bar=True) # Encoding the text data into embeddings\n        \n        self.model.stop_multi_process_pool(pool)\n        \n        return embeddings\n    \n    def convert_to_list(self, embedding_str):\n        \"\"\"\n        Converts a string or numpy array representation of embeddings into a list of floats.\n\n        Args:\n            embedding (str or np.ndarray): The embedding, either as a string representation or a numpy array.\n\n        Returns:\n            list: A list of floats extracted from the input.\n        \"\"\"\n        if isinstance(embedding_str, str):\n            # Use regex to find all numbers in the string (handles scientific notation as well)\n            numbers = re.findall(r\"[-+]?\\d*\\.\\d+e[-+]?\\d+|\\d*\\.\\d+|\\d+\", embedding_str)\n            # Convert the extracted strings to floats\n            return [float(num) for num in numbers]\n        elif isinstance(embedding_str, np.ndarray):\n            return embedding_str.tolist()\n        else:\n            raise TypeError(f\"Unsupported type for embedding: {type(embedding_str)}, {(embedding_str)}\")\n    \n    \n    def get_embeddings(self, data, column_name,  model, ready_embeddings = False, emb_col = \"emb\"):\n        \"\"\"\n        Retrieves and processes embeddings for various categories and names from provided data.\n\n        Args:\n            HDM_data (DataFrame): DataFrame containing GOOD_NAME_CL_TR2 data.\n            reference_data (DataFrame): DataFrame with sub_category and cleaned_good_name2 data.\n            model (Model): The model used for encoding the data.\n\n        Returns:\n            tuple: A tuple containing processed query, embeddings, and document data.\n        \"\"\"\n        # Ensure the column has no NaN values and is of string type\n        data[column_name] = data[column_name].fillna('').astype(str)\n        \n        if ready_embeddings: #If embeddings are already calculated and are in the data\n            \n            # Process data\n            data[f'{emb_col}_open'] = data[emb_col].apply(self.convert_to_list)\n            if \"sub_category\" in data.columns:\n                data['sub_category'] = data['sub_category'].apply(lambda x: str(x).lower().strip())\n            else:\n                pass\n            # Embeddings from data\n            original_data_list = data[column_name].tolist()\n            original_data_emb =  torch.tensor(data[f'{emb_col}_open'].tolist()).to(self.device) # send preobtained embeddings to device\n            \n        else:\n            original_data_list = data[column_name].tolist()\n            original_data_emb =  self.encode_data(column_list = original_data_list)\n            \n        return original_data_list, original_data_emb\n\n    def calculate_similarity(self, docs, doc_embeddings, query, query_emb):\n        \"\"\"\n        Calculates the cosine similarity between query embeddings and document embeddings using Faiss.\n\n        Args:\n            docs (list): List of documents.\n            doc_embeddings (numpy.ndarray): Embeddings of the documents, expected numpy array.\n            query (str): Query identifier.\n            query_emb (numpy.ndarray): Embeddings of the query, expected numpy array.\n\n        Returns:\n            DataFrame: A DataFrame containing similarity scores and maximum candidate details.\n        \"\"\"\n        start = time.time()\n        # Ensure the embeddings are in numpy array format\n        if not isinstance(doc_embeddings, np.ndarray):\n            doc_embeddings = doc_embeddings.cpu().numpy()\n        if not isinstance(query_emb, np.ndarray):\n            query_emb = query_emb.cpu().numpy()\n            \n        # Normalize the embeddings to use cosine similarity\n        faiss.normalize_L2(doc_embeddings)\n        faiss.normalize_L2(query_emb)\n\n        # Create a Faiss index for inner product (cosine similarity)\n        d = doc_embeddings.shape[1]\n        index = faiss.IndexFlatIP(d)\n        index.add(doc_embeddings)\n\n        # Perform the search\n        D, I = index.search(query_emb, k=1)  # Find the most similar document\n\n        # Extract the top scores and indices\n        max_scores = D.flatten()\n        max_indices = I.flatten()\n\n        # Create a DataFrame to store results\n        similarity_df = pd.DataFrame(columns=['max_cand', 'max_score'], index=[query])\n        similarity_df['max_cand'] = [docs[idx] for idx in max_indices]\n        similarity_df['max_score'] = max_scores\n        print(\"Similarity Calculation Completed in --- %s seconds ---\" % (time.time() - start))\n        return similarity_df\n    \n    \n    \n    def isolated_matching(self, ShetName, HdmData, CheckingList, reference_df, model=None,\n        lexical_treshold=70, semantic_treshold=0.5, lexical_match=True, semantic_matching=True,\n        special_words=None, category_name=None, force_semantic_words=None\n    ):\n        working_df = self.isolated_data(ShetName, HdmData, CheckingList)\n\n        # Check if working_df is empty\n        if working_df.empty:\n            print(\"No data to process after initial isolation.\")\n            return None  # Terminate the function if no data to process\n\n        # =======================================================================\n        #                       Prepare Force Semantic Data\n        # =======================================================================\n        if force_semantic_words:\n            # Create a regex pattern for force_semantic_words\n            force_semantic_pattern = '|'.join(force_semantic_words)\n            # Identify entries that contain force_semantic_words\n            force_semantic_mask = working_df['GOOD_NAME_CL_TR2'].str.contains(\n                force_semantic_pattern, case=False, na=False, regex=True\n            )\n            force_semantic_df = working_df[force_semantic_mask]\n            # Remove these entries from working_df to exclude from lexical matching\n            working_df = working_df[~force_semantic_mask]\n        else:\n            force_semantic_df = pd.DataFrame()\n\n        # =======================================================================\n        #                           Lexical Matching\n        # =======================================================================\n        if lexical_match:\n            lexical_result_df, remaining_df = self.lexical_matching(\n                working_df, reference_df, threshold=lexical_treshold, category_name=category_name\n            )\n\n            # Scale the matching score of the lexical method\n            lexical_result_df['max_value'] = lexical_result_df['max_value'] / 100\n\n            # Prepare remaining_df for semantic matching\n            if remaining_df.empty and semantic_matching:\n                remaining_df = force_semantic_df.copy()\n            else:\n                # Add force_semantic_df to remaining_df\n                remaining_df = pd.concat([remaining_df, force_semantic_df], ignore_index=True).drop_duplicates()\n        else:\n            print(f\"Lexical matching not required for {category_name} category\")\n            remaining_df = pd.concat([working_df, force_semantic_df], ignore_index=True).drop_duplicates()\n\n        # =======================================================================\n        #                           Semantic Matching\n        # =======================================================================\n        if semantic_matching and not remaining_df.empty:\n            semantic_results = self.semantic_matching(remaining_df, reference_df, category_matching = False)\n            \n        else:\n            semantic_results = pd.DataFrame(columns=['GOOD_NAME_CL_TR2', 'sim_cand', 'sim_cand_category', 'max_value'])\n\n        # Combine results\n        if lexical_match and semantic_matching:\n            final_results = pd.concat([\n                lexical_result_df,\n                semantic_results[['GOOD_NAME_CL_TR2', 'sim_cand', 'sim_cand_category', 'max_value']]\n            ], ignore_index=True).reset_index(drop=True)\n        elif not semantic_matching and lexical_match:\n            final_results = lexical_result_df\n        elif not lexical_match and semantic_matching:\n            final_results = semantic_results[['GOOD_NAME_CL_TR2', 'sim_cand', 'sim_cand_category', 'max_value']]\n        else:\n            final_results = pd.DataFrame()\n    \n        # Merge with original data\n        combined_working_df = pd.concat([working_df, force_semantic_df], ignore_index=True)\n        final_result_merged = final_results.drop_duplicates('GOOD_NAME_CL_TR2').merge(\n            combined_working_df[['GOOD_NAME', 'GOOD_NAME_CL_TR2']],\n            on='GOOD_NAME_CL_TR2', how='left'\n        )\n        \n        # Check special words\n        if special_words:\n            for word in special_words:\n                condition = final_result_merged['GOOD_NAME'].str.contains(\n                    word, case=False, na=False, regex=True\n                )\n                final_result_merged.loc[condition, 'sim_cand_category'] = special_words[word]\n                final_result_merged.loc[condition, 'sim_cand'] = word\n                final_result_merged.loc[condition, 'max_value'] = float(0.9)\n\n    \n        final_result_merged_final = final_result_merged[\n            final_result_merged['max_value'] &gt; semantic_treshold\n        ]\n        \n        return final_result_merged_final\n    \n    \n    def semantic_matching(self, working_data, embeddings_data, category_matching = True):\n        \n        # ================================================= Get Embeddings ===================================================================\n        \n        query, query_emb = self.get_embeddings(data = working_data, column_name = 'GOOD_NAME_CL_TR2', model = self.model)\n\n        # Docs (Good Name on Good Name)\n        docs_GN, doc_GN_emb = self.get_embeddings(data = embeddings_data, column_name = 'cleaned_good_name2', emb_col = 'emb', model = self.model, ready_embeddings=True)\n        \n        # Docs (Good Name on Final Category)\n        docs_cat, doc_cat_emb = self.get_embeddings(data = embeddings_data, column_name = 'sub_category', emb_col = 'final_cat_emb', model = self.model, ready_embeddings=True)\n\n        # ================================================= Matching Good Name on Good Name ===================================================\n        print(\"Good on good matching\")\n        good_on_good_df = self.calculate_similarity(docs_GN, doc_GN_emb, query, query_emb)\n        \n        del docs_GN, doc_GN_emb,\n\n        # Select the relevant columns and reset the index\n        good_on_good_df = good_on_good_df[['max_cand', 'max_score']].reset_index()\n        \n        # Merge with city scrape DataFrame\n        merged_results = good_on_good_df.merge(\n            embeddings_data[['sub_category', 'cleaned_good_name2']].drop_duplicates(subset='cleaned_good_name2'), \n            how='left', \n            right_on='cleaned_good_name2', \n            left_on='max_cand'\n        ).iloc[:, :-1]\n\n        # Rename columns for clarity\n        merged_results = merged_results.rename(\n            columns={\n                'level_0': 'GOOD_NAME_CL_TR2',\n                'max_cand': 'sim_cand',\n                'max_score': 'max_value', \n                'sub_category': 'sim_cand_category', \n            }\n        )\n        \n        del good_on_good_df\n        self.empty_device_cache()\n        \n        if not category_matching:\n            return merged_results\n        # ================================================ Matching Good Name on Final Category ================================================\n        print(\"Good on category matching\")\n        good_on_cat_df = self.calculate_similarity(docs_cat, doc_cat_emb, query, query_emb) \n        \n        del docs_cat, doc_cat_emb,  query, query_emb,\n        #print(\"Good Name on Final Category Completed in \\n--- %s seconds ---\" % (time.time() - start_time))\n\n        # Merge and refine final results\n        res_df = merged_results.merge(good_on_cat_df[['max_cand','max_score']].reset_index().drop_duplicates(subset='level_0'), how='left', right_on='level_0',\n                        left_on='GOOD_NAME_CL_TR2').drop(['level_0'], axis=1).drop_duplicates(subset='GOOD_NAME_CL_TR2')\n        \n        res_df = res_df.merge(working_data[['GOOD_NAME', 'GOOD_NAME_CL_TR2']], on='GOOD_NAME_CL_TR2', how='left')\n        \n        return res_df\n      \n    def conditional_decision_logic(self, all_matching_results):\n        \n        # Apply conditional decision logic for final results\n        all_matching_results['sub_category'] = np.where(\n                                        all_matching_results['max_cand'] == \"books, magazines\",\n                                        all_matching_results['max_cand'],\n                                        np.where(all_matching_results['max_score'] &gt;= all_matching_results['max_value'], all_matching_results['max_cand'], all_matching_results['sim_cand_category'])\n                                        )\n\n        all_matching_results['category_score'] = np.where(\n            all_matching_results['max_cand'] == \"books, magazines\",\n            all_matching_results['max_score'],  # Assuming you want to keep the original max_score for \"books, magazines\"\n            np.where(all_matching_results['max_score'] &gt;= all_matching_results['max_value'], all_matching_results['max_score'], all_matching_results['max_value'])\n        )\n        \n        all_matching_results = all_matching_results[['GOOD_NAME', 'sub_category', 'category_score', 'sim_cand', 'max_value', 'GOOD_NAME_CL_TR2']]\n        \n        all_matching_results.sub_category = all_matching_results.sub_category.str.lower()\n        \n        self.master_data.sub_category = self.master_data.sub_category.str.lower()\n        all_matching_results = all_matching_results.merge(self.master_data[['sub_category', 'category', 'high_category']].drop_duplicates(subset = 'sub_category'), on = 'sub_category', how = 'left')\n        \n        all_matching_results.sub_category = all_matching_results.sub_category.str.capitalize()\n        all_matching_results = all_matching_results.merge(self.data[['GOOD_NAME','GOOD_NAME_CL_TR','ADG_CODE']],  how='left', on='GOOD_NAME')\n        \n        return all_matching_results\n    \n    def regular_matching_categorization(self):\n        \n        res_df = self.semantic_matching(self.data, self.embeddings_data)\n        \n        res_df = self.conditional_decision_logic(res_df)\n        \n        return res_df\n    \n    def isolated_matching_categorization(self):\n        \"\"\"\n        Processes the matching of GOOD_NAME data with reference data using specified embeddings.\n\n        Args:\n            None\n\n        Returns:\n            DataFrame: The final DataFrame after processing matches and merging data.\n        \"\"\"\n        \n        # =====================================================================================================================================\n        #                                                 Isolated Matching: Fruits and Veggies Matching \n        # =====================================================================================================================================\n        \n        fruits_results = self.isolated_matching(ShetName = 'Fruits_veggies', HdmData = self.data, CheckingList = self.checking_list_veggies,\n                                        reference_df = self.target_veggies, semantic_treshold=0.5, model = self.model, lexical_treshold = 61, \n                                        category_name = \"Fruits and Veggies\", special_words = {\"\u0579\u056b\u0580|\u0579\u0578\u0580|\u0579\u0561\u0574\u056b\u0579\":\"Dried_fruits_and_vegetables\", \n                                                    \"\u057d\u0561\u057c\":\"Frozen_fruits_vegetables_and_berries\", \"\u0564\u0564\u0574\u056b\u056f\":\"Zucchini\",'\u056f\u0561\u0576\u0561\u0579\u056b':'Greens' }, force_semantic_words = ['cherry'])\n        \n        if fruits_results is not None:\n            # Only execute this code block if fruits_results is a DataFrame\n            Working_data = self.data[~self.data['GOOD_NAME'].isin(fruits_results['GOOD_NAME'])]\n        else:\n            # Handle the case where fruits_results is None\n            print(\"No fruits results to exclude from HDM data.\")\n            Working_data = self.data\n            \n        # Empty device cahce beofr proceeding to whole data matching\n        self.empty_device_cache()\n        # =====================================================================================================================================\n        #                                                 Isolated Matching: Grain\n        # =====================================================================================================================================\n        \n        grain_results = self.isolated_matching(ShetName = 'Grain',HdmData = Working_data, CheckingList = self.checking_list_groats,\n                                                reference_df = self.target_grain, lexical_treshold = 79, semantic_matching= True, model = self.model,\n                                    semantic_treshold = 0.7, special_words = {'\u0562\u056c\u0572\u0578\u0582\u0580':'Bulgur',  '\u0570\u0576\u0564\u056f\u0561\u0571\u0561\u057e\u0561\u0580':'Buckwheat', '\\b\u0571\u0561\u057e\u0561\u0580\\b':'Wheat_groat',\n                                                                              '\u057d\u057a\u056b\u057f\u0561\u056f\u0561\u0571\u0561\u057e\u0561\u0580|\u0574\u0561\u0576\u056b':'Semolina',  '\u057d\u056b\u057d\u0565\u057c':'Chickpeas', '\u0563\u0561\u0580\u0565\u0571\u0561\u057e\u0561\u0580':'Pearl_barley',\n                                                                              '\u0570\u0561\u0573\u0561\u0580':'Emmer', '\u0583\u0578\u056d\u056b\u0576\u0571':'Other_grain', '\u0565\u0563\u056b\u057a\u057f\u0561\u0581\u0578\u0580\u0565\u0576':'Dried_corn', '\u0562\u0580\u056b\u0576\u0571':'Rice',\n                                                                               '\u0563\u0561\u0580\u0578\u056d|\u0578\u056c\u0578\u057c':'Peas', '\u0578\u057d\u057a':'Lentil', '\u056c\u0578\u0562\u056b':'Bean', '\u056f\u0578\u0580\u0565\u056f\u0561\u0571\u0561\u057e\u0561\u0580|\u056f\u0578\u0580\u0565\u056f':\"Millet_bran\",\n                                                                               '\u057d\u0561\u0563\u0561\u056d\u0578\u057f|\u056f\u056b\u0576\u0578\u0582\u0561|\u056f\u056b\u0576\u0578\u0561|\u0584\u056b\u0576\u0578\u0561|\u0584\u056b\u0576\u0578\u0582\u0561':'Quinoa','\u0583\u0561\u0569\u056b\u056c\u0576\u0565\u0580':'Flakes', '\u057e\u0561\u0580\u057d\u0561\u056f':'Oat',\"\u0561\u056c\u0575\u0578\u0582\u0580\":\"Flour\",}, \n                                                                                force_semantic_words = ['flake', 'barley'], category_name = \"Grain\")\n        \n        if grain_results is not None:\n            # Only execute this code block if fruits_results is a DataFrame\n            Working_data = Working_data[~Working_data['GOOD_NAME'].isin(grain_results['GOOD_NAME'])]\n        else:\n            # Handle the case where fruits_results is None\n            print(\"No grain results to exclude from HDM data.\")\n            Working_data = Working_data\n\n        # Empty device cahce beofr proceeding to whole data matching\n        self.empty_device_cache()\n        \n        # =====================================================================================================================================\n        #                                                 Isolated Matching: Mix of grain and fresh veggies\n        # =====================================================================================================================================\n        \n        mixed_results = self.isolated_matching(ShetName = 'Mixed_category', HdmData = Working_data, CheckingList = ['marinated'], \n                                            reference_df = pd.concat([self.target_veggies,self.target_grain]), semantic_treshold=0.6, model = self.model, lexical_treshold = 60, \n                                            category_name = \"Mixed Category\", force_semantic_words = ['cherry'])\n        \n        \n        if mixed_results is not None:\n            # Only execute this code block if fruits_results is a DataFrame\n            Working_data = Working_data[~Working_data['GOOD_NAME'].isin(mixed_results['GOOD_NAME'])]\n        else:\n            # Handle the case where fruits_results is None\n            print(\"No mixed category results to exclude from HDM data.\")\n            Working_data = Working_data\n            \n        # Empty device cahce beofr proceeding to whole data matching\n        self.empty_device_cache()\n        \n        # =====================================================================================================================================\n        #                                                 Isolated Matching: Dairy\n        # =====================================================================================================================================\n        \n        dairy_results = self.isolated_matching(ShetName = 'Dairy', model = self.model, semantic_matching=True,\n                                HdmData = Working_data, CheckingList = self.checking_list_diary, reference_df = self.target_diary,  lexical_treshold = 75,  semantic_treshold = 0.7,\n                                   special_words = {\"\u0569\u0569\u057e\u0561\u057d\u0565\u0580\": \"Sour_cream\",\"\u056f\u0561\u0569\u0576\u0561\u0577\u0578\u057c\": \"Cottage_cheese\",  \"\u057a\u0561\u0576\u056b\u0580\": \"Cheese\", \n                                                    '\u057d\u0583\u0580\u0565\u0564':'Spread', '\u056f\u0561\u0580\u0561\u0563':'Butter',  \"\u0574\u0561\u056e\u0576\u0561\u0562\u0580\u0564\u0578\u0577\" : \"other_dairy\",\n                                                    \"\\b\u057d\u0565\u0580\u0578\u0582\u0581\u0584\\b\": \"Cream\", }, category_name = \"Dairy\")\n\n        if dairy_results is not None:\n                # Only execute this code block if fruits_results is a DataFrame\n                Working_data = Working_data[~Working_data['GOOD_NAME'].isin(dairy_results['GOOD_NAME'])]\n        else:\n            # Handle the case where fruits_results is None\n            print(\"No dairy results to exclude from HDM data.\")\n            Working_data = Working_data\n\n        # Empty device cahce beofr proceeding to whole data matching\n        self.empty_device_cache()\n            \n        # =====================================================================================================================================\n        #                                                 Isolated Matching: Petrol\n        # =====================================================================================================================================  \n        petrol_results = self.isolated_matching(ShetName = 'Petrol', lexical_treshold= 0, HdmData = Working_data, CheckingList = self.checking_list_petrol,\n                                         reference_df = self.target_petrol,  semantic_matching= False, category_name = \"Petrol\")\n\n\n        if petrol_results is not None:\n                # Only execute this code block if fruits_results is a DataFrame\n                Working_data = Working_data[~Working_data['GOOD_NAME'].isin(petrol_results['GOOD_NAME'])]\n        else:\n            # Handle the case where fruits_results is None\n            print(\"No petrol results to exclude from HDM data.\")\n            Working_data = Working_data\n\n        # Empty device cahce beofr proceeding to whole data matching\n        self.empty_device_cache()\n        \n        # =====================================================================================================================================\n        #                                                 Isolated Matching: Canned Food\n        # =====================================================================================================================================\n        \n        canned_food_results =  self.isolated_matching(ShetName = 'Canned_food', model =  self.model, semantic_treshold = 0.8, semantic_matching=True,\n                                                    HdmData = Working_data, CheckingList =  self.checking_list_canned_food, reference_df =  self.target_canned_food, \n                                                    lexical_treshold = 80,  category_name = \"Canned Food\")\n\n\n        if canned_food_results is not None:\n                # Only execute this code block if fruits_results is a DataFrame\n                Working_data = Working_data[~Working_data['GOOD_NAME'].isin(canned_food_results['GOOD_NAME'])]\n        else:\n            # Handle the case where canned_food_results is None\n            print(\"No canned food results to exclude from HDM data.\")\n            Working_data = Working_data\n\n        # Empty device cahce beofr proceeding to whole data matching\n        self.empty_device_cache()\n        \n        # =====================================================================================================================================\n        #                                                 Isolated Matching: Meat \n        # =====================================================================================================================================\n        \n        meat_products_results = self.isolated_matching( ShetName = 'meat_products', model = self.model, HdmData = Working_data, CheckingList = self.checking_list_meat_products,\n                                        reference_df = self.target_meat_products, lexical_match=False, semantic_matching= True,  semantic_treshold=0.75, \n                                        special_words = {\"\u0565\u0580\u0577\u056b\u056f\": \"Boiled_sausage\",\"\u0576\u0580\u0562\u0565\u0580\u0577\u056b\u056f\": \"Sausages\", \"\u0562\u0561\u057d\u057f\u0578\u0582\u0580\u0574\u0561|\u057d\u0578\u0582\u057b\u0578\u0582\u056d\" : \"Basturma_sujuk\"}, \n                                        category_name = \"Meat and Meat Products\")\n\n\n        if meat_products_results is not None:\n                # Only execute this code block if fruits_results is a DataFrame\n                Working_data = Working_data[~Working_data['GOOD_NAME'].isin(meat_products_results['GOOD_NAME'])]\n        else:\n            # Handle the case where meat_products_results is None\n            print(\"No meat products results to exclude from HDM data.\")\n            Working_data = Working_data\n\n        # Empty device cahce beofr proceeding to whole data matching\n        self.empty_device_cache()\n            \n        # ================================================ Prepare matching data ==============================================================\n        \n        # Debug prints to understand the filtering process\n        print(\"Initial reference_data length:\", len(self.embeddings_data))\n        reference_data = self.embeddings_data[~self.embeddings_data['cleaned_good_name2'].str.lower().isin(self.target_veggies['cleaned_good_name2'].str.lower())]\n        reference_data = reference_data[~reference_data['cleaned_good_name2'].str.lower().isin(self.target_grain['cleaned_good_name2'].str.lower())]\n        reference_data = reference_data[~reference_data['cleaned_good_name2'].str.lower().isin(self.target_diary['cleaned_good_name2'].str.lower())]\n        reference_data = reference_data[~reference_data['cleaned_good_name2'].str.lower().isin(self.target_petrol['cleaned_good_name2'].str.lower())]\n        reference_data = reference_data.drop_duplicates(subset='cleaned_good_name2')\n        print(\"Final reference_data length after dropping duplicates:\", len(reference_data))\n        \n        # =========================================Perform Regular Matching on the rest of the data===========================================\n        \n        res_df = self.semantic_matching(Working_data, reference_data)\n        \n        all_matches = pd.concat([\n            fruits_results,\n            grain_results,\n            mixed_results,\n            dairy_results,\n            petrol_results,\n            canned_food_results,\n            meat_products_results,\n            res_df\n        ], ignore_index=True)#.drop_duplicates(subset='GOOD_NAME_CL_TR2')\n        \n        \n        all_matches = self.conditional_decision_logic(all_matches)\n        \n        return all_matches\n\n    def match_category(self):\n        if self.perform_isolated_matching:\n            self.prepare_isolated_matching_data()\n            res_df = self.isolated_matching_categorization()\n        else:\n            res_df = self.regular_matching_categorization()\n\n        self.data = res_df\n        \n        return self.data\n            \n        \n    def get_model(self):\n        return SentenceTransformer(\"all-mpnet-base-v2\").to(self.device)\n\n    @staticmethod\n    def find_device():\n        \"\"\"\n        Find and return the best available device for computation.\n        \"\"\"\n        # Check if MPS is available\n        if torch.backends.mps.is_available():\n            print(\"Using MPS device\")\n            return \"mps\"\n        # Check if CUDA is available\n        elif torch.cuda.is_available():\n            print(\"Using CUDA device\")\n            return \"cuda\"\n        # Default to CPU if neither MPS nor CUDA is available\n        else:\n            print(\"Using CPU device\")\n            return \"cpu\"\n        \n    def empty_device_cache(self):\n        \"\"\"\n        Empties the cache of the current device to free up memory.\n        \n        This function clears the cache for either MPS on macOS devices, CUDA on GPU-enabled devices, or collects garbage if the device is set to CPU.\n        \"\"\"\n        if self.device == \"mps\":\n            torch.mps.empty_cache()\n        elif self.device == 'cuda':\n            torch.cuda.empty_cache()\n        else:\n            gc.collect()\n            \n    def converter_func(self, x):\n        \"\"\"\n        Converts a given value to a zero-padded 4-digit string if the value is numeric.\n        This function checks if the input `x` is a numeric value and attempts to convert it to a 4-digit string, padded with leading zeros if necessary.\n        \n        Args:\n            x (any): The value to be converted. Can be a number, a string, or `NaN`.\n        Returns:\n            str or any: A zero-padded 4-digit string if `x` is a numeric value, or the original value if it cannot be converted.\n        \"\"\"\n        if pd.isna(x):\n            return x \n        else:\n            try:\n                if isinstance(x, float) and x.is_integer():\n                    int_x = int(x)\n                    return f\"{int_x:04d}\"  \n                else:\n                    \n                    int_x = int(float(x))\n                    return f\"{int_x:04d}\"  \n            except ValueError:\n                \n                return x\n</pre> class NlpPilepine:      def __init__(self, embeddings_path, data_path, grain_emb_path , veggies_emb_path,                  dairy_emb_path , petrol_emb_path , data_source = None,matching_data_params_path = None, filtering_atg_codes_path = None):          self.perform_isolated_matching = False if data_source == 'Inv' else True         self.matching_data_params_path = matching_data_params_path         self.filtering_atg_codes_path = filtering_atg_codes_path           self.grain_emb = pd.read_csv(grain_emb_path)         self.veggies_emb = pd.read_csv(veggies_emb_path)         self.diary_emb = pd.read_csv(dairy_emb_path)         self.petrol_emb = pd.read_csv(petrol_emb_path)         self.embeddings_data = pd.read_csv(embeddings_path)                  self.master_data = pd.concat([self.embeddings_data,                                               self.grain_emb,                                               self.veggies_emb,                                               self.diary_emb,                                               self.petrol_emb])                  if self.perform_isolated_matching:             pass         else:             self.embeddings_data = self.master_data                      self.data = pd.read_csv(data_path, converters={'ADG_CODE': self.converter_func})                 self.device = self.find_device()         self.model = self.get_model()       # prepare reference data for isolated matching     def prepare_isolated_matching_data(self):             with open(self.matching_data_params_path, 'r') as json_file:                 isolated_data_params = json.load(json_file)             isolated_data_params = DotMap(isolated_data_params)             #prepare matching data             columns = [\"cleaned_good_name2\", \"sub_category\", 'emb', 'final_cat_emb']             self.target_veggies = pd.concat([self.embeddings_data[(self.embeddings_data['category'].isin(isolated_data_params.veggies.reference_data.from_category)) &amp;                                          (~self.embeddings_data['sub_category'].isin(isolated_data_params.veggies.reference_data.not_from_subcategory))                                         &amp; ~((self.embeddings_data['product_name'].str.contains(isolated_data_params.veggies.reference_data.product_name_contains, case=False)) &amp;                                              (self.embeddings_data['sub_category'] != isolated_data_params.veggies.reference_data.product_name_contains))][columns],                                         self.veggies_emb[columns],]).reset_index(drop=True)                          self.target_veggies.loc[self.target_veggies.cleaned_good_name2.str.contains('asparagus',case=False ), 'sub_category'] = 'Asparagus'              self.target_grain = pd.concat([                 self.embeddings_data[(self.embeddings_data['category'].isin(isolated_data_params.grain.reference_data.from_category)) &amp;                              (~self.embeddings_data['sub_category'].isin(isolated_data_params.grain.reference_data.not_from_sub_category))][columns],                 self.embeddings_data[(self.embeddings_data['sub_category'].isin(isolated_data_params.grain.reference_data.from_subcategory))][columns],                 self.grain_emb[columns],             ]).reset_index(drop=True)              self.target_diary = pd.concat([                 self.embeddings_data[(self.embeddings_data['category'].isin(isolated_data_params.dairy.reference_data.from_category)) &amp;                              (~self.embeddings_data['sub_category'].isin(isolated_data_params.dairy.reference_data.not_from_subcategory))][columns],                 self.diary_emb[columns]]).reset_index(drop=True)               self.target_petrol  = pd.concat([                 self.embeddings_data[self.embeddings_data['category'].isin(isolated_data_params.petrol.reference_data.from_category)][columns],                 self.petrol_emb[columns],             ]).reset_index(drop=True)              self.target_canned_food  = self.embeddings_data[self.embeddings_data['category'].isin(isolated_data_params.canned_food.reference_data.from_category) &amp;                                                 (~self.embeddings_data['sub_category'].isin(isolated_data_params.canned_food.reference_data.not_from_subcategory))][columns]              self.target_meat_products  = self.embeddings_data[self.embeddings_data['category'].isin(isolated_data_params.meat_products.reference_data.from_category) &amp;                                                      (~self.embeddings_data['sub_category'].isin(isolated_data_params.meat_products.reference_data.not_from_subcategory))][columns]                          #prepare checking data             isolated_data_params.veggies.checking_list_veggies.original_list.extend(self.embeddings_data[              (self.embeddings_data['category'].isin(isolated_data_params.veggies.checking_list_veggies.extension_category)) |              (self.embeddings_data['sub_category'].isin(isolated_data_params.veggies.checking_list_veggies.extension_subcategory))][\"brand\"].str.replace('_',' ').dropna().unique().tolist())              isolated_data_params.grain.checking_list_groats.original_list.extend(self.embeddings_data[self.embeddings_data['category'].isin(isolated_data_params.grain.checking_list_groats.extension_category)][\"brand\"].str.replace('_',' ').dropna().unique().tolist())                          self.checking_list_veggies = isolated_data_params.veggies.checking_list_veggies.original_list             self.checking_list_groats = isolated_data_params.grain.checking_list_groats.original_list             self.checking_list_diary = isolated_data_params.dairy.checking_list_diary.original_list             self.checking_list_petrol = isolated_data_params.petrol.checking_list_petrol.original_list             self.checking_list_canned_food = isolated_data_params.canned_food.checking_list_canned_food.original_list             self.checking_list_meat_products = isolated_data_params.meat_products.checking_list_meat_products.original_list              # isolate data for isolated matching     def isolated_data(self, SheetName, HdmData, CheckingList):              # loading atg codes data         filtering_atg_codes_df = pd.read_excel(self.filtering_atg_codes_path, sheet_name=SheetName, converters={'ATG CODES': self.converter_func})         filtering_atg_codes = filtering_atg_codes_df['ATG CODES'].to_list()         # getting working data         working_df = HdmData[HdmData['ADG_CODE'].isin(filtering_atg_codes)]                  checking_list = [x.lower() for x in CheckingList]         checking_pattern = r'\\b(?:' + '|'.join(checking_list) + r')\\b'          working_df = working_df[working_df['GOOD_NAME_CL_TR2'].notna()]         working_df = working_df[~working_df['GOOD_NAME_CL_TR2'].str.lower().str.contains(checking_pattern, regex=True)]                  return working_df               # ================================= Lexical Matching codes ===================================     @staticmethod     def adjusted_token_set_ratio(query, choice):         # Compute the token set ratio score         score = fuzz.token_set_ratio(query, choice)                  # Calculate word sets         query_words = set(query.lower().split())         choice_words = set(choice.lower().split())                  # Calculate the proportion of matched words         matched_words = query_words.intersection(choice_words)         total_words = query_words.union(choice_words)         #word_match_proportion = len(matched_words) / len(total_words) if total_words else 0         unmatched_proportion = (len(total_words) - len(matched_words)) / len(total_words) if total_words else 0                  # Adjust the score by multiplying with the word match proportion         adjusted_score = score - unmatched_proportion * 20                  return adjusted_score          # lexical matching       def find_best_match_token_set_ratio(self, good_name, reference_df):         choices = reference_df['cleaned_good_name2'].tolist()                  # Proceed with matching         if good_name.strip():             # Use the standard scorer             best_match = process.extractOne(good_name, choices, scorer= self.adjusted_token_set_ratio)             return best_match         else:             return None      # helper function for lexical matching, process one row     def process_row_for_lexical_match(self, row, reference_df):         good_name = row['GOOD_NAME_CL_TR2']                  # Skip if good_name is just punctuation or whitespace         if not good_name or good_name.isspace():             return None                  best_match = self.find_best_match_token_set_ratio(good_name, reference_df)                  if best_match:             best_value, best_score = best_match[0], best_match[1]             match = reference_df.loc[reference_df['cleaned_good_name2'] == best_value, 'sub_category']             if not match.empty:                 best_category = match.values[0]                          return good_name, best_value, best_score, best_category         return None          # parallelize lexical matching codes     def lexical_matching(self, working_df, reference_df, threshold=70, category_name=None):         with tqdm_joblib(desc=f\"Lexical Matching: {category_name}\", total=len(working_df)) as progress_bar:             results = Parallel(n_jobs=-1)(                 delayed(self.process_row_for_lexical_match)(row, reference_df)                 for idx, row in working_df.iterrows()             )                  # Filter out None results         results = [res for res in results if res is not None]                  # Create a DataFrame from the results         df_results_token_set_ratio_combined = pd.DataFrame(results, columns=[             'GOOD_NAME_CL_TR2', 'sim_cand', 'max_value', 'sim_cand_category'])          results_df = df_results_token_set_ratio_combined[df_results_token_set_ratio_combined['max_value'] &gt; threshold].reset_index(drop=True)         remaining_df = df_results_token_set_ratio_combined[df_results_token_set_ratio_combined['max_value'] &lt;= threshold].reset_index(drop=True)                  return results_df, remaining_df           def encode_data(self, column_list):         \"\"\"         Encodes a list of text data into embeddings using a specified model.          Args:             column_list (list): A list of text items to encode.             model (Model): The model used for encoding the text.          Returns:             Tensor: The embeddings tensor generated from the input text.         \"\"\"         pool = self.model.start_multi_process_pool([self.device, self.device, self.device, self.device])                  embeddings = self.model.encode_multi_process(column_list, pool, show_progress_bar=True) # Encoding the text data into embeddings                  self.model.stop_multi_process_pool(pool)                  return embeddings          def convert_to_list(self, embedding_str):         \"\"\"         Converts a string or numpy array representation of embeddings into a list of floats.          Args:             embedding (str or np.ndarray): The embedding, either as a string representation or a numpy array.          Returns:             list: A list of floats extracted from the input.         \"\"\"         if isinstance(embedding_str, str):             # Use regex to find all numbers in the string (handles scientific notation as well)             numbers = re.findall(r\"[-+]?\\d*\\.\\d+e[-+]?\\d+|\\d*\\.\\d+|\\d+\", embedding_str)             # Convert the extracted strings to floats             return [float(num) for num in numbers]         elif isinstance(embedding_str, np.ndarray):             return embedding_str.tolist()         else:             raise TypeError(f\"Unsupported type for embedding: {type(embedding_str)}, {(embedding_str)}\")               def get_embeddings(self, data, column_name,  model, ready_embeddings = False, emb_col = \"emb\"):         \"\"\"         Retrieves and processes embeddings for various categories and names from provided data.          Args:             HDM_data (DataFrame): DataFrame containing GOOD_NAME_CL_TR2 data.             reference_data (DataFrame): DataFrame with sub_category and cleaned_good_name2 data.             model (Model): The model used for encoding the data.          Returns:             tuple: A tuple containing processed query, embeddings, and document data.         \"\"\"         # Ensure the column has no NaN values and is of string type         data[column_name] = data[column_name].fillna('').astype(str)                  if ready_embeddings: #If embeddings are already calculated and are in the data                          # Process data             data[f'{emb_col}_open'] = data[emb_col].apply(self.convert_to_list)             if \"sub_category\" in data.columns:                 data['sub_category'] = data['sub_category'].apply(lambda x: str(x).lower().strip())             else:                 pass             # Embeddings from data             original_data_list = data[column_name].tolist()             original_data_emb =  torch.tensor(data[f'{emb_col}_open'].tolist()).to(self.device) # send preobtained embeddings to device                      else:             original_data_list = data[column_name].tolist()             original_data_emb =  self.encode_data(column_list = original_data_list)                      return original_data_list, original_data_emb      def calculate_similarity(self, docs, doc_embeddings, query, query_emb):         \"\"\"         Calculates the cosine similarity between query embeddings and document embeddings using Faiss.          Args:             docs (list): List of documents.             doc_embeddings (numpy.ndarray): Embeddings of the documents, expected numpy array.             query (str): Query identifier.             query_emb (numpy.ndarray): Embeddings of the query, expected numpy array.          Returns:             DataFrame: A DataFrame containing similarity scores and maximum candidate details.         \"\"\"         start = time.time()         # Ensure the embeddings are in numpy array format         if not isinstance(doc_embeddings, np.ndarray):             doc_embeddings = doc_embeddings.cpu().numpy()         if not isinstance(query_emb, np.ndarray):             query_emb = query_emb.cpu().numpy()                      # Normalize the embeddings to use cosine similarity         faiss.normalize_L2(doc_embeddings)         faiss.normalize_L2(query_emb)          # Create a Faiss index for inner product (cosine similarity)         d = doc_embeddings.shape[1]         index = faiss.IndexFlatIP(d)         index.add(doc_embeddings)          # Perform the search         D, I = index.search(query_emb, k=1)  # Find the most similar document          # Extract the top scores and indices         max_scores = D.flatten()         max_indices = I.flatten()          # Create a DataFrame to store results         similarity_df = pd.DataFrame(columns=['max_cand', 'max_score'], index=[query])         similarity_df['max_cand'] = [docs[idx] for idx in max_indices]         similarity_df['max_score'] = max_scores         print(\"Similarity Calculation Completed in --- %s seconds ---\" % (time.time() - start))         return similarity_df                    def isolated_matching(self, ShetName, HdmData, CheckingList, reference_df, model=None,         lexical_treshold=70, semantic_treshold=0.5, lexical_match=True, semantic_matching=True,         special_words=None, category_name=None, force_semantic_words=None     ):         working_df = self.isolated_data(ShetName, HdmData, CheckingList)          # Check if working_df is empty         if working_df.empty:             print(\"No data to process after initial isolation.\")             return None  # Terminate the function if no data to process          # =======================================================================         #                       Prepare Force Semantic Data         # =======================================================================         if force_semantic_words:             # Create a regex pattern for force_semantic_words             force_semantic_pattern = '|'.join(force_semantic_words)             # Identify entries that contain force_semantic_words             force_semantic_mask = working_df['GOOD_NAME_CL_TR2'].str.contains(                 force_semantic_pattern, case=False, na=False, regex=True             )             force_semantic_df = working_df[force_semantic_mask]             # Remove these entries from working_df to exclude from lexical matching             working_df = working_df[~force_semantic_mask]         else:             force_semantic_df = pd.DataFrame()          # =======================================================================         #                           Lexical Matching         # =======================================================================         if lexical_match:             lexical_result_df, remaining_df = self.lexical_matching(                 working_df, reference_df, threshold=lexical_treshold, category_name=category_name             )              # Scale the matching score of the lexical method             lexical_result_df['max_value'] = lexical_result_df['max_value'] / 100              # Prepare remaining_df for semantic matching             if remaining_df.empty and semantic_matching:                 remaining_df = force_semantic_df.copy()             else:                 # Add force_semantic_df to remaining_df                 remaining_df = pd.concat([remaining_df, force_semantic_df], ignore_index=True).drop_duplicates()         else:             print(f\"Lexical matching not required for {category_name} category\")             remaining_df = pd.concat([working_df, force_semantic_df], ignore_index=True).drop_duplicates()          # =======================================================================         #                           Semantic Matching         # =======================================================================         if semantic_matching and not remaining_df.empty:             semantic_results = self.semantic_matching(remaining_df, reference_df, category_matching = False)                      else:             semantic_results = pd.DataFrame(columns=['GOOD_NAME_CL_TR2', 'sim_cand', 'sim_cand_category', 'max_value'])          # Combine results         if lexical_match and semantic_matching:             final_results = pd.concat([                 lexical_result_df,                 semantic_results[['GOOD_NAME_CL_TR2', 'sim_cand', 'sim_cand_category', 'max_value']]             ], ignore_index=True).reset_index(drop=True)         elif not semantic_matching and lexical_match:             final_results = lexical_result_df         elif not lexical_match and semantic_matching:             final_results = semantic_results[['GOOD_NAME_CL_TR2', 'sim_cand', 'sim_cand_category', 'max_value']]         else:             final_results = pd.DataFrame()              # Merge with original data         combined_working_df = pd.concat([working_df, force_semantic_df], ignore_index=True)         final_result_merged = final_results.drop_duplicates('GOOD_NAME_CL_TR2').merge(             combined_working_df[['GOOD_NAME', 'GOOD_NAME_CL_TR2']],             on='GOOD_NAME_CL_TR2', how='left'         )                  # Check special words         if special_words:             for word in special_words:                 condition = final_result_merged['GOOD_NAME'].str.contains(                     word, case=False, na=False, regex=True                 )                 final_result_merged.loc[condition, 'sim_cand_category'] = special_words[word]                 final_result_merged.loc[condition, 'sim_cand'] = word                 final_result_merged.loc[condition, 'max_value'] = float(0.9)               final_result_merged_final = final_result_merged[             final_result_merged['max_value'] &gt; semantic_treshold         ]                  return final_result_merged_final               def semantic_matching(self, working_data, embeddings_data, category_matching = True):                  # ================================================= Get Embeddings ===================================================================                  query, query_emb = self.get_embeddings(data = working_data, column_name = 'GOOD_NAME_CL_TR2', model = self.model)          # Docs (Good Name on Good Name)         docs_GN, doc_GN_emb = self.get_embeddings(data = embeddings_data, column_name = 'cleaned_good_name2', emb_col = 'emb', model = self.model, ready_embeddings=True)                  # Docs (Good Name on Final Category)         docs_cat, doc_cat_emb = self.get_embeddings(data = embeddings_data, column_name = 'sub_category', emb_col = 'final_cat_emb', model = self.model, ready_embeddings=True)          # ================================================= Matching Good Name on Good Name ===================================================         print(\"Good on good matching\")         good_on_good_df = self.calculate_similarity(docs_GN, doc_GN_emb, query, query_emb)                  del docs_GN, doc_GN_emb,          # Select the relevant columns and reset the index         good_on_good_df = good_on_good_df[['max_cand', 'max_score']].reset_index()                  # Merge with city scrape DataFrame         merged_results = good_on_good_df.merge(             embeddings_data[['sub_category', 'cleaned_good_name2']].drop_duplicates(subset='cleaned_good_name2'),              how='left',              right_on='cleaned_good_name2',              left_on='max_cand'         ).iloc[:, :-1]          # Rename columns for clarity         merged_results = merged_results.rename(             columns={                 'level_0': 'GOOD_NAME_CL_TR2',                 'max_cand': 'sim_cand',                 'max_score': 'max_value',                  'sub_category': 'sim_cand_category',              }         )                  del good_on_good_df         self.empty_device_cache()                  if not category_matching:             return merged_results         # ================================================ Matching Good Name on Final Category ================================================         print(\"Good on category matching\")         good_on_cat_df = self.calculate_similarity(docs_cat, doc_cat_emb, query, query_emb)                   del docs_cat, doc_cat_emb,  query, query_emb,         #print(\"Good Name on Final Category Completed in \\n--- %s seconds ---\" % (time.time() - start_time))          # Merge and refine final results         res_df = merged_results.merge(good_on_cat_df[['max_cand','max_score']].reset_index().drop_duplicates(subset='level_0'), how='left', right_on='level_0',                         left_on='GOOD_NAME_CL_TR2').drop(['level_0'], axis=1).drop_duplicates(subset='GOOD_NAME_CL_TR2')                  res_df = res_df.merge(working_data[['GOOD_NAME', 'GOOD_NAME_CL_TR2']], on='GOOD_NAME_CL_TR2', how='left')                  return res_df            def conditional_decision_logic(self, all_matching_results):                  # Apply conditional decision logic for final results         all_matching_results['sub_category'] = np.where(                                         all_matching_results['max_cand'] == \"books, magazines\",                                         all_matching_results['max_cand'],                                         np.where(all_matching_results['max_score'] &gt;= all_matching_results['max_value'], all_matching_results['max_cand'], all_matching_results['sim_cand_category'])                                         )          all_matching_results['category_score'] = np.where(             all_matching_results['max_cand'] == \"books, magazines\",             all_matching_results['max_score'],  # Assuming you want to keep the original max_score for \"books, magazines\"             np.where(all_matching_results['max_score'] &gt;= all_matching_results['max_value'], all_matching_results['max_score'], all_matching_results['max_value'])         )                  all_matching_results = all_matching_results[['GOOD_NAME', 'sub_category', 'category_score', 'sim_cand', 'max_value', 'GOOD_NAME_CL_TR2']]                  all_matching_results.sub_category = all_matching_results.sub_category.str.lower()                  self.master_data.sub_category = self.master_data.sub_category.str.lower()         all_matching_results = all_matching_results.merge(self.master_data[['sub_category', 'category', 'high_category']].drop_duplicates(subset = 'sub_category'), on = 'sub_category', how = 'left')                  all_matching_results.sub_category = all_matching_results.sub_category.str.capitalize()         all_matching_results = all_matching_results.merge(self.data[['GOOD_NAME','GOOD_NAME_CL_TR','ADG_CODE']],  how='left', on='GOOD_NAME')                  return all_matching_results          def regular_matching_categorization(self):                  res_df = self.semantic_matching(self.data, self.embeddings_data)                  res_df = self.conditional_decision_logic(res_df)                  return res_df          def isolated_matching_categorization(self):         \"\"\"         Processes the matching of GOOD_NAME data with reference data using specified embeddings.          Args:             None          Returns:             DataFrame: The final DataFrame after processing matches and merging data.         \"\"\"                  # =====================================================================================================================================         #                                                 Isolated Matching: Fruits and Veggies Matching          # =====================================================================================================================================                  fruits_results = self.isolated_matching(ShetName = 'Fruits_veggies', HdmData = self.data, CheckingList = self.checking_list_veggies,                                         reference_df = self.target_veggies, semantic_treshold=0.5, model = self.model, lexical_treshold = 61,                                          category_name = \"Fruits and Veggies\", special_words = {\"\u0579\u056b\u0580|\u0579\u0578\u0580|\u0579\u0561\u0574\u056b\u0579\":\"Dried_fruits_and_vegetables\",                                                      \"\u057d\u0561\u057c\":\"Frozen_fruits_vegetables_and_berries\", \"\u0564\u0564\u0574\u056b\u056f\":\"Zucchini\",'\u056f\u0561\u0576\u0561\u0579\u056b':'Greens' }, force_semantic_words = ['cherry'])                  if fruits_results is not None:             # Only execute this code block if fruits_results is a DataFrame             Working_data = self.data[~self.data['GOOD_NAME'].isin(fruits_results['GOOD_NAME'])]         else:             # Handle the case where fruits_results is None             print(\"No fruits results to exclude from HDM data.\")             Working_data = self.data                      # Empty device cahce beofr proceeding to whole data matching         self.empty_device_cache()         # =====================================================================================================================================         #                                                 Isolated Matching: Grain         # =====================================================================================================================================                  grain_results = self.isolated_matching(ShetName = 'Grain',HdmData = Working_data, CheckingList = self.checking_list_groats,                                                 reference_df = self.target_grain, lexical_treshold = 79, semantic_matching= True, model = self.model,                                     semantic_treshold = 0.7, special_words = {'\u0562\u056c\u0572\u0578\u0582\u0580':'Bulgur',  '\u0570\u0576\u0564\u056f\u0561\u0571\u0561\u057e\u0561\u0580':'Buckwheat', '\\b\u0571\u0561\u057e\u0561\u0580\\b':'Wheat_groat',                                                                               '\u057d\u057a\u056b\u057f\u0561\u056f\u0561\u0571\u0561\u057e\u0561\u0580|\u0574\u0561\u0576\u056b':'Semolina',  '\u057d\u056b\u057d\u0565\u057c':'Chickpeas', '\u0563\u0561\u0580\u0565\u0571\u0561\u057e\u0561\u0580':'Pearl_barley',                                                                               '\u0570\u0561\u0573\u0561\u0580':'Emmer', '\u0583\u0578\u056d\u056b\u0576\u0571':'Other_grain', '\u0565\u0563\u056b\u057a\u057f\u0561\u0581\u0578\u0580\u0565\u0576':'Dried_corn', '\u0562\u0580\u056b\u0576\u0571':'Rice',                                                                                '\u0563\u0561\u0580\u0578\u056d|\u0578\u056c\u0578\u057c':'Peas', '\u0578\u057d\u057a':'Lentil', '\u056c\u0578\u0562\u056b':'Bean', '\u056f\u0578\u0580\u0565\u056f\u0561\u0571\u0561\u057e\u0561\u0580|\u056f\u0578\u0580\u0565\u056f':\"Millet_bran\",                                                                                '\u057d\u0561\u0563\u0561\u056d\u0578\u057f|\u056f\u056b\u0576\u0578\u0582\u0561|\u056f\u056b\u0576\u0578\u0561|\u0584\u056b\u0576\u0578\u0561|\u0584\u056b\u0576\u0578\u0582\u0561':'Quinoa','\u0583\u0561\u0569\u056b\u056c\u0576\u0565\u0580':'Flakes', '\u057e\u0561\u0580\u057d\u0561\u056f':'Oat',\"\u0561\u056c\u0575\u0578\u0582\u0580\":\"Flour\",},                                                                                  force_semantic_words = ['flake', 'barley'], category_name = \"Grain\")                  if grain_results is not None:             # Only execute this code block if fruits_results is a DataFrame             Working_data = Working_data[~Working_data['GOOD_NAME'].isin(grain_results['GOOD_NAME'])]         else:             # Handle the case where fruits_results is None             print(\"No grain results to exclude from HDM data.\")             Working_data = Working_data          # Empty device cahce beofr proceeding to whole data matching         self.empty_device_cache()                  # =====================================================================================================================================         #                                                 Isolated Matching: Mix of grain and fresh veggies         # =====================================================================================================================================                  mixed_results = self.isolated_matching(ShetName = 'Mixed_category', HdmData = Working_data, CheckingList = ['marinated'],                                              reference_df = pd.concat([self.target_veggies,self.target_grain]), semantic_treshold=0.6, model = self.model, lexical_treshold = 60,                                              category_name = \"Mixed Category\", force_semantic_words = ['cherry'])                           if mixed_results is not None:             # Only execute this code block if fruits_results is a DataFrame             Working_data = Working_data[~Working_data['GOOD_NAME'].isin(mixed_results['GOOD_NAME'])]         else:             # Handle the case where fruits_results is None             print(\"No mixed category results to exclude from HDM data.\")             Working_data = Working_data                      # Empty device cahce beofr proceeding to whole data matching         self.empty_device_cache()                  # =====================================================================================================================================         #                                                 Isolated Matching: Dairy         # =====================================================================================================================================                  dairy_results = self.isolated_matching(ShetName = 'Dairy', model = self.model, semantic_matching=True,                                 HdmData = Working_data, CheckingList = self.checking_list_diary, reference_df = self.target_diary,  lexical_treshold = 75,  semantic_treshold = 0.7,                                    special_words = {\"\u0569\u0569\u057e\u0561\u057d\u0565\u0580\": \"Sour_cream\",\"\u056f\u0561\u0569\u0576\u0561\u0577\u0578\u057c\": \"Cottage_cheese\",  \"\u057a\u0561\u0576\u056b\u0580\": \"Cheese\",                                                      '\u057d\u0583\u0580\u0565\u0564':'Spread', '\u056f\u0561\u0580\u0561\u0563':'Butter',  \"\u0574\u0561\u056e\u0576\u0561\u0562\u0580\u0564\u0578\u0577\" : \"other_dairy\",                                                     \"\\b\u057d\u0565\u0580\u0578\u0582\u0581\u0584\\b\": \"Cream\", }, category_name = \"Dairy\")          if dairy_results is not None:                 # Only execute this code block if fruits_results is a DataFrame                 Working_data = Working_data[~Working_data['GOOD_NAME'].isin(dairy_results['GOOD_NAME'])]         else:             # Handle the case where fruits_results is None             print(\"No dairy results to exclude from HDM data.\")             Working_data = Working_data          # Empty device cahce beofr proceeding to whole data matching         self.empty_device_cache()                      # =====================================================================================================================================         #                                                 Isolated Matching: Petrol         # =====================================================================================================================================           petrol_results = self.isolated_matching(ShetName = 'Petrol', lexical_treshold= 0, HdmData = Working_data, CheckingList = self.checking_list_petrol,                                          reference_df = self.target_petrol,  semantic_matching= False, category_name = \"Petrol\")           if petrol_results is not None:                 # Only execute this code block if fruits_results is a DataFrame                 Working_data = Working_data[~Working_data['GOOD_NAME'].isin(petrol_results['GOOD_NAME'])]         else:             # Handle the case where fruits_results is None             print(\"No petrol results to exclude from HDM data.\")             Working_data = Working_data          # Empty device cahce beofr proceeding to whole data matching         self.empty_device_cache()                  # =====================================================================================================================================         #                                                 Isolated Matching: Canned Food         # =====================================================================================================================================                  canned_food_results =  self.isolated_matching(ShetName = 'Canned_food', model =  self.model, semantic_treshold = 0.8, semantic_matching=True,                                                     HdmData = Working_data, CheckingList =  self.checking_list_canned_food, reference_df =  self.target_canned_food,                                                      lexical_treshold = 80,  category_name = \"Canned Food\")           if canned_food_results is not None:                 # Only execute this code block if fruits_results is a DataFrame                 Working_data = Working_data[~Working_data['GOOD_NAME'].isin(canned_food_results['GOOD_NAME'])]         else:             # Handle the case where canned_food_results is None             print(\"No canned food results to exclude from HDM data.\")             Working_data = Working_data          # Empty device cahce beofr proceeding to whole data matching         self.empty_device_cache()                  # =====================================================================================================================================         #                                                 Isolated Matching: Meat          # =====================================================================================================================================                  meat_products_results = self.isolated_matching( ShetName = 'meat_products', model = self.model, HdmData = Working_data, CheckingList = self.checking_list_meat_products,                                         reference_df = self.target_meat_products, lexical_match=False, semantic_matching= True,  semantic_treshold=0.75,                                          special_words = {\"\u0565\u0580\u0577\u056b\u056f\": \"Boiled_sausage\",\"\u0576\u0580\u0562\u0565\u0580\u0577\u056b\u056f\": \"Sausages\", \"\u0562\u0561\u057d\u057f\u0578\u0582\u0580\u0574\u0561|\u057d\u0578\u0582\u057b\u0578\u0582\u056d\" : \"Basturma_sujuk\"},                                          category_name = \"Meat and Meat Products\")           if meat_products_results is not None:                 # Only execute this code block if fruits_results is a DataFrame                 Working_data = Working_data[~Working_data['GOOD_NAME'].isin(meat_products_results['GOOD_NAME'])]         else:             # Handle the case where meat_products_results is None             print(\"No meat products results to exclude from HDM data.\")             Working_data = Working_data          # Empty device cahce beofr proceeding to whole data matching         self.empty_device_cache()                      # ================================================ Prepare matching data ==============================================================                  # Debug prints to understand the filtering process         print(\"Initial reference_data length:\", len(self.embeddings_data))         reference_data = self.embeddings_data[~self.embeddings_data['cleaned_good_name2'].str.lower().isin(self.target_veggies['cleaned_good_name2'].str.lower())]         reference_data = reference_data[~reference_data['cleaned_good_name2'].str.lower().isin(self.target_grain['cleaned_good_name2'].str.lower())]         reference_data = reference_data[~reference_data['cleaned_good_name2'].str.lower().isin(self.target_diary['cleaned_good_name2'].str.lower())]         reference_data = reference_data[~reference_data['cleaned_good_name2'].str.lower().isin(self.target_petrol['cleaned_good_name2'].str.lower())]         reference_data = reference_data.drop_duplicates(subset='cleaned_good_name2')         print(\"Final reference_data length after dropping duplicates:\", len(reference_data))                  # =========================================Perform Regular Matching on the rest of the data===========================================                  res_df = self.semantic_matching(Working_data, reference_data)                  all_matches = pd.concat([             fruits_results,             grain_results,             mixed_results,             dairy_results,             petrol_results,             canned_food_results,             meat_products_results,             res_df         ], ignore_index=True)#.drop_duplicates(subset='GOOD_NAME_CL_TR2')                           all_matches = self.conditional_decision_logic(all_matches)                  return all_matches      def match_category(self):         if self.perform_isolated_matching:             self.prepare_isolated_matching_data()             res_df = self.isolated_matching_categorization()         else:             res_df = self.regular_matching_categorization()          self.data = res_df                  return self.data                           def get_model(self):         return SentenceTransformer(\"all-mpnet-base-v2\").to(self.device)      @staticmethod     def find_device():         \"\"\"         Find and return the best available device for computation.         \"\"\"         # Check if MPS is available         if torch.backends.mps.is_available():             print(\"Using MPS device\")             return \"mps\"         # Check if CUDA is available         elif torch.cuda.is_available():             print(\"Using CUDA device\")             return \"cuda\"         # Default to CPU if neither MPS nor CUDA is available         else:             print(\"Using CPU device\")             return \"cpu\"              def empty_device_cache(self):         \"\"\"         Empties the cache of the current device to free up memory.                  This function clears the cache for either MPS on macOS devices, CUDA on GPU-enabled devices, or collects garbage if the device is set to CPU.         \"\"\"         if self.device == \"mps\":             torch.mps.empty_cache()         elif self.device == 'cuda':             torch.cuda.empty_cache()         else:             gc.collect()                  def converter_func(self, x):         \"\"\"         Converts a given value to a zero-padded 4-digit string if the value is numeric.         This function checks if the input `x` is a numeric value and attempts to convert it to a 4-digit string, padded with leading zeros if necessary.                  Args:             x (any): The value to be converted. Can be a number, a string, or `NaN`.         Returns:             str or any: A zero-padded 4-digit string if `x` is a numeric value, or the original value if it cannot be converted.         \"\"\"         if pd.isna(x):             return x          else:             try:                 if isinstance(x, float) and x.is_integer():                     int_x = int(x)                     return f\"{int_x:04d}\"                   else:                                          int_x = int(float(x))                     return f\"{int_x:04d}\"               except ValueError:                                  return x"}, {"location": "docstrings/py_doc_Strings/docs/", "title": "Welcome to MkDocs", "text": "<p>For full documentation visit mkdocs.org.</p>"}, {"location": "docstrings/py_doc_Strings/docs/#commands", "title": "Commands", "text": "<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"}, {"location": "docstrings/py_doc_Strings/docs/#project-layout", "title": "Project layout", "text": "<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"}, {"location": "docstrings/py_doc_Strings/docs/docstrings/", "title": "Overview", "text": ""}, {"location": "docstrings/py_doc_Strings/docs/docstrings/#py_doc_Strings.matching.NlpPilepine", "title": "<code>NlpPilepine</code>", "text": "Source code in <code>projects/docstrings/py_doc_Strings/matching.py</code> <pre><code>class NlpPilepine:\n\n    def __init__(self, embeddings_path, data_path, grain_emb_path , veggies_emb_path,\n                 dairy_emb_path , petrol_emb_path , data_source = None,matching_data_params_path = None, filtering_atg_codes_path = None):\n\n        self.perform_isolated_matching = False if data_source == 'Inv' else True\n        self.matching_data_params_path = matching_data_params_path\n        self.filtering_atg_codes_path = filtering_atg_codes_path\n\n\n        self.grain_emb = pd.read_csv(grain_emb_path)\n        self.veggies_emb = pd.read_csv(veggies_emb_path)\n        self.diary_emb = pd.read_csv(dairy_emb_path)\n        self.petrol_emb = pd.read_csv(petrol_emb_path)\n        self.embeddings_data = pd.read_csv(embeddings_path)\n\n        self.master_data = pd.concat([self.embeddings_data,\n                                              self.grain_emb,\n                                              self.veggies_emb,\n                                              self.diary_emb,\n                                              self.petrol_emb])\n\n        if self.perform_isolated_matching:\n            pass\n        else:\n            self.embeddings_data = self.master_data\n\n        self.data = pd.read_csv(data_path, converters={'ADG_CODE': self.converter_func})\n\n        self.device = self.find_device()\n        self.model = self.get_model()\n\n\n    # prepare reference data for isolated matching\n    def prepare_isolated_matching_data(self):\n            with open(self.matching_data_params_path, 'r') as json_file:\n                isolated_data_params = json.load(json_file)\n            isolated_data_params = DotMap(isolated_data_params)\n            #prepare matching data\n            columns = [\"cleaned_good_name2\", \"sub_category\", 'emb', 'final_cat_emb']\n            self.target_veggies = pd.concat([self.embeddings_data[(self.embeddings_data['category'].isin(isolated_data_params.veggies.reference_data.from_category)) &amp; \n                                        (~self.embeddings_data['sub_category'].isin(isolated_data_params.veggies.reference_data.not_from_subcategory))\n                                        &amp; ~((self.embeddings_data['product_name'].str.contains(isolated_data_params.veggies.reference_data.product_name_contains, case=False)) &amp; \n                                            (self.embeddings_data['sub_category'] != isolated_data_params.veggies.reference_data.product_name_contains))][columns],\n                                        self.veggies_emb[columns],]).reset_index(drop=True)\n\n            self.target_veggies.loc[self.target_veggies.cleaned_good_name2.str.contains('asparagus',case=False ), 'sub_category'] = 'Asparagus'\n\n            self.target_grain = pd.concat([\n                self.embeddings_data[(self.embeddings_data['category'].isin(isolated_data_params.grain.reference_data.from_category)) &amp; \n                            (~self.embeddings_data['sub_category'].isin(isolated_data_params.grain.reference_data.not_from_sub_category))][columns],\n                self.embeddings_data[(self.embeddings_data['sub_category'].isin(isolated_data_params.grain.reference_data.from_subcategory))][columns],\n                self.grain_emb[columns],\n            ]).reset_index(drop=True)\n\n            self.target_diary = pd.concat([\n                self.embeddings_data[(self.embeddings_data['category'].isin(isolated_data_params.dairy.reference_data.from_category)) &amp; \n                            (~self.embeddings_data['sub_category'].isin(isolated_data_params.dairy.reference_data.not_from_subcategory))][columns],\n                self.diary_emb[columns]]).reset_index(drop=True)\n\n\n            self.target_petrol  = pd.concat([\n                self.embeddings_data[self.embeddings_data['category'].isin(isolated_data_params.petrol.reference_data.from_category)][columns],\n                self.petrol_emb[columns],\n            ]).reset_index(drop=True)\n\n            self.target_canned_food  = self.embeddings_data[self.embeddings_data['category'].isin(isolated_data_params.canned_food.reference_data.from_category) &amp;\n                                                (~self.embeddings_data['sub_category'].isin(isolated_data_params.canned_food.reference_data.not_from_subcategory))][columns]\n\n            self.target_meat_products  = self.embeddings_data[self.embeddings_data['category'].isin(isolated_data_params.meat_products.reference_data.from_category) &amp; \n                                                    (~self.embeddings_data['sub_category'].isin(isolated_data_params.meat_products.reference_data.not_from_subcategory))][columns]\n\n            #prepare checking data\n            isolated_data_params.veggies.checking_list_veggies.original_list.extend(self.embeddings_data[ \n            (self.embeddings_data['category'].isin(isolated_data_params.veggies.checking_list_veggies.extension_category)) | \n            (self.embeddings_data['sub_category'].isin(isolated_data_params.veggies.checking_list_veggies.extension_subcategory))][\"brand\"].str.replace('_',' ').dropna().unique().tolist())\n\n            isolated_data_params.grain.checking_list_groats.original_list.extend(self.embeddings_data[self.embeddings_data['category'].isin(isolated_data_params.grain.checking_list_groats.extension_category)][\"brand\"].str.replace('_',' ').dropna().unique().tolist())\n\n            self.checking_list_veggies = isolated_data_params.veggies.checking_list_veggies.original_list\n            self.checking_list_groats = isolated_data_params.grain.checking_list_groats.original_list\n            self.checking_list_diary = isolated_data_params.dairy.checking_list_diary.original_list\n            self.checking_list_petrol = isolated_data_params.petrol.checking_list_petrol.original_list\n            self.checking_list_canned_food = isolated_data_params.canned_food.checking_list_canned_food.original_list\n            self.checking_list_meat_products = isolated_data_params.meat_products.checking_list_meat_products.original_list\n\n    # isolate data for isolated matching\n    def isolated_data(self, SheetName, HdmData, CheckingList):\n\n        # loading atg codes data\n        filtering_atg_codes_df = pd.read_excel(self.filtering_atg_codes_path, sheet_name=SheetName, converters={'ATG CODES': self.converter_func})\n        filtering_atg_codes = filtering_atg_codes_df['ATG CODES'].to_list()\n        # getting working data\n        working_df = HdmData[HdmData['ADG_CODE'].isin(filtering_atg_codes)]\n\n        checking_list = [x.lower() for x in CheckingList]\n        checking_pattern = r'\\b(?:' + '|'.join(checking_list) + r')\\b'\n\n        working_df = working_df[working_df['GOOD_NAME_CL_TR2'].notna()]\n        working_df = working_df[~working_df['GOOD_NAME_CL_TR2'].str.lower().str.contains(checking_pattern, regex=True)]\n\n        return working_df\n\n\n    # ================================= Lexical Matching codes ===================================\n    @staticmethod\n    def adjusted_token_set_ratio(query, choice):\n        # Compute the token set ratio score\n        score = fuzz.token_set_ratio(query, choice)\n\n        # Calculate word sets\n        query_words = set(query.lower().split())\n        choice_words = set(choice.lower().split())\n\n        # Calculate the proportion of matched words\n        matched_words = query_words.intersection(choice_words)\n        total_words = query_words.union(choice_words)\n        #word_match_proportion = len(matched_words) / len(total_words) if total_words else 0\n        unmatched_proportion = (len(total_words) - len(matched_words)) / len(total_words) if total_words else 0\n\n        # Adjust the score by multiplying with the word match proportion\n        adjusted_score = score - unmatched_proportion * 20\n\n        return adjusted_score\n\n    # lexical matching  \n    def find_best_match_token_set_ratio(self, good_name, reference_df):\n        choices = reference_df['cleaned_good_name2'].tolist()\n\n        # Proceed with matching\n        if good_name.strip():\n            # Use the standard scorer\n            best_match = process.extractOne(good_name, choices, scorer= self.adjusted_token_set_ratio)\n            return best_match\n        else:\n            return None\n\n    # helper function for lexical matching, process one row\n    def process_row_for_lexical_match(self, row, reference_df):\n        good_name = row['GOOD_NAME_CL_TR2']\n\n        # Skip if good_name is just punctuation or whitespace\n        if not good_name or good_name.isspace():\n            return None\n\n        best_match = self.find_best_match_token_set_ratio(good_name, reference_df)\n\n        if best_match:\n            best_value, best_score = best_match[0], best_match[1]\n            match = reference_df.loc[reference_df['cleaned_good_name2'] == best_value, 'sub_category']\n            if not match.empty:\n                best_category = match.values[0]\n\n            return good_name, best_value, best_score, best_category\n        return None\n\n    # parallelize lexical matching codes\n    def lexical_matching(self, working_df, reference_df, threshold=70, category_name=None):\n        with tqdm_joblib(desc=f\"Lexical Matching: {category_name}\", total=len(working_df)) as progress_bar:\n            results = Parallel(n_jobs=-1)(\n                delayed(self.process_row_for_lexical_match)(row, reference_df)\n                for idx, row in working_df.iterrows()\n            )\n\n        # Filter out None results\n        results = [res for res in results if res is not None]\n\n        # Create a DataFrame from the results\n        df_results_token_set_ratio_combined = pd.DataFrame(results, columns=[\n            'GOOD_NAME_CL_TR2', 'sim_cand', 'max_value', 'sim_cand_category'])\n\n        results_df = df_results_token_set_ratio_combined[df_results_token_set_ratio_combined['max_value'] &gt; threshold].reset_index(drop=True)\n        remaining_df = df_results_token_set_ratio_combined[df_results_token_set_ratio_combined['max_value'] &lt;= threshold].reset_index(drop=True)\n\n        return results_df, remaining_df\n\n\n    def encode_data(self, column_list):\n        \"\"\"\n        Encodes a list of text data into embeddings using a specified model.\n\n        Args:\n            column_list (list): A list of text items to encode.\n            model (Model): The model used for encoding the text.\n\n        Returns:\n            Tensor: The embeddings tensor generated from the input text.\n        \"\"\"\n        pool = self.model.start_multi_process_pool([self.device, self.device, self.device, self.device])\n\n        embeddings = self.model.encode_multi_process(column_list, pool, show_progress_bar=True) # Encoding the text data into embeddings\n\n        self.model.stop_multi_process_pool(pool)\n\n        return embeddings\n\n    def convert_to_list(self, embedding_str):\n        \"\"\"\n        Converts a string or numpy array representation of embeddings into a list of floats.\n\n        Args:\n            embedding (str or np.ndarray): The embedding, either as a string representation or a numpy array.\n\n        Returns:\n            list: A list of floats extracted from the input.\n        \"\"\"\n        if isinstance(embedding_str, str):\n            # Use regex to find all numbers in the string (handles scientific notation as well)\n            numbers = re.findall(r\"[-+]?\\d*\\.\\d+e[-+]?\\d+|\\d*\\.\\d+|\\d+\", embedding_str)\n            # Convert the extracted strings to floats\n            return [float(num) for num in numbers]\n        elif isinstance(embedding_str, np.ndarray):\n            return embedding_str.tolist()\n        else:\n            raise TypeError(f\"Unsupported type for embedding: {type(embedding_str)}, {(embedding_str)}\")\n\n\n    def get_embeddings(self, data, column_name,  model, ready_embeddings = False, emb_col = \"emb\"):\n        \"\"\"\n        Retrieves and processes embeddings for various categories and names from provided data.\n\n        Args:\n            HDM_data (DataFrame): DataFrame containing GOOD_NAME_CL_TR2 data.\n            reference_data (DataFrame): DataFrame with sub_category and cleaned_good_name2 data.\n            model (Model): The model used for encoding the data.\n\n        Returns:\n            tuple: A tuple containing processed query, embeddings, and document data.\n        \"\"\"\n        # Ensure the column has no NaN values and is of string type\n        data[column_name] = data[column_name].fillna('').astype(str)\n\n        if ready_embeddings: #If embeddings are already calculated and are in the data\n\n            # Process data\n            data[f'{emb_col}_open'] = data[emb_col].apply(self.convert_to_list)\n            if \"sub_category\" in data.columns:\n                data['sub_category'] = data['sub_category'].apply(lambda x: str(x).lower().strip())\n            else:\n                pass\n            # Embeddings from data\n            original_data_list = data[column_name].tolist()\n            original_data_emb =  torch.tensor(data[f'{emb_col}_open'].tolist()).to(self.device) # send preobtained embeddings to device\n\n        else:\n            original_data_list = data[column_name].tolist()\n            original_data_emb =  self.encode_data(column_list = original_data_list)\n\n        return original_data_list, original_data_emb\n\n    def calculate_similarity(self, docs, doc_embeddings, query, query_emb):\n        \"\"\"\n        Calculates the cosine similarity between query embeddings and document embeddings using Faiss.\n\n        Args:\n            docs (list): List of documents.\n            doc_embeddings (numpy.ndarray): Embeddings of the documents, expected numpy array.\n            query (str): Query identifier.\n            query_emb (numpy.ndarray): Embeddings of the query, expected numpy array.\n\n        Returns:\n            DataFrame: A DataFrame containing similarity scores and maximum candidate details.\n        \"\"\"\n        start = time.time()\n        # Ensure the embeddings are in numpy array format\n        if not isinstance(doc_embeddings, np.ndarray):\n            doc_embeddings = doc_embeddings.cpu().numpy()\n        if not isinstance(query_emb, np.ndarray):\n            query_emb = query_emb.cpu().numpy()\n\n        # Normalize the embeddings to use cosine similarity\n        faiss.normalize_L2(doc_embeddings)\n        faiss.normalize_L2(query_emb)\n\n        # Create a Faiss index for inner product (cosine similarity)\n        d = doc_embeddings.shape[1]\n        index = faiss.IndexFlatIP(d)\n        index.add(doc_embeddings)\n\n        # Perform the search\n        D, I = index.search(query_emb, k=1)  # Find the most similar document\n\n        # Extract the top scores and indices\n        max_scores = D.flatten()\n        max_indices = I.flatten()\n\n        # Create a DataFrame to store results\n        similarity_df = pd.DataFrame(columns=['max_cand', 'max_score'], index=[query])\n        similarity_df['max_cand'] = [docs[idx] for idx in max_indices]\n        similarity_df['max_score'] = max_scores\n        print(\"Similarity Calculation Completed in --- %s seconds ---\" % (time.time() - start))\n        return similarity_df\n\n\n\n    def isolated_matching(self, ShetName, HdmData, CheckingList, reference_df, model=None,\n        lexical_treshold=70, semantic_treshold=0.5, lexical_match=True, semantic_matching=True,\n        special_words=None, category_name=None, force_semantic_words=None\n    ):\n        working_df = self.isolated_data(ShetName, HdmData, CheckingList)\n\n        # Check if working_df is empty\n        if working_df.empty:\n            print(\"No data to process after initial isolation.\")\n            return None  # Terminate the function if no data to process\n\n        # =======================================================================\n        #                       Prepare Force Semantic Data\n        # =======================================================================\n        if force_semantic_words:\n            # Create a regex pattern for force_semantic_words\n            force_semantic_pattern = '|'.join(force_semantic_words)\n            # Identify entries that contain force_semantic_words\n            force_semantic_mask = working_df['GOOD_NAME_CL_TR2'].str.contains(\n                force_semantic_pattern, case=False, na=False, regex=True\n            )\n            force_semantic_df = working_df[force_semantic_mask]\n            # Remove these entries from working_df to exclude from lexical matching\n            working_df = working_df[~force_semantic_mask]\n        else:\n            force_semantic_df = pd.DataFrame()\n\n        # =======================================================================\n        #                           Lexical Matching\n        # =======================================================================\n        if lexical_match:\n            lexical_result_df, remaining_df = self.lexical_matching(\n                working_df, reference_df, threshold=lexical_treshold, category_name=category_name\n            )\n\n            # Scale the matching score of the lexical method\n            lexical_result_df['max_value'] = lexical_result_df['max_value'] / 100\n\n            # Prepare remaining_df for semantic matching\n            if remaining_df.empty and semantic_matching:\n                remaining_df = force_semantic_df.copy()\n            else:\n                # Add force_semantic_df to remaining_df\n                remaining_df = pd.concat([remaining_df, force_semantic_df], ignore_index=True).drop_duplicates()\n        else:\n            print(f\"Lexical matching not required for {category_name} category\")\n            remaining_df = pd.concat([working_df, force_semantic_df], ignore_index=True).drop_duplicates()\n\n        # =======================================================================\n        #                           Semantic Matching\n        # =======================================================================\n        if semantic_matching and not remaining_df.empty:\n            semantic_results = self.semantic_matching(remaining_df, reference_df, category_matching = False)\n\n        else:\n            semantic_results = pd.DataFrame(columns=['GOOD_NAME_CL_TR2', 'sim_cand', 'sim_cand_category', 'max_value'])\n\n        # Combine results\n        if lexical_match and semantic_matching:\n            final_results = pd.concat([\n                lexical_result_df,\n                semantic_results[['GOOD_NAME_CL_TR2', 'sim_cand', 'sim_cand_category', 'max_value']]\n            ], ignore_index=True).reset_index(drop=True)\n        elif not semantic_matching and lexical_match:\n            final_results = lexical_result_df\n        elif not lexical_match and semantic_matching:\n            final_results = semantic_results[['GOOD_NAME_CL_TR2', 'sim_cand', 'sim_cand_category', 'max_value']]\n        else:\n            final_results = pd.DataFrame()\n\n        # Merge with original data\n        combined_working_df = pd.concat([working_df, force_semantic_df], ignore_index=True)\n        final_result_merged = final_results.drop_duplicates('GOOD_NAME_CL_TR2').merge(\n            combined_working_df[['GOOD_NAME', 'GOOD_NAME_CL_TR2']],\n            on='GOOD_NAME_CL_TR2', how='left'\n        )\n\n        # Check special words\n        if special_words:\n            for word in special_words:\n                condition = final_result_merged['GOOD_NAME'].str.contains(\n                    word, case=False, na=False, regex=True\n                )\n                final_result_merged.loc[condition, 'sim_cand_category'] = special_words[word]\n                final_result_merged.loc[condition, 'sim_cand'] = word\n                final_result_merged.loc[condition, 'max_value'] = float(0.9)\n\n\n        final_result_merged_final = final_result_merged[\n            final_result_merged['max_value'] &gt; semantic_treshold\n        ]\n\n        return final_result_merged_final\n\n\n    def semantic_matching(self, working_data, embeddings_data, category_matching = True):\n\n        # ================================================= Get Embeddings ===================================================================\n\n        query, query_emb = self.get_embeddings(data = working_data, column_name = 'GOOD_NAME_CL_TR2', model = self.model)\n\n        # Docs (Good Name on Good Name)\n        docs_GN, doc_GN_emb = self.get_embeddings(data = embeddings_data, column_name = 'cleaned_good_name2', emb_col = 'emb', model = self.model, ready_embeddings=True)\n\n        # Docs (Good Name on Final Category)\n        docs_cat, doc_cat_emb = self.get_embeddings(data = embeddings_data, column_name = 'sub_category', emb_col = 'final_cat_emb', model = self.model, ready_embeddings=True)\n\n        # ================================================= Matching Good Name on Good Name ===================================================\n        print(\"Good on good matching\")\n        good_on_good_df = self.calculate_similarity(docs_GN, doc_GN_emb, query, query_emb)\n\n        del docs_GN, doc_GN_emb,\n\n        # Select the relevant columns and reset the index\n        good_on_good_df = good_on_good_df[['max_cand', 'max_score']].reset_index()\n\n        # Merge with city scrape DataFrame\n        merged_results = good_on_good_df.merge(\n            embeddings_data[['sub_category', 'cleaned_good_name2']].drop_duplicates(subset='cleaned_good_name2'), \n            how='left', \n            right_on='cleaned_good_name2', \n            left_on='max_cand'\n        ).iloc[:, :-1]\n\n        # Rename columns for clarity\n        merged_results = merged_results.rename(\n            columns={\n                'level_0': 'GOOD_NAME_CL_TR2',\n                'max_cand': 'sim_cand',\n                'max_score': 'max_value', \n                'sub_category': 'sim_cand_category', \n            }\n        )\n\n        del good_on_good_df\n        self.empty_device_cache()\n\n        if not category_matching:\n            return merged_results\n        # ================================================ Matching Good Name on Final Category ================================================\n        print(\"Good on category matching\")\n        good_on_cat_df = self.calculate_similarity(docs_cat, doc_cat_emb, query, query_emb) \n\n        del docs_cat, doc_cat_emb,  query, query_emb,\n        #print(\"Good Name on Final Category Completed in \\n--- %s seconds ---\" % (time.time() - start_time))\n\n        # Merge and refine final results\n        res_df = merged_results.merge(good_on_cat_df[['max_cand','max_score']].reset_index().drop_duplicates(subset='level_0'), how='left', right_on='level_0',\n                        left_on='GOOD_NAME_CL_TR2').drop(['level_0'], axis=1).drop_duplicates(subset='GOOD_NAME_CL_TR2')\n\n        res_df = res_df.merge(working_data[['GOOD_NAME', 'GOOD_NAME_CL_TR2']], on='GOOD_NAME_CL_TR2', how='left')\n\n        return res_df\n\n    def conditional_decision_logic(self, all_matching_results):\n\n        # Apply conditional decision logic for final results\n        all_matching_results['sub_category'] = np.where(\n                                        all_matching_results['max_cand'] == \"books, magazines\",\n                                        all_matching_results['max_cand'],\n                                        np.where(all_matching_results['max_score'] &gt;= all_matching_results['max_value'], all_matching_results['max_cand'], all_matching_results['sim_cand_category'])\n                                        )\n\n        all_matching_results['category_score'] = np.where(\n            all_matching_results['max_cand'] == \"books, magazines\",\n            all_matching_results['max_score'],  # Assuming you want to keep the original max_score for \"books, magazines\"\n            np.where(all_matching_results['max_score'] &gt;= all_matching_results['max_value'], all_matching_results['max_score'], all_matching_results['max_value'])\n        )\n\n        all_matching_results = all_matching_results[['GOOD_NAME', 'sub_category', 'category_score', 'sim_cand', 'max_value', 'GOOD_NAME_CL_TR2']]\n\n        all_matching_results.sub_category = all_matching_results.sub_category.str.lower()\n\n        self.master_data.sub_category = self.master_data.sub_category.str.lower()\n        all_matching_results = all_matching_results.merge(self.master_data[['sub_category', 'category', 'high_category']].drop_duplicates(subset = 'sub_category'), on = 'sub_category', how = 'left')\n\n        all_matching_results.sub_category = all_matching_results.sub_category.str.capitalize()\n        all_matching_results = all_matching_results.merge(self.data[['GOOD_NAME','GOOD_NAME_CL_TR','ADG_CODE']],  how='left', on='GOOD_NAME')\n\n        return all_matching_results\n\n    def regular_matching_categorization(self):\n\n        res_df = self.semantic_matching(self.data, self.embeddings_data)\n\n        res_df = self.conditional_decision_logic(res_df)\n\n        return res_df\n\n    def isolated_matching_categorization(self):\n        \"\"\"\n        Processes the matching of GOOD_NAME data with reference data using specified embeddings.\n\n        Args:\n            None\n\n        Returns:\n            DataFrame: The final DataFrame after processing matches and merging data.\n        \"\"\"\n\n        # =====================================================================================================================================\n        #                                                 Isolated Matching: Fruits and Veggies Matching \n        # =====================================================================================================================================\n\n        fruits_results = self.isolated_matching(ShetName = 'Fruits_veggies', HdmData = self.data, CheckingList = self.checking_list_veggies,\n                                        reference_df = self.target_veggies, semantic_treshold=0.5, model = self.model, lexical_treshold = 61, \n                                        category_name = \"Fruits and Veggies\", special_words = {\"\u0579\u056b\u0580|\u0579\u0578\u0580|\u0579\u0561\u0574\u056b\u0579\":\"Dried_fruits_and_vegetables\", \n                                                    \"\u057d\u0561\u057c\":\"Frozen_fruits_vegetables_and_berries\", \"\u0564\u0564\u0574\u056b\u056f\":\"Zucchini\",'\u056f\u0561\u0576\u0561\u0579\u056b':'Greens' }, force_semantic_words = ['cherry'])\n\n        if fruits_results is not None:\n            # Only execute this code block if fruits_results is a DataFrame\n            Working_data = self.data[~self.data['GOOD_NAME'].isin(fruits_results['GOOD_NAME'])]\n        else:\n            # Handle the case where fruits_results is None\n            print(\"No fruits results to exclude from HDM data.\")\n            Working_data = self.data\n\n        # Empty device cahce beofr proceeding to whole data matching\n        self.empty_device_cache()\n        # =====================================================================================================================================\n        #                                                 Isolated Matching: Grain\n        # =====================================================================================================================================\n\n        grain_results = self.isolated_matching(ShetName = 'Grain',HdmData = Working_data, CheckingList = self.checking_list_groats,\n                                                reference_df = self.target_grain, lexical_treshold = 79, semantic_matching= True, model = self.model,\n                                    semantic_treshold = 0.7, special_words = {'\u0562\u056c\u0572\u0578\u0582\u0580':'Bulgur',  '\u0570\u0576\u0564\u056f\u0561\u0571\u0561\u057e\u0561\u0580':'Buckwheat', '\\b\u0571\u0561\u057e\u0561\u0580\\b':'Wheat_groat',\n                                                                              '\u057d\u057a\u056b\u057f\u0561\u056f\u0561\u0571\u0561\u057e\u0561\u0580|\u0574\u0561\u0576\u056b':'Semolina',  '\u057d\u056b\u057d\u0565\u057c':'Chickpeas', '\u0563\u0561\u0580\u0565\u0571\u0561\u057e\u0561\u0580':'Pearl_barley',\n                                                                              '\u0570\u0561\u0573\u0561\u0580':'Emmer', '\u0583\u0578\u056d\u056b\u0576\u0571':'Other_grain', '\u0565\u0563\u056b\u057a\u057f\u0561\u0581\u0578\u0580\u0565\u0576':'Dried_corn', '\u0562\u0580\u056b\u0576\u0571':'Rice',\n                                                                               '\u0563\u0561\u0580\u0578\u056d|\u0578\u056c\u0578\u057c':'Peas', '\u0578\u057d\u057a':'Lentil', '\u056c\u0578\u0562\u056b':'Bean', '\u056f\u0578\u0580\u0565\u056f\u0561\u0571\u0561\u057e\u0561\u0580|\u056f\u0578\u0580\u0565\u056f':\"Millet_bran\",\n                                                                               '\u057d\u0561\u0563\u0561\u056d\u0578\u057f|\u056f\u056b\u0576\u0578\u0582\u0561|\u056f\u056b\u0576\u0578\u0561|\u0584\u056b\u0576\u0578\u0561|\u0584\u056b\u0576\u0578\u0582\u0561':'Quinoa','\u0583\u0561\u0569\u056b\u056c\u0576\u0565\u0580':'Flakes', '\u057e\u0561\u0580\u057d\u0561\u056f':'Oat',\"\u0561\u056c\u0575\u0578\u0582\u0580\":\"Flour\",}, \n                                                                                force_semantic_words = ['flake', 'barley'], category_name = \"Grain\")\n\n        if grain_results is not None:\n            # Only execute this code block if fruits_results is a DataFrame\n            Working_data = Working_data[~Working_data['GOOD_NAME'].isin(grain_results['GOOD_NAME'])]\n        else:\n            # Handle the case where fruits_results is None\n            print(\"No grain results to exclude from HDM data.\")\n            Working_data = Working_data\n\n        # Empty device cahce beofr proceeding to whole data matching\n        self.empty_device_cache()\n\n        # =====================================================================================================================================\n        #                                                 Isolated Matching: Mix of grain and fresh veggies\n        # =====================================================================================================================================\n\n        mixed_results = self.isolated_matching(ShetName = 'Mixed_category', HdmData = Working_data, CheckingList = ['marinated'], \n                                            reference_df = pd.concat([self.target_veggies,self.target_grain]), semantic_treshold=0.6, model = self.model, lexical_treshold = 60, \n                                            category_name = \"Mixed Category\", force_semantic_words = ['cherry'])\n\n\n        if mixed_results is not None:\n            # Only execute this code block if fruits_results is a DataFrame\n            Working_data = Working_data[~Working_data['GOOD_NAME'].isin(mixed_results['GOOD_NAME'])]\n        else:\n            # Handle the case where fruits_results is None\n            print(\"No mixed category results to exclude from HDM data.\")\n            Working_data = Working_data\n\n        # Empty device cahce beofr proceeding to whole data matching\n        self.empty_device_cache()\n\n        # =====================================================================================================================================\n        #                                                 Isolated Matching: Dairy\n        # =====================================================================================================================================\n\n        dairy_results = self.isolated_matching(ShetName = 'Dairy', model = self.model, semantic_matching=True,\n                                HdmData = Working_data, CheckingList = self.checking_list_diary, reference_df = self.target_diary,  lexical_treshold = 75,  semantic_treshold = 0.7,\n                                   special_words = {\"\u0569\u0569\u057e\u0561\u057d\u0565\u0580\": \"Sour_cream\",\"\u056f\u0561\u0569\u0576\u0561\u0577\u0578\u057c\": \"Cottage_cheese\",  \"\u057a\u0561\u0576\u056b\u0580\": \"Cheese\", \n                                                    '\u057d\u0583\u0580\u0565\u0564':'Spread', '\u056f\u0561\u0580\u0561\u0563':'Butter',  \"\u0574\u0561\u056e\u0576\u0561\u0562\u0580\u0564\u0578\u0577\" : \"other_dairy\",\n                                                    \"\\b\u057d\u0565\u0580\u0578\u0582\u0581\u0584\\b\": \"Cream\", }, category_name = \"Dairy\")\n\n        if dairy_results is not None:\n                # Only execute this code block if fruits_results is a DataFrame\n                Working_data = Working_data[~Working_data['GOOD_NAME'].isin(dairy_results['GOOD_NAME'])]\n        else:\n            # Handle the case where fruits_results is None\n            print(\"No dairy results to exclude from HDM data.\")\n            Working_data = Working_data\n\n        # Empty device cahce beofr proceeding to whole data matching\n        self.empty_device_cache()\n\n        # =====================================================================================================================================\n        #                                                 Isolated Matching: Petrol\n        # =====================================================================================================================================  \n        petrol_results = self.isolated_matching(ShetName = 'Petrol', lexical_treshold= 0, HdmData = Working_data, CheckingList = self.checking_list_petrol,\n                                         reference_df = self.target_petrol,  semantic_matching= False, category_name = \"Petrol\")\n\n\n        if petrol_results is not None:\n                # Only execute this code block if fruits_results is a DataFrame\n                Working_data = Working_data[~Working_data['GOOD_NAME'].isin(petrol_results['GOOD_NAME'])]\n        else:\n            # Handle the case where fruits_results is None\n            print(\"No petrol results to exclude from HDM data.\")\n            Working_data = Working_data\n\n        # Empty device cahce beofr proceeding to whole data matching\n        self.empty_device_cache()\n\n        # =====================================================================================================================================\n        #                                                 Isolated Matching: Canned Food\n        # =====================================================================================================================================\n\n        canned_food_results =  self.isolated_matching(ShetName = 'Canned_food', model =  self.model, semantic_treshold = 0.8, semantic_matching=True,\n                                                    HdmData = Working_data, CheckingList =  self.checking_list_canned_food, reference_df =  self.target_canned_food, \n                                                    lexical_treshold = 80,  category_name = \"Canned Food\")\n\n\n        if canned_food_results is not None:\n                # Only execute this code block if fruits_results is a DataFrame\n                Working_data = Working_data[~Working_data['GOOD_NAME'].isin(canned_food_results['GOOD_NAME'])]\n        else:\n            # Handle the case where canned_food_results is None\n            print(\"No canned food results to exclude from HDM data.\")\n            Working_data = Working_data\n\n        # Empty device cahce beofr proceeding to whole data matching\n        self.empty_device_cache()\n\n        # =====================================================================================================================================\n        #                                                 Isolated Matching: Meat \n        # =====================================================================================================================================\n\n        meat_products_results = self.isolated_matching( ShetName = 'meat_products', model = self.model, HdmData = Working_data, CheckingList = self.checking_list_meat_products,\n                                        reference_df = self.target_meat_products, lexical_match=False, semantic_matching= True,  semantic_treshold=0.75, \n                                        special_words = {\"\u0565\u0580\u0577\u056b\u056f\": \"Boiled_sausage\",\"\u0576\u0580\u0562\u0565\u0580\u0577\u056b\u056f\": \"Sausages\", \"\u0562\u0561\u057d\u057f\u0578\u0582\u0580\u0574\u0561|\u057d\u0578\u0582\u057b\u0578\u0582\u056d\" : \"Basturma_sujuk\"}, \n                                        category_name = \"Meat and Meat Products\")\n\n\n        if meat_products_results is not None:\n                # Only execute this code block if fruits_results is a DataFrame\n                Working_data = Working_data[~Working_data['GOOD_NAME'].isin(meat_products_results['GOOD_NAME'])]\n        else:\n            # Handle the case where meat_products_results is None\n            print(\"No meat products results to exclude from HDM data.\")\n            Working_data = Working_data\n\n        # Empty device cahce beofr proceeding to whole data matching\n        self.empty_device_cache()\n\n        # ================================================ Prepare matching data ==============================================================\n\n        # Debug prints to understand the filtering process\n        print(\"Initial reference_data length:\", len(self.embeddings_data))\n        reference_data = self.embeddings_data[~self.embeddings_data['cleaned_good_name2'].str.lower().isin(self.target_veggies['cleaned_good_name2'].str.lower())]\n        reference_data = reference_data[~reference_data['cleaned_good_name2'].str.lower().isin(self.target_grain['cleaned_good_name2'].str.lower())]\n        reference_data = reference_data[~reference_data['cleaned_good_name2'].str.lower().isin(self.target_diary['cleaned_good_name2'].str.lower())]\n        reference_data = reference_data[~reference_data['cleaned_good_name2'].str.lower().isin(self.target_petrol['cleaned_good_name2'].str.lower())]\n        reference_data = reference_data.drop_duplicates(subset='cleaned_good_name2')\n        print(\"Final reference_data length after dropping duplicates:\", len(reference_data))\n\n        # =========================================Perform Regular Matching on the rest of the data===========================================\n\n        res_df = self.semantic_matching(Working_data, reference_data)\n\n        all_matches = pd.concat([\n            fruits_results,\n            grain_results,\n            mixed_results,\n            dairy_results,\n            petrol_results,\n            canned_food_results,\n            meat_products_results,\n            res_df\n        ], ignore_index=True)#.drop_duplicates(subset='GOOD_NAME_CL_TR2')\n\n\n        all_matches = self.conditional_decision_logic(all_matches)\n\n        return all_matches\n\n    def match_category(self):\n        if self.perform_isolated_matching:\n            self.prepare_isolated_matching_data()\n            res_df = self.isolated_matching_categorization()\n        else:\n            res_df = self.regular_matching_categorization()\n\n        self.data = res_df\n\n        return self.data\n\n\n    def get_model(self):\n        return SentenceTransformer(\"all-mpnet-base-v2\").to(self.device)\n\n    @staticmethod\n    def find_device():\n        \"\"\"\n        Find and return the best available device for computation.\n        \"\"\"\n        # Check if MPS is available\n        if torch.backends.mps.is_available():\n            print(\"Using MPS device\")\n            return \"mps\"\n        # Check if CUDA is available\n        elif torch.cuda.is_available():\n            print(\"Using CUDA device\")\n            return \"cuda\"\n        # Default to CPU if neither MPS nor CUDA is available\n        else:\n            print(\"Using CPU device\")\n            return \"cpu\"\n\n    def empty_device_cache(self):\n        \"\"\"\n        Empties the cache of the current device to free up memory.\n\n        This function clears the cache for either MPS on macOS devices, CUDA on GPU-enabled devices, or collects garbage if the device is set to CPU.\n        \"\"\"\n        if self.device == \"mps\":\n            torch.mps.empty_cache()\n        elif self.device == 'cuda':\n            torch.cuda.empty_cache()\n        else:\n            gc.collect()\n\n    def converter_func(self, x):\n        \"\"\"\n        Converts a given value to a zero-padded 4-digit string if the value is numeric.\n        This function checks if the input `x` is a numeric value and attempts to convert it to a 4-digit string, padded with leading zeros if necessary.\n\n        Args:\n            x (any): The value to be converted. Can be a number, a string, or `NaN`.\n        Returns:\n            str or any: A zero-padded 4-digit string if `x` is a numeric value, or the original value if it cannot be converted.\n        \"\"\"\n        if pd.isna(x):\n            return x \n        else:\n            try:\n                if isinstance(x, float) and x.is_integer():\n                    int_x = int(x)\n                    return f\"{int_x:04d}\"  \n                else:\n\n                    int_x = int(float(x))\n                    return f\"{int_x:04d}\"  \n            except ValueError:\n\n                return x\n</code></pre>"}, {"location": "docstrings/py_doc_Strings/docs/docstrings/#py_doc_Strings.matching.NlpPilepine.calculate_similarity", "title": "<code>calculate_similarity(docs, doc_embeddings, query, query_emb)</code>", "text": "<p>Calculates the cosine similarity between query embeddings and document embeddings using Faiss.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>list</code> <p>List of documents.</p> required <code>doc_embeddings</code> <code>ndarray</code> <p>Embeddings of the documents, expected numpy array.</p> required <code>query</code> <code>str</code> <p>Query identifier.</p> required <code>query_emb</code> <code>ndarray</code> <p>Embeddings of the query, expected numpy array.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>A DataFrame containing similarity scores and maximum candidate details.</p> Source code in <code>projects/docstrings/py_doc_Strings/matching.py</code> <pre><code>def calculate_similarity(self, docs, doc_embeddings, query, query_emb):\n    \"\"\"\n    Calculates the cosine similarity between query embeddings and document embeddings using Faiss.\n\n    Args:\n        docs (list): List of documents.\n        doc_embeddings (numpy.ndarray): Embeddings of the documents, expected numpy array.\n        query (str): Query identifier.\n        query_emb (numpy.ndarray): Embeddings of the query, expected numpy array.\n\n    Returns:\n        DataFrame: A DataFrame containing similarity scores and maximum candidate details.\n    \"\"\"\n    start = time.time()\n    # Ensure the embeddings are in numpy array format\n    if not isinstance(doc_embeddings, np.ndarray):\n        doc_embeddings = doc_embeddings.cpu().numpy()\n    if not isinstance(query_emb, np.ndarray):\n        query_emb = query_emb.cpu().numpy()\n\n    # Normalize the embeddings to use cosine similarity\n    faiss.normalize_L2(doc_embeddings)\n    faiss.normalize_L2(query_emb)\n\n    # Create a Faiss index for inner product (cosine similarity)\n    d = doc_embeddings.shape[1]\n    index = faiss.IndexFlatIP(d)\n    index.add(doc_embeddings)\n\n    # Perform the search\n    D, I = index.search(query_emb, k=1)  # Find the most similar document\n\n    # Extract the top scores and indices\n    max_scores = D.flatten()\n    max_indices = I.flatten()\n\n    # Create a DataFrame to store results\n    similarity_df = pd.DataFrame(columns=['max_cand', 'max_score'], index=[query])\n    similarity_df['max_cand'] = [docs[idx] for idx in max_indices]\n    similarity_df['max_score'] = max_scores\n    print(\"Similarity Calculation Completed in --- %s seconds ---\" % (time.time() - start))\n    return similarity_df\n</code></pre>"}, {"location": "docstrings/py_doc_Strings/docs/docstrings/#py_doc_Strings.matching.NlpPilepine.convert_to_list", "title": "<code>convert_to_list(embedding_str)</code>", "text": "<p>Converts a string or numpy array representation of embeddings into a list of floats.</p> <p>Parameters:</p> Name Type Description Default <code>embedding</code> <code>str or ndarray</code> <p>The embedding, either as a string representation or a numpy array.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>A list of floats extracted from the input.</p> Source code in <code>projects/docstrings/py_doc_Strings/matching.py</code> <pre><code>def convert_to_list(self, embedding_str):\n    \"\"\"\n    Converts a string or numpy array representation of embeddings into a list of floats.\n\n    Args:\n        embedding (str or np.ndarray): The embedding, either as a string representation or a numpy array.\n\n    Returns:\n        list: A list of floats extracted from the input.\n    \"\"\"\n    if isinstance(embedding_str, str):\n        # Use regex to find all numbers in the string (handles scientific notation as well)\n        numbers = re.findall(r\"[-+]?\\d*\\.\\d+e[-+]?\\d+|\\d*\\.\\d+|\\d+\", embedding_str)\n        # Convert the extracted strings to floats\n        return [float(num) for num in numbers]\n    elif isinstance(embedding_str, np.ndarray):\n        return embedding_str.tolist()\n    else:\n        raise TypeError(f\"Unsupported type for embedding: {type(embedding_str)}, {(embedding_str)}\")\n</code></pre>"}, {"location": "docstrings/py_doc_Strings/docs/docstrings/#py_doc_Strings.matching.NlpPilepine.converter_func", "title": "<code>converter_func(x)</code>", "text": "<p>Converts a given value to a zero-padded 4-digit string if the value is numeric. This function checks if the input <code>x</code> is a numeric value and attempts to convert it to a 4-digit string, padded with leading zeros if necessary.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>any</code> <p>The value to be converted. Can be a number, a string, or <code>NaN</code>.</p> required <p>Returns:     str or any: A zero-padded 4-digit string if <code>x</code> is a numeric value, or the original value if it cannot be converted.</p> Source code in <code>projects/docstrings/py_doc_Strings/matching.py</code> <pre><code>def converter_func(self, x):\n    \"\"\"\n    Converts a given value to a zero-padded 4-digit string if the value is numeric.\n    This function checks if the input `x` is a numeric value and attempts to convert it to a 4-digit string, padded with leading zeros if necessary.\n\n    Args:\n        x (any): The value to be converted. Can be a number, a string, or `NaN`.\n    Returns:\n        str or any: A zero-padded 4-digit string if `x` is a numeric value, or the original value if it cannot be converted.\n    \"\"\"\n    if pd.isna(x):\n        return x \n    else:\n        try:\n            if isinstance(x, float) and x.is_integer():\n                int_x = int(x)\n                return f\"{int_x:04d}\"  \n            else:\n\n                int_x = int(float(x))\n                return f\"{int_x:04d}\"  \n        except ValueError:\n\n            return x\n</code></pre>"}, {"location": "docstrings/py_doc_Strings/docs/docstrings/#py_doc_Strings.matching.NlpPilepine.empty_device_cache", "title": "<code>empty_device_cache()</code>", "text": "<p>Empties the cache of the current device to free up memory.</p> <p>This function clears the cache for either MPS on macOS devices, CUDA on GPU-enabled devices, or collects garbage if the device is set to CPU.</p> Source code in <code>projects/docstrings/py_doc_Strings/matching.py</code> <pre><code>def empty_device_cache(self):\n    \"\"\"\n    Empties the cache of the current device to free up memory.\n\n    This function clears the cache for either MPS on macOS devices, CUDA on GPU-enabled devices, or collects garbage if the device is set to CPU.\n    \"\"\"\n    if self.device == \"mps\":\n        torch.mps.empty_cache()\n    elif self.device == 'cuda':\n        torch.cuda.empty_cache()\n    else:\n        gc.collect()\n</code></pre>"}, {"location": "docstrings/py_doc_Strings/docs/docstrings/#py_doc_Strings.matching.NlpPilepine.encode_data", "title": "<code>encode_data(column_list)</code>", "text": "<p>Encodes a list of text data into embeddings using a specified model.</p> <p>Parameters:</p> Name Type Description Default <code>column_list</code> <code>list</code> <p>A list of text items to encode.</p> required <code>model</code> <code>Model</code> <p>The model used for encoding the text.</p> required <p>Returns:</p> Name Type Description <code>Tensor</code> <p>The embeddings tensor generated from the input text.</p> Source code in <code>projects/docstrings/py_doc_Strings/matching.py</code> <pre><code>def encode_data(self, column_list):\n    \"\"\"\n    Encodes a list of text data into embeddings using a specified model.\n\n    Args:\n        column_list (list): A list of text items to encode.\n        model (Model): The model used for encoding the text.\n\n    Returns:\n        Tensor: The embeddings tensor generated from the input text.\n    \"\"\"\n    pool = self.model.start_multi_process_pool([self.device, self.device, self.device, self.device])\n\n    embeddings = self.model.encode_multi_process(column_list, pool, show_progress_bar=True) # Encoding the text data into embeddings\n\n    self.model.stop_multi_process_pool(pool)\n\n    return embeddings\n</code></pre>"}, {"location": "docstrings/py_doc_Strings/docs/docstrings/#py_doc_Strings.matching.NlpPilepine.find_device", "title": "<code>find_device()</code>  <code>staticmethod</code>", "text": "<p>Find and return the best available device for computation.</p> Source code in <code>projects/docstrings/py_doc_Strings/matching.py</code> <pre><code>@staticmethod\ndef find_device():\n    \"\"\"\n    Find and return the best available device for computation.\n    \"\"\"\n    # Check if MPS is available\n    if torch.backends.mps.is_available():\n        print(\"Using MPS device\")\n        return \"mps\"\n    # Check if CUDA is available\n    elif torch.cuda.is_available():\n        print(\"Using CUDA device\")\n        return \"cuda\"\n    # Default to CPU if neither MPS nor CUDA is available\n    else:\n        print(\"Using CPU device\")\n        return \"cpu\"\n</code></pre>"}, {"location": "docstrings/py_doc_Strings/docs/docstrings/#py_doc_Strings.matching.NlpPilepine.get_embeddings", "title": "<code>get_embeddings(data, column_name, model, ready_embeddings=False, emb_col='emb')</code>", "text": "<p>Retrieves and processes embeddings for various categories and names from provided data.</p> <p>Parameters:</p> Name Type Description Default <code>HDM_data</code> <code>DataFrame</code> <p>DataFrame containing GOOD_NAME_CL_TR2 data.</p> required <code>reference_data</code> <code>DataFrame</code> <p>DataFrame with sub_category and cleaned_good_name2 data.</p> required <code>model</code> <code>Model</code> <p>The model used for encoding the data.</p> required <p>Returns:</p> Name Type Description <code>tuple</code> <p>A tuple containing processed query, embeddings, and document data.</p> Source code in <code>projects/docstrings/py_doc_Strings/matching.py</code> <pre><code>def get_embeddings(self, data, column_name,  model, ready_embeddings = False, emb_col = \"emb\"):\n    \"\"\"\n    Retrieves and processes embeddings for various categories and names from provided data.\n\n    Args:\n        HDM_data (DataFrame): DataFrame containing GOOD_NAME_CL_TR2 data.\n        reference_data (DataFrame): DataFrame with sub_category and cleaned_good_name2 data.\n        model (Model): The model used for encoding the data.\n\n    Returns:\n        tuple: A tuple containing processed query, embeddings, and document data.\n    \"\"\"\n    # Ensure the column has no NaN values and is of string type\n    data[column_name] = data[column_name].fillna('').astype(str)\n\n    if ready_embeddings: #If embeddings are already calculated and are in the data\n\n        # Process data\n        data[f'{emb_col}_open'] = data[emb_col].apply(self.convert_to_list)\n        if \"sub_category\" in data.columns:\n            data['sub_category'] = data['sub_category'].apply(lambda x: str(x).lower().strip())\n        else:\n            pass\n        # Embeddings from data\n        original_data_list = data[column_name].tolist()\n        original_data_emb =  torch.tensor(data[f'{emb_col}_open'].tolist()).to(self.device) # send preobtained embeddings to device\n\n    else:\n        original_data_list = data[column_name].tolist()\n        original_data_emb =  self.encode_data(column_list = original_data_list)\n\n    return original_data_list, original_data_emb\n</code></pre>"}, {"location": "docstrings/py_doc_Strings/docs/docstrings/#py_doc_Strings.matching.NlpPilepine.isolated_matching_categorization", "title": "<code>isolated_matching_categorization()</code>", "text": "<p>Processes the matching of GOOD_NAME data with reference data using specified embeddings.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <p>The final DataFrame after processing matches and merging data.</p> Source code in <code>projects/docstrings/py_doc_Strings/matching.py</code> <pre><code>def isolated_matching_categorization(self):\n    \"\"\"\n    Processes the matching of GOOD_NAME data with reference data using specified embeddings.\n\n    Args:\n        None\n\n    Returns:\n        DataFrame: The final DataFrame after processing matches and merging data.\n    \"\"\"\n\n    # =====================================================================================================================================\n    #                                                 Isolated Matching: Fruits and Veggies Matching \n    # =====================================================================================================================================\n\n    fruits_results = self.isolated_matching(ShetName = 'Fruits_veggies', HdmData = self.data, CheckingList = self.checking_list_veggies,\n                                    reference_df = self.target_veggies, semantic_treshold=0.5, model = self.model, lexical_treshold = 61, \n                                    category_name = \"Fruits and Veggies\", special_words = {\"\u0579\u056b\u0580|\u0579\u0578\u0580|\u0579\u0561\u0574\u056b\u0579\":\"Dried_fruits_and_vegetables\", \n                                                \"\u057d\u0561\u057c\":\"Frozen_fruits_vegetables_and_berries\", \"\u0564\u0564\u0574\u056b\u056f\":\"Zucchini\",'\u056f\u0561\u0576\u0561\u0579\u056b':'Greens' }, force_semantic_words = ['cherry'])\n\n    if fruits_results is not None:\n        # Only execute this code block if fruits_results is a DataFrame\n        Working_data = self.data[~self.data['GOOD_NAME'].isin(fruits_results['GOOD_NAME'])]\n    else:\n        # Handle the case where fruits_results is None\n        print(\"No fruits results to exclude from HDM data.\")\n        Working_data = self.data\n\n    # Empty device cahce beofr proceeding to whole data matching\n    self.empty_device_cache()\n    # =====================================================================================================================================\n    #                                                 Isolated Matching: Grain\n    # =====================================================================================================================================\n\n    grain_results = self.isolated_matching(ShetName = 'Grain',HdmData = Working_data, CheckingList = self.checking_list_groats,\n                                            reference_df = self.target_grain, lexical_treshold = 79, semantic_matching= True, model = self.model,\n                                semantic_treshold = 0.7, special_words = {'\u0562\u056c\u0572\u0578\u0582\u0580':'Bulgur',  '\u0570\u0576\u0564\u056f\u0561\u0571\u0561\u057e\u0561\u0580':'Buckwheat', '\\b\u0571\u0561\u057e\u0561\u0580\\b':'Wheat_groat',\n                                                                          '\u057d\u057a\u056b\u057f\u0561\u056f\u0561\u0571\u0561\u057e\u0561\u0580|\u0574\u0561\u0576\u056b':'Semolina',  '\u057d\u056b\u057d\u0565\u057c':'Chickpeas', '\u0563\u0561\u0580\u0565\u0571\u0561\u057e\u0561\u0580':'Pearl_barley',\n                                                                          '\u0570\u0561\u0573\u0561\u0580':'Emmer', '\u0583\u0578\u056d\u056b\u0576\u0571':'Other_grain', '\u0565\u0563\u056b\u057a\u057f\u0561\u0581\u0578\u0580\u0565\u0576':'Dried_corn', '\u0562\u0580\u056b\u0576\u0571':'Rice',\n                                                                           '\u0563\u0561\u0580\u0578\u056d|\u0578\u056c\u0578\u057c':'Peas', '\u0578\u057d\u057a':'Lentil', '\u056c\u0578\u0562\u056b':'Bean', '\u056f\u0578\u0580\u0565\u056f\u0561\u0571\u0561\u057e\u0561\u0580|\u056f\u0578\u0580\u0565\u056f':\"Millet_bran\",\n                                                                           '\u057d\u0561\u0563\u0561\u056d\u0578\u057f|\u056f\u056b\u0576\u0578\u0582\u0561|\u056f\u056b\u0576\u0578\u0561|\u0584\u056b\u0576\u0578\u0561|\u0584\u056b\u0576\u0578\u0582\u0561':'Quinoa','\u0583\u0561\u0569\u056b\u056c\u0576\u0565\u0580':'Flakes', '\u057e\u0561\u0580\u057d\u0561\u056f':'Oat',\"\u0561\u056c\u0575\u0578\u0582\u0580\":\"Flour\",}, \n                                                                            force_semantic_words = ['flake', 'barley'], category_name = \"Grain\")\n\n    if grain_results is not None:\n        # Only execute this code block if fruits_results is a DataFrame\n        Working_data = Working_data[~Working_data['GOOD_NAME'].isin(grain_results['GOOD_NAME'])]\n    else:\n        # Handle the case where fruits_results is None\n        print(\"No grain results to exclude from HDM data.\")\n        Working_data = Working_data\n\n    # Empty device cahce beofr proceeding to whole data matching\n    self.empty_device_cache()\n\n    # =====================================================================================================================================\n    #                                                 Isolated Matching: Mix of grain and fresh veggies\n    # =====================================================================================================================================\n\n    mixed_results = self.isolated_matching(ShetName = 'Mixed_category', HdmData = Working_data, CheckingList = ['marinated'], \n                                        reference_df = pd.concat([self.target_veggies,self.target_grain]), semantic_treshold=0.6, model = self.model, lexical_treshold = 60, \n                                        category_name = \"Mixed Category\", force_semantic_words = ['cherry'])\n\n\n    if mixed_results is not None:\n        # Only execute this code block if fruits_results is a DataFrame\n        Working_data = Working_data[~Working_data['GOOD_NAME'].isin(mixed_results['GOOD_NAME'])]\n    else:\n        # Handle the case where fruits_results is None\n        print(\"No mixed category results to exclude from HDM data.\")\n        Working_data = Working_data\n\n    # Empty device cahce beofr proceeding to whole data matching\n    self.empty_device_cache()\n\n    # =====================================================================================================================================\n    #                                                 Isolated Matching: Dairy\n    # =====================================================================================================================================\n\n    dairy_results = self.isolated_matching(ShetName = 'Dairy', model = self.model, semantic_matching=True,\n                            HdmData = Working_data, CheckingList = self.checking_list_diary, reference_df = self.target_diary,  lexical_treshold = 75,  semantic_treshold = 0.7,\n                               special_words = {\"\u0569\u0569\u057e\u0561\u057d\u0565\u0580\": \"Sour_cream\",\"\u056f\u0561\u0569\u0576\u0561\u0577\u0578\u057c\": \"Cottage_cheese\",  \"\u057a\u0561\u0576\u056b\u0580\": \"Cheese\", \n                                                '\u057d\u0583\u0580\u0565\u0564':'Spread', '\u056f\u0561\u0580\u0561\u0563':'Butter',  \"\u0574\u0561\u056e\u0576\u0561\u0562\u0580\u0564\u0578\u0577\" : \"other_dairy\",\n                                                \"\\b\u057d\u0565\u0580\u0578\u0582\u0581\u0584\\b\": \"Cream\", }, category_name = \"Dairy\")\n\n    if dairy_results is not None:\n            # Only execute this code block if fruits_results is a DataFrame\n            Working_data = Working_data[~Working_data['GOOD_NAME'].isin(dairy_results['GOOD_NAME'])]\n    else:\n        # Handle the case where fruits_results is None\n        print(\"No dairy results to exclude from HDM data.\")\n        Working_data = Working_data\n\n    # Empty device cahce beofr proceeding to whole data matching\n    self.empty_device_cache()\n\n    # =====================================================================================================================================\n    #                                                 Isolated Matching: Petrol\n    # =====================================================================================================================================  \n    petrol_results = self.isolated_matching(ShetName = 'Petrol', lexical_treshold= 0, HdmData = Working_data, CheckingList = self.checking_list_petrol,\n                                     reference_df = self.target_petrol,  semantic_matching= False, category_name = \"Petrol\")\n\n\n    if petrol_results is not None:\n            # Only execute this code block if fruits_results is a DataFrame\n            Working_data = Working_data[~Working_data['GOOD_NAME'].isin(petrol_results['GOOD_NAME'])]\n    else:\n        # Handle the case where fruits_results is None\n        print(\"No petrol results to exclude from HDM data.\")\n        Working_data = Working_data\n\n    # Empty device cahce beofr proceeding to whole data matching\n    self.empty_device_cache()\n\n    # =====================================================================================================================================\n    #                                                 Isolated Matching: Canned Food\n    # =====================================================================================================================================\n\n    canned_food_results =  self.isolated_matching(ShetName = 'Canned_food', model =  self.model, semantic_treshold = 0.8, semantic_matching=True,\n                                                HdmData = Working_data, CheckingList =  self.checking_list_canned_food, reference_df =  self.target_canned_food, \n                                                lexical_treshold = 80,  category_name = \"Canned Food\")\n\n\n    if canned_food_results is not None:\n            # Only execute this code block if fruits_results is a DataFrame\n            Working_data = Working_data[~Working_data['GOOD_NAME'].isin(canned_food_results['GOOD_NAME'])]\n    else:\n        # Handle the case where canned_food_results is None\n        print(\"No canned food results to exclude from HDM data.\")\n        Working_data = Working_data\n\n    # Empty device cahce beofr proceeding to whole data matching\n    self.empty_device_cache()\n\n    # =====================================================================================================================================\n    #                                                 Isolated Matching: Meat \n    # =====================================================================================================================================\n\n    meat_products_results = self.isolated_matching( ShetName = 'meat_products', model = self.model, HdmData = Working_data, CheckingList = self.checking_list_meat_products,\n                                    reference_df = self.target_meat_products, lexical_match=False, semantic_matching= True,  semantic_treshold=0.75, \n                                    special_words = {\"\u0565\u0580\u0577\u056b\u056f\": \"Boiled_sausage\",\"\u0576\u0580\u0562\u0565\u0580\u0577\u056b\u056f\": \"Sausages\", \"\u0562\u0561\u057d\u057f\u0578\u0582\u0580\u0574\u0561|\u057d\u0578\u0582\u057b\u0578\u0582\u056d\" : \"Basturma_sujuk\"}, \n                                    category_name = \"Meat and Meat Products\")\n\n\n    if meat_products_results is not None:\n            # Only execute this code block if fruits_results is a DataFrame\n            Working_data = Working_data[~Working_data['GOOD_NAME'].isin(meat_products_results['GOOD_NAME'])]\n    else:\n        # Handle the case where meat_products_results is None\n        print(\"No meat products results to exclude from HDM data.\")\n        Working_data = Working_data\n\n    # Empty device cahce beofr proceeding to whole data matching\n    self.empty_device_cache()\n\n    # ================================================ Prepare matching data ==============================================================\n\n    # Debug prints to understand the filtering process\n    print(\"Initial reference_data length:\", len(self.embeddings_data))\n    reference_data = self.embeddings_data[~self.embeddings_data['cleaned_good_name2'].str.lower().isin(self.target_veggies['cleaned_good_name2'].str.lower())]\n    reference_data = reference_data[~reference_data['cleaned_good_name2'].str.lower().isin(self.target_grain['cleaned_good_name2'].str.lower())]\n    reference_data = reference_data[~reference_data['cleaned_good_name2'].str.lower().isin(self.target_diary['cleaned_good_name2'].str.lower())]\n    reference_data = reference_data[~reference_data['cleaned_good_name2'].str.lower().isin(self.target_petrol['cleaned_good_name2'].str.lower())]\n    reference_data = reference_data.drop_duplicates(subset='cleaned_good_name2')\n    print(\"Final reference_data length after dropping duplicates:\", len(reference_data))\n\n    # =========================================Perform Regular Matching on the rest of the data===========================================\n\n    res_df = self.semantic_matching(Working_data, reference_data)\n\n    all_matches = pd.concat([\n        fruits_results,\n        grain_results,\n        mixed_results,\n        dairy_results,\n        petrol_results,\n        canned_food_results,\n        meat_products_results,\n        res_df\n    ], ignore_index=True)#.drop_duplicates(subset='GOOD_NAME_CL_TR2')\n\n\n    all_matches = self.conditional_decision_logic(all_matches)\n\n    return all_matches\n</code></pre>"}]}